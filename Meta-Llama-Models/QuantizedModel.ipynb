{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b496808-9635-4526-b31b-404fceea48f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Quantized meta Llama model from Hugging Face due to space and memory limitations\n",
    "# https://huggingface.co/neuralmagic/Meta-Llama-3-8B-Instruct-quantized.w8a16\n",
    "# insturct quantized model \n",
    "\n",
    "#imports\n",
    "%pip install transformers datasets accelerate\n",
    "%pip install scikit-learn\n",
    "%pip install transformers datasets accelerate\n",
    "%pip install optimum \n",
    "%pip install auto-gptq\n",
    "%pip install safetensors\n",
    "%pip install tf-keras\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline,TrainingArguments, Trainer\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51c09cf-10f2-43aa-843f-c27031f0809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "torch.backends.cuda.matmul.allow_tf32 = True  # Enable TensorFloat32\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddca3158-4d18-4ea5-ab7b-c6525cdd7063",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model from hugging face\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"neuralmagic/Meta-Llama-3-8B-Instruct-quantized.w8a16\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"neuralmagic/Meta-Llama-3-8B-Instruct-quantized.w8a16\",\n",
    "    device_map=\"auto\",  # gpu use\n",
    "    torch_dtype=torch.float16,  # mixed precision for memory mgmt\n",
    "    offload_folder=\"./offload\",\n",
    "    low_cpu_mem_usage=True  \n",
    ")\n",
    "\n",
    "# Define input messages\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Generate a CWL file\"},\n",
    "]\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "output = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=50,  # Adjust max tokens as needed\n",
    ")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90214d7-72a9-4dd7-bef3-727e4eed1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dataframes\n",
    "\n",
    "df1 = pd.read_json(\"merged_cwl_documents.json\")\n",
    "df2 = pd.read_json(\"documents_with_descriptions.json\")\n",
    "\n",
    "df = pd.concat([df1, df2])\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb0e655-d30c-457a-bafc-f17fc4112b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install auto-gptq[cuda]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a68c2c7-e621-41fd-877b-4da23ce1479d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load model and tokenizer from hugging face\n",
    "model_name = \"neuralmagic/Meta-Llama-3-8B-Instruct-quantized.w8a16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# fix padding error\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token if available\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a new pad token\n",
    "        model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings to include the new token\n",
    "\n",
    "#  target columns to use as input features\n",
    "target_column = ['description', 'inputs', 'outputs']\n",
    "missing_columns = [col for col in target_column if col not in df.columns]\n",
    "if missing_columns:\n",
    "    raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "\n",
    "df['features'] = df[target_column].apply(lambda x: ' '.join(map(str, x)), axis=1)\n",
    "\n",
    "df['label'] = df['outputs'].astype(str)\n",
    "df = df[df['label'].notnull()]\n",
    "#split\n",
    "X = df['features']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# xtest to string because of error\n",
    "X_test = X_test.astype(str).tolist()\n",
    "\n",
    "# hugging face preprocess for tokenizer\n",
    "def preprocess_texts(texts):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of text inputs for the model.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"  # Return PyTorch tensors\n",
    "    )\n",
    "test_encodings = preprocess_texts(X_test)\n",
    "\n",
    "\n",
    "model.eval()  # Set model to evaluation mode\n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    logits = outputs.logits\n",
    "    test_predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "\n",
    "test_predictions_labels = [str(label) for label in test_predictions]\n",
    "\n",
    "# test accuracy of model\n",
    "y_test = y_test.astype(str).tolist()  # Ensure y_test is a list of strings\n",
    "correct_predictions = sum(\n",
    "    1 for true, pred in zip(y_test, test_predictions_labels) if true.lower() == pred.lower()\n",
    ")\n",
    "total_predictions = len(y_test)\n",
    "accuracy_percentage = (correct_predictions / total_predictions) * 100\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Model Accuracy: {accuracy_percentage:.2f}%\")\n",
    "\n",
    "# Print example predictions\n",
    "print(\"\\nExample Predictions:\")\n",
    "for input_text, true_label, prediction in zip(X_test[:10], y_test[:10], test_predictions_labels[:10]):\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b8c4a6-3670-4350-872a-a4a985dbf909",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "# https://towardsdatascience.com/qa-lora-fine-tune-a-quantized-large-language-model-on-your-gpu-c7291866706c\n",
    "\n",
    "# need to try using lora bc the other one will not work\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",  # Automatically place the model on the available device\n",
    "    torch_dtype=torch.float16  # Use mixed precision for memory savings\n",
    ")\n",
    "\n",
    "# padding\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # Use eos_token as pad_token\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})  # Add a new pad_token\n",
    "        model.resize_token_embeddings(len(tokenizer))  # Resize model embeddings if necessary\n",
    "\n",
    "# gradient checkpointing to save memory\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "#  LoRA for quantized models \n",
    "lora_config = LoraConfig(\n",
    "    r=8,                \n",
    "    lora_alpha=32,      \n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  \n",
    "    lora_dropout=0.1,    \n",
    "    bias=\"none\",         \n",
    "    task_type=\"CAUSAL_LM\"  \n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "\n",
    "# `train_dataset` and `val_dataset` a\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"combined\"], truncation=True, padding=\"max_length\")\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "val_dataset = val_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "# Set up training arguments with optimizations for memory usage\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               \n",
    "    evaluation_strategy=\"epoch\",         \n",
    "    learning_rate=5e-5,                  \n",
    "    per_device_train_batch_size=1,        \n",
    "    gradient_accumulation_steps=16,       # accumulate gradients over 16 steps for size\n",
    "    num_train_epochs=3,                 \n",
    "    weight_decay=0.01,                    \n",
    "    save_steps=10_000,                    \n",
    "    save_total_limit=2,                  \n",
    "    logging_dir=None,                     # disable logging to save memory\n",
    "    logging_steps=500,                    \n",
    "    fp16=True,                            # nable mixed precision for memory\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   \n",
    "    eval_dataset=val_dataset,      \n",
    "    tokenizer=tokenizer,           \n",
    ")\n",
    "\n",
    "#try training without lora \n",
    "\n",
    "trainer.train()\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113db9ba-7b77-425a-8397-3308c812c699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "with torch.no_grad():\n",
    "    outputs = model(**test_encodings)\n",
    "    logits = outputs.logits\n",
    "    test_predictions = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "# Print accuracy\n",
    "print(f\"Model Accuracy: {accuracy_percentage:.2f}%\")\n",
    "\n",
    "# Print example predictions\n",
    "print(\"\\nExample Predictions:\")\n",
    "for input_text, true_label, prediction in zip(X_test[:10], y_test[:10], test_predictions_labels[:10]):\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"True Label: {true_label}\")\n",
    "    print(f\"Predicted: {prediction}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a662cec0-6dd8-48ca-a5aa-aa77ae3b10a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    "    load_in_8bit=True  # Ensure 8-bit quantization compatibility\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a new pad_token and resize tokenizer embeddings if needed\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Enable gradient checkpointing for memory efficiency\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# train quantized with lora \n",
    "lora_config = LoraConfig(\n",
    "    r=8,                      # Rank of the LoRA matrix\n",
    "    lora_alpha=32,            # LoRA scaling factor\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Modules to apply LoRA to\n",
    "    lora_dropout=0.1,         # Dropout rate for LoRA\n",
    "    bias=\"none\",              # Whether to use bias\n",
    "    task_type=\"CAUSAL_LM\"     # Task type: Causal Language Modeling\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",               # Save model outputs here\n",
    "    evaluation_strategy=\"epoch\",          # Evaluate after each epoch\n",
    "    learning_rate=5e-5,                   # Learning rate\n",
    "    per_device_train_batch_size=1,        # Reduce batch size to fit in memory\n",
    "    gradient_accumulation_steps=16,       # Accumulate gradients over 16 steps\n",
    "    num_train_epochs=3,                   # Train for 3 epochs\n",
    "    weight_decay=0.01,                    # Weight decay for regularization\n",
    "    save_steps=10_000,                    # Save model every 10k steps\n",
    "    save_total_limit=2,                   # Keep only the last 2 checkpoints\n",
    "    logging_dir=None,                     # Disable logging to save memory\n",
    "    logging_steps=500,                    # Log every 500 steps (optional)\n",
    "    fp16=True,                            # Enable mixed precision (half-precision)\n",
    ")\n",
    "\n",
    "# Create the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,   # Replace with actual train dataset\n",
    "    eval_dataset=val_dataset,      # Replace with actual validation dataset\n",
    "    tokenizer=tokenizer,           # Tokenizer used for preprocessing\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
