{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f87feeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f012939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Unnamed: 0                                              title license  \\\n",
      "0         []                     Genomics - Read pre-processing     MIT   \n",
      "1         []   Virus genome assembly with Unicycler and Spades.     MIT   \n",
      "2         []  Genomics - Read pre-processing without downloa...     MIT   \n",
      "3         []         Genomics - Assembly of the genome sequence     MIT   \n",
      "4         []                           Genomics - MRCA analysis     MIT   \n",
      "\n",
      "                                         description  latest_version  \\\n",
      "0  Preprocessing of raw SARS-CoV-2 reads. More in...               1   \n",
      "1  Virus genome assembly with Unicycler and Spade...               1   \n",
      "2  Preprocessing of raw SARS-CoV-2 reads. This wo...               1   \n",
      "3  This workflow uses Illumina and Oxford Nanopor...               1   \n",
      "4  Dating the most recent common ancestor (MRCA) ...               1   \n",
      "\n",
      "                       tags  \\\n",
      "0              ['covid-19']   \n",
      "1  ['Assembly', 'covid-19']   \n",
      "2              ['covid-19']   \n",
      "3              ['covid-19']   \n",
      "4              ['covid-19']   \n",
      "\n",
      "                                            versions  version  \\\n",
      "0  [{'version': 1, 'revision_comments': None, 'ur...        1   \n",
      "1  [{'version': 1, 'revision_comments': None, 'ur...        1   \n",
      "2  [{'version': 1, 'revision_comments': None, 'ur...        1   \n",
      "3  [{'version': 1, 'revision_comments': None, 'ur...        1   \n",
      "4  [{'version': 1, 'revision_comments': None, 'ur...        1   \n",
      "\n",
      "  revision_comments                created_at  ...  doi content_blobs  \\\n",
      "0               NaN  2020-04-10T10:20:13.000Z  ...  NaN            []   \n",
      "1               NaN  2020-04-10T10:45:00.000Z  ...  NaN            []   \n",
      "2               NaN  2020-04-10T12:06:00.000Z  ...  NaN            []   \n",
      "3               NaN  2020-04-10T12:32:21.000Z  ...  NaN            []   \n",
      "4               NaN  2020-04-10T12:44:38.000Z  ...  NaN            []   \n",
      "\n",
      "  creators                                     other_creators  \\\n",
      "0       []  Dannon Baker, Marius van den Beek, Dave Bouvie...   \n",
      "1       []                                                NaN   \n",
      "2       []  Dannon Baker, Marius van den Beek, Dave Bouvie...   \n",
      "3       []  Dannon Baker, Marius van den Beek, Dave Bouvie...   \n",
      "4       []  Dannon Baker, Marius van den Beek, Dave Bouvie...   \n",
      "\n",
      "                                      workflow_class operation_annotations  \\\n",
      "0  {'title': 'Galaxy', 'key': 'galaxy', 'descript...                    []   \n",
      "1  {'title': 'Common Workflow Language', 'key': '...                    []   \n",
      "2  {'title': 'Galaxy', 'key': 'galaxy', 'descript...                    []   \n",
      "3  {'title': 'Galaxy', 'key': 'galaxy', 'descript...                    []   \n",
      "4  {'title': 'Galaxy', 'key': 'galaxy', 'descript...                    []   \n",
      "\n",
      "  topic_annotations                                          internals tools  \\\n",
      "0                []  {'inputs': [{'id': '0_Input Dataset', 'name': ...    []   \n",
      "1                []  {'inputs': [{'id': 'fastq_file_type', 'name': ...    []   \n",
      "2                []  {'inputs': [{'id': '0_Input Dataset Collection...    []   \n",
      "3                []  {'inputs': [{'id': '0_Input Dataset', 'name': ...    []   \n",
      "4                []  {'inputs': [{'id': '0_Input Dataset', 'name': ...    []   \n",
      "\n",
      "  id  \n",
      "0  2  \n",
      "1  3  \n",
      "2  4  \n",
      "3  5  \n",
      "4  6  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "(693, 21)\n"
     ]
    }
   ],
   "source": [
    "# put everything into dataframe \n",
    "\n",
    "filename = os.path.join(os.getcwd(),  \"workflow_cwlfiles.csv\")\n",
    "df = pd.read_csv(filename, header=0)\n",
    "\n",
    "\n",
    "print(df.head())\n",
    "print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1377002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "   Unnamed: 0  title  license  description  latest_version   tags  versions  \\\n",
      "0       False  False    False        False           False  False     False   \n",
      "1       False  False    False        False           False  False     False   \n",
      "2       False  False    False        False           False  False     False   \n",
      "3       False  False    False        False           False  False     False   \n",
      "4       False  False    False        False           False  False     False   \n",
      "\n",
      "   version  revision_comments  created_at  ...   doi  content_blobs  creators  \\\n",
      "0    False               True       False  ...  True          False     False   \n",
      "1    False               True       False  ...  True          False     False   \n",
      "2    False               True       False  ...  True          False     False   \n",
      "3    False               True       False  ...  True          False     False   \n",
      "4    False               True       False  ...  True          False     False   \n",
      "\n",
      "   other_creators  workflow_class  operation_annotations  topic_annotations  \\\n",
      "0           False           False                  False              False   \n",
      "1            True           False                  False              False   \n",
      "2           False           False                  False              False   \n",
      "3           False           False                  False              False   \n",
      "4           False           False                  False              False   \n",
      "\n",
      "   internals  tools     id  \n",
      "0      False  False  False  \n",
      "1      False  False  False  \n",
      "2      False  False  False  \n",
      "3      False  False  False  \n",
      "4      False  False  False  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "Unnamed: 0                 0\n",
      "title                      0\n",
      "license                    1\n",
      "description               15\n",
      "latest_version             0\n",
      "tags                       0\n",
      "versions                   0\n",
      "version                    0\n",
      "revision_comments        182\n",
      "created_at                 0\n",
      "updated_at                 0\n",
      "doi                      437\n",
      "content_blobs              0\n",
      "creators                   0\n",
      "other_creators           543\n",
      "workflow_class             0\n",
      "operation_annotations      0\n",
      "topic_annotations          0\n",
      "internals                  6\n",
      "tools                      0\n",
      "id                         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().values.any())\n",
    "print(df.isnull().head())\n",
    "\n",
    "nan = np.sum(df.isnull(), axis = 0)\n",
    "print(nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59296082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['license', 'description', 'revision_comments', 'doi', 'other_creators',\n",
      "       'internals'],\n",
      "      dtype='object')\n",
      "['license', 'description', 'revision_comments', 'doi', 'other_creators', 'internals']\n",
      "license              object\n",
      "description          object\n",
      "revision_comments    object\n",
      "doi                  object\n",
      "other_creators       object\n",
      "internals            object\n",
      "dtype: object\n",
      "['Preprocessing of raw SARS-CoV-2 reads. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'Virus genome assembly with Unicycler and Spades,\\nThe 2 assemblers works in parallel. The graph  visualization is made with Bandage.\\nworkflow git repository : https://github.com/fjrmoreews/cwl-workflow-SARS-CoV-2/blob/master/Assembly/workflow/assembly-wf-virus.cwl\\nBased on  https://github.com/galaxyproject/SARS-CoV-2/blob/master/genomics/2-Assembly/as_wf.png\\n'\n",
      " 'Preprocessing of raw SARS-CoV-2 reads. This workflow contains an alternate starting point to avoid the data to be downloaded from the NCBI SRA. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'This workflow uses Illumina and Oxford Nanopore reads that were pre-processed to remove human-derived sequences. Two assembly tools are used: spades and unicycler. In addition to assemblies (actual sequences) the two tools produce assembly graphs that can be used for visualization of assembly with bandage. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'Dating the most recent common ancestor (MRCA) of SARS-CoV-2. The workflow is used to extract full length sequences of SARS-CoV-2, tidy up their names in FASTA files, produce a multiple sequences alignment and compute a maximum likelihood tree. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'Analysis of variation within individual COVID-19 samples using Illumina Paired End data. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'Analysis of variation within individual COVID-19 samples using Illumina Single End data. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'Analysis of S-protein polymorphism. This workflow includes: obtaining coding sequences of S proteins from a diverse group of coronaviruses and generating amino acid alignments to assess conservation of the polymorphic location. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'This workflow employs a recombination detection algorithm (GARD) developed by Kosakovsky Pond et al. and implemented in the hyphy package. More info can be found at https://covid19.galaxyproject.org/genomics/'\n",
      " 'A version of V-pipe (analysis of next generation sequencing (NGS) data from viral pathogens) specifically adapted to analyze high-throughput sequencing data of SARS-CoV-2.'\n",
      " 'This workflow is used form the preparation of protein and ligands for docking. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'This workflow generates a file describing the active site of the protein for each of the fragment screening crystal structures using rDock s rbcavity. It also creates a single hybrid molecule that contains all the ligands - the \"frankenstein\" ligand. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'Docking performed by rDock using as 3 different kind of inputs. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'This workflow generates binding scores that correlate well with binding affinities using an additional tool SuCOS Max, developed at Oxford University. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'This workflow generates binding scores that correlate well with binding affinities using an additional tool TransFS, developed at Oxford University. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'This workflow combines SDF files from all fragments into a single dataset and filters to include only the lowest (best) scoring pose for each compound. This file of optimal poses for all ligands is used to compare to a database of Enamine and Chemspace compounds to select the best scoring 500 matches. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " 'This workflow is used for the virtual screening of the SARS-CoV-2 main protease (de.NBI-cloud, STFC). It includes Charge enumeration, Generation of 3D conformations, Preparation of active site for docking using rDock, Docking, Scoring and Selection of compounds available. More info can be found at https://covid19.galaxyproject.org/cheminformatics/'\n",
      " '<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviralreconhttpsrawgithubusercontentcomnfcoreviralreconmasterdocsimagesnfcoreviralrecon_logopng_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/viralrecon/master/docs/images/nf-core-viralrecon_logo.png\" alt=\"nf-core/viralrecon\"></h1>\\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20CI/badge.svg\" alt=\"GitHub Actions CI Status\"></a> <a href=\"https://github.com/nf-core/viralrecon/actions\"><img src=\"https://github.com/nf-core/viralrecon/workflows/nf-core%20linting/badge.svg\" alt=\"GitHub Actions Linting Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A519.10.0-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a></p>\\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"https://hub.docker.com/r/nfcore/viralrecon\"><img src=\"https://img.shields.io/docker/automated/nfcore/viralrecon.svg\" alt=\"Docker\"></a> <a href=\"https://doi.org/10.5281/zenodo.3872730\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.3872730.svg\" alt=\"DOI\"></a></p>\\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\"><strong>nfcore/viralrecon</strong> is a bioinformatics analysis pipeline used to perform assembly and intrahost/low-frequency variant calling for viral samples. The pipeline currently supports metagenomics and amplicon sequencing data derived from the Illumina sequencing platform.</p>\\n<p class=\"has-line-data\" data-line-start=\"10\" data-line-end=\"11\">This pipeline is a re-implementation of the <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_consensus-nf\">SARS_Cov2_consensus-nf</a> and <a href=\"https://github.com/BU-ISCIII/SARS_Cov2_assembly-nf\">SARS_Cov2_assembly-nf</a> pipelines initially developed by <a href=\"https://github.com/svarona\">Sarai Varona</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a>. Porting both of these pipelines to nf-core was an international collaboration between numerous contributors and developers, led by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from the <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London. We appreciated the need to have a portable, reproducible and scalable pipeline for the analysis of COVID-19 sequencing samples and so the Avengers Assembled! Please come and join us and add yourself to the contributor list :)</p>\\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">We have integrated a number of options in the pipeline to allow you to run specific aspects of the workflow if you so wish. For example, you can skip all of the assembly steps with the <code>--skip_assembly</code> parameter. See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\\n<p class=\"has-line-data\" data-line-start=\"14\" data-line-end=\"15\">Please click <a href=\"https://raw.githack.com/nf-core/viralrecon/master/docs/html/multiqc_report.html\">here</a> to see an example MultiQC report generated using the parameters defined in <a href=\"https://github.com/nf-core/viralrecon/blob/master/conf/test_full.config\">this configuration file</a> to run the pipeline on <a href=\"https://zenodo.org/record/3735111\">samples</a> which were prepared from the <a href=\"https://artic.network/ncov-2019\">ncov-2019 ARTIC Network V1 amplicon set</a> and sequenced on the Illumina MiSeq platform in 301bp paired-end format.</p>\\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. Furthermore, automated continuous integration tests to run the pipeline on a full-sized dataset are passing on AWS cloud.</p>\\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19><a id=\"Pipeline_summary_18\"></a>Pipeline summary</h2>\\n<ol>\\n<li class=\"has-line-data\" data-line-start=\"20\" data-line-end=\"21\">Download samples via SRA, ENA or GEO ids (<a href=\"https://ena-docs.readthedocs.io/en/latest/retrieval/file-download.html\"><code>ENA FTP</code></a>, <a href=\"https://github.com/rvalieris/parallel-fastq-dump\"><code>parallel-fastq-dump</code></a>; <em>if required</em>)</li>\\n<li class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"22\">Merge re-sequenced FastQ files (<a href=\"http://www.linfo.org/cat.html\"><code>cat</code></a>; <em>if required</em>)</li>\\n<li class=\"has-line-data\" data-line-start=\"22\" data-line-end=\"23\">Read QC (<a href=\"https://www.bioinformatics.babraham.ac.uk/projects/fastqc/\"><code>FastQC</code></a>)</li>\\n<li class=\"has-line-data\" data-line-start=\"23\" data-line-end=\"24\">Adapter trimming (<a href=\"https://github.com/OpenGene/fastp\"><code>fastp</code></a>)</li>\\n<li class=\"has-line-data\" data-line-start=\"24\" data-line-end=\"33\">Variant calling<br>\\ni. Read alignment (<a href=\"http://bowtie-bio.sourceforge.net/bowtie2/index.shtml\"><code>Bowtie 2</code></a>)<br>\\nii. Sort and index alignments (<a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\\niii. Primer sequence removal (<a href=\"https://github.com/andersen-lab/ivar\"><code>iVar</code></a>; <em>amplicon data only</em>)<br>\\niv. Duplicate read marking (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>; <em>removal optional</em>)<br>\\nv. Alignment-level QC (<a href=\"https://broadinstitute.github.io/picard/\"><code>picard</code></a>, <a href=\"https://sourceforge.net/projects/samtools/files/samtools/\"><code>SAMtools</code></a>)<br>\\nvi. Choice of multiple variant calling and consensus sequence generation routes (<a href=\"https://dkoboldt.github.io/varscan/\"><code>VarScan 2</code></a>, <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a> <em>||</em> <a href=\"https://github.com/andersen-lab/ivar\"><code>iVar variants and consensus</code></a> <em>||</em> <a href=\"https://samtools.github.io/bcftools/bcftools.html\"><code>BCFTools</code></a>, <a href=\"https://github.com/arq5x/bedtools2/\"><code>BEDTools</code></a>)<br>\\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)<br>\\n- Consensus assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)</li>\\n<li class=\"has-line-data\" data-line-start=\"33\" data-line-end=\"43\"><em>De novo</em> assembly<br>\\ni. Primer trimming (<a href=\"https://cutadapt.readthedocs.io/en/stable/guide.html\"><code>Cutadapt</code></a>; <em>amplicon data only</em>)<br>\\nii. Removal of host reads (<a href=\"http://ccb.jhu.edu/software/kraken2/\"><code>Kraken 2</code></a>)<br>\\niii. Choice of multiple assembly tools (<a href=\"http://cab.spbu.ru/software/spades/\"><code>SPAdes</code></a> <em>||</em> <a href=\"http://cab.spbu.ru/software/meta-spades/\"><code>metaSPAdes</code></a> <em>||</em> <a href=\"https://github.com/rrwick/Unicycler\"><code>Unicycler</code></a> <em>||</em> <a href=\"https://github.com/GATB/minia\"><code>minia</code></a>)<br>\\n- Blast to reference genome (<a href=\"https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastSearch\"><code>blastn</code></a>)<br>\\n- Contiguate assembly (<a href=\"https://www.sanger.ac.uk/science/tools/pagit\"><code>ABACAS</code></a>)<br>\\n- Assembly report (<a href=\"https://github.com/BU-ISCIII/plasmidID\"><code>PlasmidID</code></a>)<br>\\n- Assembly assessment report (<a href=\"http://quast.sourceforge.net/quast\"><code>QUAST</code></a>)<br>\\n- Call variants relative to reference (<a href=\"https://github.com/lh3/minimap2\"><code>Minimap2</code></a>, <a href=\"https://github.com/ekg/seqwish\"><code>seqwish</code></a>, <a href=\"https://github.com/vgteam/vg\"><code>vg</code></a>, <a href=\"https://github.com/rrwick/Bandage\"><code>Bandage</code></a>)<br>\\n- Variant annotation (<a href=\"http://snpeff.sourceforge.net/SnpEff.html\"><code>SnpEff</code></a>, <a href=\"http://snpeff.sourceforge.net/SnpSift.html\"><code>SnpSift</code></a>)</li>\\n<li class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"45\">Present QC and visualisation for raw read, alignment, assembly and variant calling results (<a href=\"http://multiqc.info/\"><code>MultiQC</code></a>)</li>\\n</ol>\\n<h2 class=\"code-line\" data-line-start=45 data-line-end=46><a id=\"Quick_Start_45\"></a>Quick Start</h2>\\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">i. Install <a href=\"https://nf-co.re/usage/installation\"><code>nextflow</code></a></p>\\n<p class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\">ii. Install either <a href=\"https://docs.docker.com/engine/installation/\"><code>Docker</code></a> or <a href=\"https://www.sylabs.io/guides/3.0/user-guide/\"><code>Singularity</code></a> for full pipeline reproducibility (please only use <a href=\"https://conda.io/miniconda.html\"><code>Conda</code></a> as a last resort; see <a href=\"https://nf-co.re/usage/configuration#basic-configuration-profiles\">docs</a>)</p>\\n<p class=\"has-line-data\" data-line-start=\"51\" data-line-end=\"52\">iii. Download the pipeline and test it on a minimal dataset with a single command</p>\\n<pre><code class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"56\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile <span class=\"hljs-built_in\">test</span>,&lt;docker/singularity/conda/institute&gt;\\n</code></pre>\\n<blockquote>\\n<p class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Please check <a href=\"https://github.com/nf-core/configs#documentation\">nf-core/configs</a> to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use <code>-profile &lt;institute&gt;</code> in your command. This will enable either <code>docker</code> or <code>singularity</code> and set the appropriate execution settings for your local compute environment.</p>\\n</blockquote>\\n<p class=\"has-line-data\" data-line-start=\"59\" data-line-end=\"60\">iv. Start running your own analysis!</p>\\n<pre><code class=\"has-line-data\" data-line-start=\"62\" data-line-end=\"64\" class=\"language-bash\">nextflow run nf-core/viralrecon -profile &lt;docker/singularity/conda/institute&gt; --input samplesheet.csv --genome <span class=\"hljs-string\">\\'NC_045512.2\\'</span> -profile docker\\n</code></pre>\\n<p class=\"has-line-data\" data-line-start=\"65\" data-line-end=\"66\">See <a href=\"docs/usage.md\">usage docs</a> for all of the available options when running the pipeline.</p>\\n<h2 class=\"code-line\" data-line-start=67 data-line-end=68><a id=\"Documentation_67\"></a>Documentation</h2>\\n<p class=\"has-line-data\" data-line-start=\"69\" data-line-end=\"70\">The nf-core/viralrecon pipeline comes with documentation about the pipeline, found in the <code>docs/</code> directory:</p>\\n<ol>\\n<li class=\"has-line-data\" data-line-start=\"71\" data-line-end=\"72\"><a href=\"https://nf-co.re/usage/installation\">Installation</a></li>\\n<li class=\"has-line-data\" data-line-start=\"72\" data-line-end=\"76\">Pipeline configuration\\n<ul>\\n<li class=\"has-line-data\" data-line-start=\"73\" data-line-end=\"74\"><a href=\"https://nf-co.re/usage/local_installation\">Local installation</a></li>\\n<li class=\"has-line-data\" data-line-start=\"74\" data-line-end=\"75\"><a href=\"https://nf-co.re/usage/adding_own_config\">Adding your own system config</a></li>\\n<li class=\"has-line-data\" data-line-start=\"75\" data-line-end=\"76\"><a href=\"docs/usage.md#reference-genomes\">Reference genomes</a></li>\\n</ul>\\n</li>\\n<li class=\"has-line-data\" data-line-start=\"76\" data-line-end=\"77\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\\n<li class=\"has-line-data\" data-line-start=\"77\" data-line-end=\"78\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\\n<li class=\"has-line-data\" data-line-start=\"78\" data-line-end=\"80\"><a href=\"https://nf-co.re/usage/troubleshooting\">Troubleshooting</a></li>\\n</ol>\\n<h2 class=\"code-line\" data-line-start=80 data-line-end=81><a id=\"Credits_80\"></a>Credits</h2>\\n<p class=\"has-line-data\" data-line-start=\"82\" data-line-end=\"83\">These scripts were originally written by <a href=\"https://github.com/svarona\">Sarai Varona</a>, <a href=\"https://github.com/MiguelJulia\">Miguel Juliá</a> and <a href=\"https://github.com/saramonzon\">Sara Monzon</a> from <a href=\"https://github.com/BU-ISCIII\">BU-ISCIII</a> and co-ordinated by Isabel Cuesta for the <a href=\"https://eng.isciii.es/eng.isciii.es/Paginas/Inicio.html\">Institute of Health Carlos III</a>, Spain. Through collaboration with the nf-core community the pipeline has now been updated substantially to include additional processing steps, to standardise inputs/outputs and to improve pipeline reporting; implemented primarily by <a href=\"https://github.com/drpatelh\">Harshil Patel</a> from <a href=\"https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/\">The Bioinformatics &amp; Biostatistics Group</a> at <a href=\"https://www.crick.ac.uk/\">The Francis Crick Institute</a>, London.</p>\\n<p class=\"has-line-data\" data-line-start=\"84\" data-line-end=\"85\">Many thanks to others who have helped out and contributed along the way too, including (but not limited to):</p>\\n<table class=\"table table-striped table-bordered\">\\n<thead>\\n<tr>\\n<th>Name</th>\\n<th>Affiliation</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td><a href=\"https://github.com/apeltzer\">Alexander Peltzer</a></td>\\n<td><a href=\"https://www.boehringer-ingelheim.de/\">Boehringer Ingelheim, Germany</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/ameynert\">Alison Meynert</a></td>\\n<td><a href=\"https://www.ed.ac.uk/\">University of Edinburgh, Scotland</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/edgano\">Edgar Garriga Nogales</a></td>\\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/ekg\">Erik Garrison</a></td>\\n<td><a href=\"https://www.ucsc.edu/\">UCSC, USA</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/ggabernet\">Gisela Gabernet</a></td>\\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of Tübingen, Germany</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/jcurado-flomics\">Joao Curado</a></td>\\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/JoseEspinosa\">Jose Espinosa-Carrasco</a></td>\\n<td><a href=\"https://www.crg.eu/\">Centre for Genomic Regulation, Spain</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/ktrns\">Katrin Sameith</a></td>\\n<td><a href=\"https://genomecenter.tu-dresden.de\">DRESDEN-concept Genome Center, Germany</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/lcabus-flomics\">Lluc Cabus</a></td>\\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/mpozuelo-flomics\">Marta Pozuelo</a></td>\\n<td><a href=\"https://www.flomics.com/\">Flomics Biotech, Spain</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/MaxUlysse\">Maxime Garcia</a></td>\\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/heuermh\">Michael Heuer</a></td>\\n<td><a href=\"https://https://rise.cs.berkeley.edu\">UC Berkeley, USA</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/ewels\">Phil Ewels</a></td>\\n<td><a href=\"https://www.scilifelab.se/\">SciLifeLab, Sweden</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/subwaystation\">Simon Heumos</a></td>\\n<td><a href=\"https://portal.qbic.uni-tuebingen.de/portal/\">QBiC, University of Tübingen, Germany</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/stevekm\">Stephen Kelly</a></td>\\n<td><a href=\"https://www.mskcc.org/\">Memorial Sloan Kettering Cancer Center, USA</a></td>\\n</tr>\\n<tr>\\n<td><a href=\"https://github.com/thanhleviet\">Thanh Le Viet</a></td>\\n<td><a href=\"https://quadram.ac.uk/\">Quadram Institute, UK</a></td>\\n</tr>\\n</tbody>\\n</table>\\n<blockquote>\\n<p class=\"has-line-data\" data-line-start=\"105\" data-line-end=\"106\">Listed in alphabetical order</p>\\n</blockquote>\\n<h2 class=\"code-line\" data-line-start=107 data-line-end=108><a id=\"Contributions_and_Support_107\"></a>Contributions and Support</h2>\\n<p class=\"has-line-data\" data-line-start=\"109\" data-line-end=\"110\">If you would like to contribute to this pipeline, please see the <a href=\"https://github.com/nf-core/viralrecon/blob/master/.github/CONTRIBUTING.md\">contributing guidelines</a>.</p>\\n<p class=\"has-line-data\" data-line-start=\"111\" data-line-end=\"112\">For further information or help, don’t hesitate to get in touch on <a href=\"https://nfcore.slack.com/channels/viralrecon\">Slack</a> (you can join with <a href=\"https://nf-co.re/join/slack\">this invite</a>).</p>\\n<h2 class=\"code-line\" data-line-start=113 data-line-end=114><a id=\"Citation_113\"></a>Citation</h2>\\n<p class=\"has-line-data\" data-line-start=\"115\" data-line-end=\"116\">If you use nf-core/viralrecon for your analysis, please cite it using the following doi: <a href=\"https://doi.org/10.5281/zenodo.3872730\">10.5281/zenodo.3872730</a></p>\\n<p class=\"has-line-data\" data-line-start=\"117\" data-line-end=\"118\">An extensive list of references for the tools used by the pipeline can be found in the <a href=\"https://github.com/nf-core/viralrecon/blob/master/CITATIONS.md\"><code>CITATIONS.md</code></a> file.</p>\\n<p class=\"has-line-data\" data-line-start=\"119\" data-line-end=\"120\">You can cite the <code>nf-core</code> publication as follows:</p>\\n<blockquote>\\n<p class=\"has-line-data\" data-line-start=\"121\" data-line-end=\"122\"><strong>The nf-core framework for community-curated bioinformatics pipelines.</strong></p>\\n<p class=\"has-line-data\" data-line-start=\"123\" data-line-end=\"124\">Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso &amp; Sven Nahnsen.</p>\\n<p class=\"has-line-data\" data-line-start=\"125\" data-line-end=\"127\"><em>Nat Biotechnol.</em> 2020 Feb 13. doi: <a href=\"https://dx.doi.org/10.1038/s41587-020-0439-x\">10.1038/s41587-020-0439-x</a>.<br>\\nReadCube: <a href=\"https://rdcu.be/b1GjZ\">Full Access Link</a></p>\\n</blockquote>\\n</body></html>'\n",
      " '<!DOCTYPE html><html><head><meta charset=\"utf-8\"><style></style></head><body id=\"preview\">\\n<h1 class=\"code-line\" data-line-start=0 data-line-end=1><a id=\"nfcoreviprhttpsrawgithubusercontentcomnfcoreviprmasterdocsimagesvipr_logosvg_0\"></a><img src=\"https://raw.githubusercontent.com/nf-core/vipr/master/docs/images/vipr_logo.png\" alt=\"nf-core/vipr\" height=\"200\" width=\"500\"></h1>\\n<p class=\"has-line-data\" data-line-start=\"2\" data-line-end=\"3\"><a href=\"https://travis-ci.org/nf-core/vipr\"><img src=\"https://travis-ci.org/nf-core/vipr.svg?branch=master\" alt=\"Build Status\"></a> <a href=\"https://www.nextflow.io/\"><img src=\"https://img.shields.io/badge/nextflow-%E2%89%A50.31.1-brightgreen.svg\" alt=\"Nextflow\"></a> <a href=\"https://gitter.im/nf-core/Lobby\"><img src=\"https://img.shields.io/badge/gitter-%20join%20chat%20%E2%86%92-4fb99a.svg\" alt=\"Gitter\"></a></p>\\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><a href=\"http://bioconda.github.io/\"><img src=\"https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg\" alt=\"install with bioconda\"></a> <a href=\"https://hub.docker.com/r/nfcore/vipr/\"><img src=\"https://img.shields.io/docker/automated/nfcore/vipr.svg\" alt=\"Docker Container available\"></a> <a href=\"https://singularity-hub.org/collections/1405\"><img src=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\" alt=\"https://www.singularity-hub.org/static/img/hosted-singularity--hub-%23e32929.svg\"></a></p>\\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\"><strong>nf-core/vipr</strong> is a bioinformatics best-practice analysis pipeline for assembly and intrahost / low-frequency variant calling for viral samples.</p>\\n<p class=\"has-line-data\" data-line-start=\"18\" data-line-end=\"19\">The pipeline is built using <a href=\"https://www.nextflow.io\">Nextflow</a>, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.</p>\\n<h3 class=\"code-line\" data-line-start=20 data-line-end=21><a id=\"Pipeline_Steps_20\"></a>Pipeline Steps</h3>\\n<table class=\"table table-striped table-bordered\">\\n<thead>\\n<tr>\\n<th>Step</th>\\n<th>Main program/s</th>\\n</tr>\\n</thead>\\n<tbody>\\n<tr>\\n<td>Trimming, combining of read-pairs per sample and QC</td>\\n<td>Skewer, FastQC</td>\\n</tr>\\n<tr>\\n<td>Decontamination</td>\\n<td>decont</td>\\n</tr>\\n<tr>\\n<td>Metagenomics classification / Sample purity</td>\\n<td>Kraken</td>\\n</tr>\\n<tr>\\n<td>Assembly to contigs</td>\\n<td>BBtools’ Tadpole</td>\\n</tr>\\n<tr>\\n<td>Assembly polishing</td>\\n<td>ViPR Tools</td>\\n</tr>\\n<tr>\\n<td>Mapping to assembly</td>\\n<td>BWA, LoFreq</td>\\n</tr>\\n<tr>\\n<td>Low frequency variant calling</td>\\n<td>LoFreq</td>\\n</tr>\\n<tr>\\n<td>Coverage and variant AF plots (two processes)</td>\\n<td>Bedtools, ViPR Tools</td>\\n</tr>\\n</tbody>\\n</table>\\n<h3 class=\"code-line\" data-line-start=33 data-line-end=34><a id=\"Documentation_33\"></a>Documentation</h3>\\n<p class=\"has-line-data\" data-line-start=\"35\" data-line-end=\"36\">Documentation about the pipeline can be found in the <code>docs/</code> directory:</p>\\n<ol>\\n<li class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><a href=\"docs/installation.md\">Installation and configuration</a></li>\\n<li class=\"has-line-data\" data-line-start=\"38\" data-line-end=\"39\"><a href=\"docs/usage.md\">Running the pipeline</a></li>\\n<li class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"41\"><a href=\"docs/output.md\">Output and how to interpret the results</a></li>\\n</ol>\\n<h3 class=\"code-line\" data-line-start=41 data-line-end=42><a id=\"Credits_41\"></a>Credits</h3>\\n<p class=\"has-line-data\" data-line-start=\"43\" data-line-end=\"46\">This pipeline was originally developed by Andreas Wilm (<a href=\"https://github.com/andreas-wilm\">andreas-wilm</a>) at <a href=\"https://www.a-star.edu.sg/gis/\">Genome Institute of Singapore</a>.<br>\\nIt started out as an ecosystem around LoFreq and went through a couple of iterations.<br>\\nThe current version had three predecessors <a href=\"https://github.com/CSB5/vipr\">ViPR 1</a>, <a href=\"https://github.com/CSB5/vipr2\">ViPR 2</a> and <a href=\"https://github.com/gis-rpd/pipelines/tree/master/germs/vipr\">ViPR 3</a>.</p>\\n<p class=\"has-line-data\" data-line-start=\"47\" data-line-end=\"48\">An incomplete list of publications using (previous versions of) ViPR:</p>\\n<ul>\\n<li class=\"has-line-data\" data-line-start=\"49\" data-line-end=\"50\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26327586\">Sessions et. al., PLoS Negl Trop Dis., 2015</a></li>\\n<li class=\"has-line-data\" data-line-start=\"50\" data-line-end=\"52\"><a href=\"https://www.ncbi.nlm.nih.gov/pubmed/26325059\">Sim et al., PLoS Negl Trop Dis., 2015</a></li>\\n</ul>\\n<p class=\"has-line-data\" data-line-start=\"52\" data-line-end=\"53\">Plenty of people provided essential feedback, including:</p>\\n<ul>\\n<li class=\"has-line-data\" data-line-start=\"54\" data-line-end=\"55\"><a href=\"https://www.duke-nus.edu.sg/content/sessions-october\">October SESSIONS</a></li>\\n<li class=\"has-line-data\" data-line-start=\"55\" data-line-end=\"56\"><a href=\"https://www.a-star.edu.sg/gis/Our-People/Platform-Leaders\">Paola Florez DE SESSIONS</a></li>\\n<li class=\"has-line-data\" data-line-start=\"56\" data-line-end=\"57\">ZHU Yuan</li>\\n<li class=\"has-line-data\" data-line-start=\"57\" data-line-end=\"58\">Shuzhen SIM</li>\\n<li class=\"has-line-data\" data-line-start=\"58\" data-line-end=\"59\">CHU Wenhan Collins</li>\\n</ul>\\n</body></html>'\n",
      " 'Given a set of pathways generated by RetroPath2.0, this workflow informs the user as to the theoretically best performing ones based on four criteria: FBA, thermodynamic feasibility, length of the pathway, and reaction rule score.'\n",
      " 'This workflow converts the top-ranking predicted pathways from the \"RetroSynthesis\" and \"Pathway Analysis\" workflows to plasmids intended to be expressed in the specified organism'\n",
      " 'Generate possible metabolic routes for the production of a target molecule in an organism of choice'\n",
      " 'The workflow runs the RetroSynthesis algorithm to generate a collection of heterologous pathways in a host organism of choice, converts them to SBML files, performs analysis on the pathways to then rank the theoretical best performing ones.'\n",
      " '<p class=\"has-line-data\" data-line-start=\"0\" data-line-end=\"1\"><img src=\"https://img.shields.io/badge/CWL-1.2.0--dev2-green\" alt=\"\"> <img src=\"https://img.shields.io/badge/nextflow-20.01.0-brightgreen\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-docker-blue.svg\" alt=\"\"> <img src=\"https://img.shields.io/badge/uses-conda-yellow.svg\" alt=\"\"> <img src=\"https://api.travis-ci.org/EBI-Metagenomics/emg-viral-pipeline.svg\" alt=\"\"></p>\\n<h1 class=\"code-line\" data-line-start=2 data-line-end=3 ><a id=\"VIRify_2\"></a>VIRify</h1>\\n<p class=\"has-line-data\" data-line-start=\"4\" data-line-end=\"5\"><img width=\"500px\" src=\"https://raw.githubusercontent.com/EBI-Metagenomics/emg-viral-pipeline/master/nextflow/figures/sankey.png\" alt=\"Sankey plot\"></p>\\n<p class=\"has-line-data\" data-line-start=\"6\" data-line-end=\"7\">VIRify is a recently developed pipeline for the detection, annotation, and taxonomic classification of viral contigs in metagenomic and metatranscriptomic assemblies. The pipeline is part of the repertoire of analysis services offered by <a href=\"https://www.ebi.ac.uk/metagenomics/\">MGnify</a>. VIRify’s taxonomic classification relies on the detection of taxon-specific profile hidden Markov models (HMMs), built upon a set of 22,014 orthologous protein domains and referred to as ViPhOGs.</p>\\n<p class=\"has-line-data\" data-line-start=\"8\" data-line-end=\"9\">VIRify was implemented in CWL.</p>\\n<h2 class=\"code-line\" data-line-start=10 data-line-end=11 ><a id=\"What_do_I_need_10\"></a>What do I need?</h2>\\n<p class=\"has-line-data\" data-line-start=\"12\" data-line-end=\"13\">The current implementation uses CWL version 1.2 dev+2. It was tested using Toil version 4.10 as the workflow engine and conda to manage the software dependencies.</p>\\n<h3 class=\"code-line\" data-line-start=14 data-line-end=15 ><a id=\"Docker__Singularity_support_14\"></a>Docker - Singularity support</h3>\\n<p class=\"has-line-data\" data-line-start=\"16\" data-line-end=\"17\">Soon…</p>\\n<h2 class=\"code-line\" data-line-start=18 data-line-end=19 ><a id=\"Setup_environment_18\"></a>Setup environment</h2>\\n<pre><code class=\"has-line-data\" data-line-start=\"21\" data-line-end=\"24\" class=\"language-bash\">conda env create <span class=\"hljs-operator\">-f</span> cwl/requirements/conda_env.yml\\nconda activate viral_pipeline\\n</code></pre>\\n<h2 class=\"code-line\" data-line-start=25 data-line-end=26 ><a id=\"Basic_execution_25\"></a>Basic execution</h2>\\n<pre><code class=\"has-line-data\" data-line-start=\"28\" data-line-end=\"31\" class=\"language-bash\"><span class=\"hljs-built_in\">cd</span> cwl/\\nvirify.sh -h\\n</code></pre>\\n<h1 class=\"code-line\" data-line-start=32 data-line-end=33 ><a id=\"A_note_about_metatranscriptomes_32\"></a>A note about metatranscriptomes</h1>\\n<p class=\"has-line-data\" data-line-start=\"34\" data-line-end=\"36\">Although VIRify has been benchmarked and validated with metagenomic data in mind, it is also possible to use this tool to detect RNA viruses in metatranscriptome assemblies (e.g. SARS-CoV-2). However, some additional considerations for this purpose are outlined below:<br>\\n<strong>1. Quality control:</strong> As for metagenomic data, a thorough quality control of the FASTQ sequence reads to remove low-quality bases, adapters and host contamination (if appropriate) is required prior to assembly. This is especially important for metatranscriptomes as small errors can further decrease the quality and contiguity of the assembly obtained. We have used <a href=\"https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/\">TrimGalore</a> for this purpose.</p>\\n<p class=\"has-line-data\" data-line-start=\"37\" data-line-end=\"38\"><strong>2. Assembly:</strong> There are many assemblers available that are appropriate for either metagenomic or single-species transcriptomic data. However, to our knowledge, there is no assembler currently available specifically for metatranscriptomic data. From our preliminary investigations, we have found that transcriptome-specific assemblers (e.g. <a href=\"http://cab.spbu.ru/software/spades/\">rnaSPAdes</a>) generate more contiguous and complete metatranscriptome assemblies compared to metagenomic alternatives (e.g. <a href=\"https://github.com/voutcn/megahit/releases\">MEGAHIT</a> and <a href=\"http://cab.spbu.ru/software/spades/\">metaSPAdes</a>).</p>\\n<p class=\"has-line-data\" data-line-start=\"39\" data-line-end=\"40\"><strong>3. Post-processing:</strong> Metatranscriptomes generate highly fragmented assemblies. Therefore, filtering contigs based on a set minimum length has a substantial impact in the number of contigs processed in VIRify. It has also been observed that the number of false-positive detections of <a href=\"https://github.com/jessieren/VirFinder/releases\">VirFinder</a> (one of the tools included in VIRify) is lower among larger contigs. The choice of a length threshold will depend on the complexity of the sample and the sequencing technology used, but in our experience any contigs &lt;2 kb should be analysed with caution.</p>\\n<p class=\"has-line-data\" data-line-start=\"41\" data-line-end=\"42\"><strong>4. Classification:</strong> The classification module of VIRify depends on the presence of a minimum number and proportion of phylogenetically-informative genes within each contig in order to confidently assign a taxonomic lineage. Therefore, short contigs typically obtained from metatranscriptome assemblies remain generally unclassified. For targeted classification of RNA viruses (for instance, to search for Coronavirus-related sequences), alternative DNA- or protein-based classification methods can be used. Two of the possible options are: (i) using <a href=\"https://github.com/marbl/MashMap/releases\">MashMap</a> to screen the VIRify contigs against a database of RNA viruses (e.g. Coronaviridae) or (ii) using <a href=\"http://hmmer.org/download.html\">hmmsearch</a> to screen the proteins obtained in the VIRify contigs against marker genes of the taxon of interest.</p>\\n<h2>Contact us</h2>\\n<a href=\"https://www.ebi.ac.uk/support/metagenomics\">MGnify helpdesk</a>'\n",
      " 'Analysis of variation within individual COVID-19 samples \\nusing bowtie2, bwa, fastp, multiqc , picard ,samtools, snpEff \\nWorkflow, tools and data are available on https://github.com/fjrmoreews/cwl-workflow-SARS-CoV-2/tree/master/Variation\\nThis worklow was ported into CWL from a  Galaxy Workflow \\n  ( https://github.com/galaxyproject/SARS-CoV-2/tree/master/genomics/4-Variation migrated to CWL).\\n\\n'\n",
      " 'Common Workflow Language example that illustrate the process of setting up a simulation system containing a protein, step by step, using the [BioExcel Building Blocks](/projects/11) library (biobb). The particular example used is the Lysozyme protein (PDB code 1AKI). This workflow returns a resulting protein structure and simulated 3D trajectories.'\n",
      " '\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com & ambari73_sit@jnu.ac.in\\n\\nThis is a proposed standard operating procedure for genomic variant detection using GATK4.\\n\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\n\\n\\n\\nIt uses Illumina RNASEQ reads and genome sequence.\\n'\n",
      " '\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com; ambari73_sit@jnu.ac.in\\n\\nThis is a proposed standard operating procedure for genomic variant detection using VARSCAN.\\n\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\n\\n\\n\\nIt uses Illumina RNASEQ reads and genome sequence.\\n'\n",
      " '\\nAuthor: AMBARISH KUMAR er.ambarish@gmail.com; ambari73_sit@jnu.ac.in\\n\\nThis is a proposed standard operating procedure for genomic variant detection using SAMTools.\\n\\nIt is hoped to be effective and useful for getting SARS-CoV-2 genome variants.\\n\\n\\n\\nIt uses Illumina RNASEQ reads and genome sequence.\\n'\n",
      " 'Detects SNPs and INDELs.' 'Detects SNPs and INDELs using VARSCAN2.'\n",
      " 'Alignment, assembly and annotation of RNQSEQ reads using TOPHAT  (without filtering out host reads).'\n",
      " 'Alignment, assembly RNASEQ reads and annotation of generated transcripts.'\n",
      " 'Alignment, assembly and annotation of RNASEQ reads as well as annotation of generated transcripts.'\n",
      " 'Alignment, assembly and annotation of generated transcripts from RNASEQ reads.'\n",
      " 'This workflow has been created as part of Demonstrator 6 of the project EOSC-Life (within WP3) and is focused on reusing publicly available RNAi screens to gain insights into the nucleolus biology. The workflow downloads images from the Image Data Resource (IDR), performs object segmentation (of nuclei and nucleoli) and feature extraction of the images and objects identified.\\n\\nTutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/tutorial-CP/tutorial.html'\n",
      " 'The tutorial for this workflow can be found on [Galaxy Training Network](https://training.galaxyproject.org/training-material/topics/climate/tutorials/climate-101/tutorial.html)'\n",
      " 'CWL workflow for NMR spectra Peak Picking\\nThe workflow takes as input a series of 2D 1H 15N HSQC NMR spectra and uses nmrpipe tools to convert the spectra in nmrpipe format and performs an automatic peak picking.\\nThis test uses a protein MDM2 with different ligands and peptide and generates a peak list with 1H and 15N chemical shift values for each spectrum. The difference among these peak lists can be used to characterize the ligand binding site on the protein.'\n",
      " 'RNA sequencing analysis pipeline for gene/isoform quantification and extensive quality control.'\n",
      " 'Amplicon analysis workflow using NG-Tax\\n\\n**Steps:**\\n\\n* Quality control on the reads\\n* Execute NGTax for ASV detection and classification\\n\\nFor more information about NG-Tax 2.0 have a look at https://doi.org/10.3389/fgene.2019.01366'\n",
      " 'Abstract CWL Automatically generated from the Galaxy workflow file: Workflow with Copernicus Essential Climate Variable - select and plot'\n",
      " 'Description: SSP-based RCP scenario with high radiative forcing by the end of century. Following approximately RCP8.5 global forcing pathway with SSP5 socioeconomic conditions. Concentration-driven.\\nRationale: the scenario represents the high end of plausible future pathways. SSP5 is the only SSP with emissions high enough to produce the 8.5 W/m2 level of forcing in 2100.\\n\\nThis workflow is answering to the following scientific question:\\n- Is it worth investing in artificial snowmaking equipment at RATECE-PLANICA?'\n",
      " 'Galaxy-E (ecology.usegalaxy.eu) workflow to calculate species presence / absence, community metrics and compute generalized linear models to identify effects and significativity of these effects on biodiversity.'\n",
      " 'Basic workflows inspired by the Nanopolish tutorials'\n",
      " 'Genome assembly: Flye-based WF for highly repetitive genomes [Schmid et al. NAR 2018]'\n",
      " 'Genome assembly: Unicycler-based WF for Klebsiella pneumoniae [Wick et al. Microbial genomics 2017]'\n",
      " 'Metagenomics: taxa classification'\n",
      " '# Automatic Ligand parameterization tutorial using BioExcel Building Blocks (biobb)\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **ligand parameterization** for a **small molecule**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Sulfasalazine** protein (3-letter code SAS), used to treat rheumatoid arthritis, ulcerative colitis, and Crohn\\'s disease.\\n\\n**OpenBabel and ACPype** packages are used to **add hydrogens, energetically minimize the structure**, and **generate parameters** for the **GROMACS** package. With *Generalized Amber Force Field (GAFF) forcefield and AM1-BCC* charges.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Mutation Free Energy Calculations using BioExcel Building Blocks (biobb)\\n\\n***\\n\\n**Based on the official [pmx tutorial](http://pmx.mpibpc.mpg.de/sardinia2018_tutorial1/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate how to compute a **fast-growth mutation free energy** calculation, step by step, using the BioExcel **Building Blocks library (biobb)**. The particular example used is the **Staphylococcal nuclease** protein (PDB code 1STN), a small, minimal protein, appropriate for a short tutorial.\\n\\nThe **non-equilibrium free energy calculation** protocol performs a **fast alchemical transition** in the direction **WT->Mut** and back **Mut->WT**. The two equilibrium trajectories needed for the tutorial, one for **Wild Type (WT)** and another for the **Mutated (Mut)** protein (Isoleucine 10 to Alanine -I10A-), have already been generated and are included in this example. We will name **WT as stateA** and **Mut as stateB**.\\n\\n![](https://raw.githubusercontent.com/bioexcel/biobb_wf_pmx_tutorial/master/biobb_wf_pmx_tutorial/notebooks/schema.png)\\n\\nThe tutorial calculates the **free energy difference** in the folded state of a protein. Starting from **two 1ns-length independent equilibrium simulations** (WT and mutant), snapshots are selected to start **fast (50ps) transitions** driving the system in the **forward** (WT to mutant) and **reverse** (mutant to WT) directions, and the **work values** required to perform these transitions are collected. With these values, **Crooks Gaussian Intersection** (CGI), **Bennett Acceptance Ratio** (BAR) and **Jarzynski estimator** methods are used to calculate the **free energy difference** between the two states.\\n\\n*Please note that for the sake of disk space this tutorial is using 1ns-length equilibrium trajectories, whereas in the [original example](http://pmx.mpibpc.mpg.de/sardinia2018_tutorial1/eq.mdp) the equilibrium trajectories used were obtained from 10ns-length simulations.*\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Protein Ligand Complex MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/complex/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation system** containing a **protein in complex with a ligand**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **T4 lysozyme** L99A/M102Q protein (PDB code 3HTB), in complex with the **2-propylphenol** small molecule (3-letter Code JZ4). \\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'Pre-processing of mass spectrometry-based metabolomics data'\n",
      " '# RNA-Seq pipeline\\nHere we provide the tools to perform paired end or single read RNA-Seq analysis including raw data quality control, differential expression (DE) analysis and functional annotation. As input files you may use either zipped fastq-files (.fastq.gz) or mapped read data (.bam files). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes.\\n\\n\\n## Pipeline Workflow\\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_RNAseq_pipeline.html#R7R1Zk5s489e4Knmwi%2Ft4nDOz2WSyO5OtbPYlJZCw%2BYLBATzXr%2F90AOYQNtgY8Ex2UjuDACG1Wn13ayJfLJ8%2BhGC1%2BBxA5E0kAT5N5MuJJImKJE3IPwE%2BsxZd11nDPHRh8tCm4d59QUmjkLSuXYiiwoNxEHixuyo22oHvIzsutIEwDB6LjzmBV%2FzqCsxRpeHeBl619ZsL40XSKmrm5sYNcueL5NOGlMzPAvbPeRis%2FeR7fuAjdmcJ0m6SOUYLAIPHXJN8NZEvwiCI2V%2FLpwvkEbCmEGPvXdfczYYcIj9u8sLiRdX%2F%2FXT3%2FOR9Xj7NbwX7r3%2FtaTa4%2BDmFBYIYNMllEMaLYB74wLvatJ7T%2BSLSrYCv%2FrdertLn52CFWzZvfQoC3HApksdQHD8nKw%2FWcYCbFvHSS%2B6iJzf%2Bl3Q4U5Or77k7l0%2FJt%2BjFc3rhx%2BFz7iVy%2BT1%2Fb%2FMavUrfi%2BIw%2BJmtM16H8yooE%2BhGwTq0E7g82Ld%2F339bf1xfvvz39Y%2Bv19%2Bjh%2F%2BmcoIgMQjnKN7yoJIAmkA394lkqT6gYInwGPEDIfJA7D4UERMk%2BD3PntssNP4jWWv%2But8IH53FrRF7D4s76%2Fbm%2FvrHy8003XgPwFsnn5pImodncA7dB%2FJFz5379Ib2a01Q9Dyk6J9d4r%2FmyW%2F6mhWWW%2FDAaF9pawnPcqvvAQt559lWugi8IKQPydf0P%2B6iOYEfJ%2FgkamTcIFpQpBQLKEquHNfz8p1K5CfrNL1D9658Pg8BdDEilJrtYOnaCQbNPRBFKTal21qgn43x4gUEcFNT2IZZDyiM0dNWTEjuGmpCaxLaOjU1dv2Yo1RGQoAXOSKlpIt8CPZwkVl80%2BhzpZGfY6BPB9giGUIBWzIWm8MWRTCr2KJ1gS0v3%2BLLr8K%2F%2F328vrsKlyL8%2BuH5j6l8MIt5SwylIT%2BRlaH4CXeNVfG1LzLy4RmRNPGlTfYv2c2k8dr10uFU8aBAVHpACnkopBDlx9sb9zE4%2B3O%2B%2BvLjW%2FDxXBGnEodJ9I8kXJAcdx0S%2FtgD3LcNk8OcCdMrLEDKjsmNaURBeIYfEJXVE4dXX0iTs%2FMlcH0CP3eFPBfzNtx4vp2Ns68Wm5mYsO9AuplO0ku0An5FHCk3ALLublz6TuiDCP2apbCYYQEkeHjOfypdqbQFqMCWFc1GpiVA3QKCaiqm4uiOo5qaIAlT2wRQRbbqiFDSZR0auqBrjgUdVRUECymKAnUgI6vwkUWInMJnFnFMVOczgkjS9dyNF2trhmUQfOEuLSfACIz%2FvP1wT4YuzQN8YXmBRdANRDHCq3idTirCf9%2FdnuF54j92T5iBrfa5FCcAB08qQOdiE2%2B5apCsLEKOBvWmlCK5MbDw27m9JAkQOa7vUh1CEt5Z5M771ptr1PP2AgAjSgujFbIpNZQEOlFiNQng2iMod%2FrzQ1GEGYwLCLkGPswmNyXTdh0sP1AmBDDNJ%2Fttjwl3gINPyF7HBN45HKwZSb3%2B1YXaYooFtcWoai2yztFaROlYOq6yW3jButuK%2FInp6gpreWTyTBZJRUAjbchshwoXXFu5eGMYihWQpcDJQyxtayyKJN%2F7K3AptiUfm0pqYcH0YgeB40RYRCovQjboA2wP5pCaRvr394LWsUvTKOgZG7WjB3VSa6o5DKU4bBt1rXWpSvGs4IkQPNefM5JnBSFE4RQ34wsmIyX7ghJEAW8CNbuTN%2BKT2xNJRgr5yZ5YAQizvqVt%2FKMiw%2FDEFb4waWNyG2LpK5rFTwVLWVmGhKYGRV2Fmg5EYAiKaIgSFiNFxZIMgGQw1R1bMETTAVARJB0CSVJkU9ZsQVRtLGealiqogoNFxkFkyNp5MiCVb2%2BTGGtkwcoi8PlnKn5V0akeV8qsFS0LQLQxduNB2RRZch0KlacylMo9JdXjVWwF8LnSWBEIY1hucdMGvGD405dXVBwB3nPk4lXB%2B1OYMxGMeq6EFPoE6ckSCkwmwM9pYEmcO74VrdgssFQ%2FY1D488tD9O0rnsq7P79Mv319P8vB3eWsRXWU1RausFt9jDRWodAGLh%2Fwzl%2FhSfhYEosYNcM7zs%2BoZhkPxzqhhv2%2FkYGRRt6WIe1s29fItUTecm3gnSUejJhICOepP8NDDnkrwE85HjUGOtT%2Bl5cVHhdujO4xESA9PoZgtY2DtxCRi4Z9WZQ48p5YFfiMo3mB1N%2BSWHNJLF2u0Yhi%2F0Qo%2FGL9j4RTYCpPXGhvR9QqUvZaQUu1DV3XHMlxREUxVUeVbRGopmTqjgMdxZqKGJUtyTItLFxZQFZMXVRUJBkOhIajiwKybSQpUB1G0KqZJQMQn7n9FrJaCVkRlom8zGSyN6Nzy%2F1RYaS1zNGaQ2Pe1eHgQwSgjbc2oSekZzYHISd3vlv77q81xYa1NcXUlawNIReU2cDg%2FbYpH0tMmTP5rysozHPiJJGqHTLbApr0ObkQrTwszcSdLnPWJaGXR5pUO%2Fltu5wi5RhoIticoMCXBSPuEvjEPYQBfJmTB1oIgZLxWwhsLgSmRuSTEQJXXM69xMPHGjJlzsJqI%2Fex9indQeSelruHkT6eJrvqjLJnn%2Fk9dkl3f22cYzx3SUUM4VKdVbltUXlqr6ky8kBuGnxR9ZwKuW3l08x3NHsAYdTArWxZqmBAKIuyqEhYaIVAFIGlSw6CyNEgmiITmhDaClINBe9QoBu6ZGL6AQSkmo4sQQgVU5Hso0uqmYdvI6funC0DWd1jjWXXRONogybHQYoSR12FwYMLCYLnfYY8ZN%2FKYukIKHRv8YsFIBVmzvkue43MleubFN4Fq3jWXjrLgXMnl5a2c%2Bnx8eMs6WEXPzb11mR5f3bMi%2F3p1W2ZfnC0fkuluIrFDo7nt5R%2B%2By0369W%2F%2FMPHBimNhE6wQUv3T9oFk%2BiStzZIUe3IKMYviErJ6spmXOnoLAzBc%2B6xFXkg2jJgVeMOuHZcpefTcW2Qm42gW1TnxU6MUZ60A99x5%2BuQ5mzkg454tq1RiJCgkVSGBflZ6K1mPiFltZLZdCOdQfRAMvquGUBIO3lsxi5rBTL%2BQ7XiWI0dkRt%2BGfKnXydB9wGSVYjA0sKiay08OE%2FsYVftSQbtEav4xukg8OpBWbpbC8aq2NkCywYT%2BCsNNOqTUKTHIPxJAghp3z6YU%2Bl7m9zNazukIUSM5UUtvvuuuwFiQIC1RxiYm4wggWXkrcOCF4BELrjEuOoH5HEfISIo4cXb%2FRGCv0noY5LTlXbrBTbRvXIL2Ghe7ydpiCfvLkfNyihF9iJBeGKCwQNImBFWusji%2F0SI4OQGMFg%2BjV1%2FHu2MsihpX6NWplRZ52gBHDVA1EoSWmfu7DSgcUDVSWkLxVenOnGT1MXqQgyapD6MMtVVLlnjZDLtKDpYW41HF4wC1kmatlXjKT8vG%2BohGs8uA3mmKVyDKP77gkoBTWzTv9aYCMfPkyTijhJ%2F6isMwSOFKU0iIPPAktPPZHN0aX3Fj%2BH%2FO%2FjGL3vGmjN5a6exjiFRrbGORyZLPCfx33%2BiV5eyVsNpaJZ20rEw4eT1HcSE1DJuyZzUeo1DUTWhHssPMOnxU2sPJn49kbrjkSylIclSjmM2akuylFIOvpxYmepIVuV5xZwcQLKaodWOPArkWcHjb5Saqp3XbekEpRRVOj6K7CgO9BtFEqpjjhNFxD5QZHvdhzeCIsd2U6hiaWlTZ1LH3oUKI9J6YESpQjL%2B0iGd1nlpTF06V8MOK%2BazPd7rJDZ8ppNnF6xTaZdS3sdy68OQFE3rh6QoRrckhWsw0oekKCNztXeHmnLncs6eQYmZWeX%2B69ldY4vLHUrcKqsVDTs%2Fom0lc3JFGKjtjStKDqzHNK4ct4SDlNZrGMiYwodt1ciPF2t6T1ZL2EQGlKgHiTXIQTkJNShapwgUy6HkSxdCSlxKC7JpuUvAIE9qygnSipI04J3t%2FITe4mGp5%2FgfBt0FIRfqJfGIqefi5hr%2FI4%2BH8UXg466BS1cWYWx9RFE8qVZA7AAHZIN8vIAFSgUJuH4dWWmNBA1XvL6YwG5fLy2cknh8hT2CT76s49Wa5sMEHiQoc1ahYfjmL3ucoSeZuzLIpuEynyXL6dmMu0n05%2BEpF3mETUt4XoOl6xFcu0HeAyK9TvpwX%2BpKsSirrKrN8HwPYtcQz%2FWR4blQQfUQRWsvHmmcVT5zLdo2nGN8HLqOg8Is9Bs9rTCsIlYErAq1Xob04QsZiB%2B69mKJ%2FM33x7vVj7CtVaEqw3C39dGS7FN9e8Tb2gvmI93TeGTTlGXZgedhYZHGCDH2hYC9mGyCcl49MityjzwqejGMp2vBVeX%2FxPnl8i5a3vijO22gnaq%2Bs%2F4vvko%2BScTpxICCQpdm0dxnCetdK%2FBpqPlOBd4czJS4ddyjIW1VyTxERIEaKXXbN7nw8opNa3a3hPleKzmF0BINZDuSbkuOYQFoObZkAmSqOhJMQZyKliWZJoIahrJsAMG0LUMVBc2EDrB1UVQUAxi6WvzIMXIKabQw%2Fp0t13W0cP3nH6wK7Q%2FWjDffDxZleV0HAQa90t22IdxtY9d5CzSj48fDmC2Qt0JhNLvbtlJANGwk6tAwdFs2bVGWbVWWoQmRIUBoy1PbkgQJiQCJoi7aUHdMVdaBaMqGZTqKSRZMEnTdGuFK7QBFumT8x8Yafv8XHiim9BmFIXQoJIDMlwKmksssh0FZkO%2FnACsMhFBTDBVYGQ%2FM1mMiz9jrKKZdbaq8bXI5cwHHeHmjBSZ%2BWc2zNIR6z6Di16fUa5wqtPygZFk6nEfyg78OL6%2BQl5gSo2L%2FIlPy4Y28dFS%2F%2FJHiwDixpsX0wTRwa5cTrcOwU9cnhoOnFRUz%2FbixNwQSarBkhIZYhylNSLO%2FWWAqDUf1XCsECSIf3V1Smkxrz4nSV1jqccNQZXFYzwmfEh2evzwcJSq5%2F9uSpQMokToMJWoaIdQhJao0RGuLhLFLWLaLn1eIr0W19N5SKy2rGsbK02LaQYSZ5xXqh0SVJ9WeRqmvg0aZI6RRqRNx%2FKFkYzhqTGlaHF4Z7KwxbnSPNuQin1p0T%2BNFlo8TeNY6XiylF1n61o7cidLz6mG5E425XcIHfjC34aFczEEgXof9srD2nEs7Sc5VjkuS1RFyrh3VgcYRVju6XNbUXrOTuKXVecYTulhpsMDygljHyEnlhwjKNt7btDar5c6%2FufM%2Bkkjx0O1k6O2Jiv4qiIpqGrOqd3VwsiKfAlnpVNwxBhN3DqUIH778uMrCbRpv902ETtHin6u6PUe09E4fokVhDu1pgfE6aMHA5rvzG1G8mL%2B4yvJGhnfXcPnpCU5PpvrgcKa646Rx7jTVZTFm%2FZnq4Hp1ByAI9%2FIW4Jdp%2FfckOpEKG33QFvzdkAy6PVkxT5KslC1uysD5FPzSPIcnb55caZ4CheqSAqlNA7lGWppH3WGrGao0DxGAzgP4jPUrqTHJC1NTTaqTCeSvRJwiH0T%2BnMRb9EH6yCfJYRh4MFJr8qfmz6I%2FXfKnjtFsIw9anOfkHA6Nk6CNnpSzZkzuZCowjcLh0DideJwOB0XbzsSGcjhcXv24vLpHv5pzsMstSUUbS0EvojvCv9tzLuV1JEIr8rCca6tFfDTB99W8ojgE9s%2BRxt4z4zqZcjLIxOD24EaksGOUqMdvMWWOawk%2FWp4RP1bt1YcINMhGasHUj37MRqnas14%2BHaPxMRumVOior2M29OKxGeM4ZsMY9TEbd4gKHzbRlT8gP1jy3ZoNSWT%2FeVW0tgw5CB6iJ5o0Oi9Mopzlk0VwCLSWv0vpP%2F3GPHb6Ti%2BpJIxsStAnuSL5OvP0rPtdh6vtdeDXqEvO62K15LyYRjUWsjuEDvgUfwcPXnI%2BoyFvt%2BQ8P1N5e13L3u3aRVNPJkb0LVMMlOFscoIYucsmjivD2ayvPURPSucQfit4IiefUzgS%2Bm4FIdaQprg54wH156ZbWBeZUyxMVSoZKeQneyI7EJ3clrbxjpzhIUmHpWi5XRt7a%2Be%2FM5AU6yPhX%2B8wiuNVY7UmWF5nMak4zdX8iQWF8P3mGksO1FaT1XxhSViAohrvyACWt5WL%2B4hitIryJ8M0Odv7WKePt%2Bz%2FjQxs0upo81M4qdxMi4emibMm5xyF1OLUuWjFNwEMWgN0P89M5%2By6Sx4simqVCQ96dnkzHpyNexBL5x9%2Bkyoj4BGCGIzT0klPiJlsSv%2B9e3FXKwTfbxtZLzqtD5YshQJzwweiu1LHBjPDroDLuC9KRKGmNQ16HH8ElisPibM7cUZhPJu%2FjHuc0h7jPGnTdslgkF7urAZxNHtB6kgasdOGlFcuFBEZESmzwHJSKvD4plwzvIrM%2FRaAO%2FnC7HqfoRSpH3l3vEznhxQdmsxAqzHdJUWemgYsJAAhHJ3Zqikvx3v7JwweWSlUViyJjProWZJJPSkyg9aRC2Ye%2FqcbuaArVYLRZ%2BTCjfDRWdwasfewuLNub%2B6vf7zcTOsDFyqoRegsl4%2BUDUwKj118QnMiPm5Ql3VXYRdWLbtI6sbnFolb451wiVJTpdR8bWl5HlYUSWYnnEQuchK5ghdKGq%2FJcwMcwki4SCBXkIAeRj%2FZlHNt4EbZveW6AJ1Rio8wqluK50Epe7Y7A90hXmOtLEPm7K0pNDeNFK48BQNrFESBsyLyK1fxbrM4g8uKewqIbYxxR0CuzNiRxy6Fh13qkbCrT4%2FmVvR%2Buw5NLlhO0BOVGMOvmVNly3Z8a16nrcB4s26V0Q5sQH9PByym7NvhyS88DtPFgT5cSjbkOScuM%2BdfByVb2Cij2N6AhauMnKJYPW6KL%2F9oR8LO%2BuM6Mkv5HzEGp50PbN8Y0el6riOaaJggW1aX%2BdUql2UhVpOaLeLRtMuqmf2S5fxSsxNiQZOQJMETGYMXl0HDMN7QkvEsy%2BnBvgXfiHn4knEtyydT36K7HL1qgGHtyuZNzd9N7%2BNX948r6%2F7%2BKXgOnj9%2B%2F%2FLPtKmlOT2OaOjM83Isr1zIJN%2F5vKL1k3meHDpEbRyfyDFJjat4Batnuh7RT%2FIJesKSgNEvF2uWNKYSyBFt4ks3ImoN8FGwJtfJrKjFLbONS1zreD2ynZxxvJyQrgvDFvzipipzzC0Dxy23oo34IgsnLtPLtod17%2BmJ42Js05zm1H8yksR1aWTVWTrEhhL3lPtHhqHz1o3iQbspi%2BOytOrx72ViVpal9y5C1UxcS%2FW0V4iZJTo1MJniQ38oKrVt1I1cBuvQez4nKdco3s30N%2FjCrmKWlS1fTs2WkG8uJSi6XDoBO9uoBd2oKibIegc%2BGf5yH17A5IhFuqS21UFbVQhstJn62SQ1qkm5HKB5nJRmXSyqQJrZjwr0ee3F7t8XzYOA1sslCAkKSQJ%2BLYvX26Hf4CHMQm8184n7rFbHmW70HIgekFdQc1gx4yUZ7y97W8hPPbqcnFJTjvjhHXnWp1KzNauv1nlZDT0I6UjbGcULbsQSvcxBnSL7eebfTA11E0m%2Bpv9xaVbePq6VaViRT%2BWtf7jTK4381JkK5yGALl7%2BUjNGM5IOQingnCSHpMR0ASCllQKPIXaAT6YszLRiLQRFUbKmPAtUjZnICRkSDXV2LEYo1aeC1CFSbQwLfWvtlVu8zJb%2Fd9ksTA9i9AKW1sfCGWmJQY9Roi2Y6VWi7DctyZESJO6apUNEZFWjuOhh2NbDPxf3hM6mRRoEjBiPzANRrMCz79B%2BrYEfk2PldhfF2dbbp%2FQkOhom5BEhgfG%2FCEXRshg0tHVU9MQomjJF14BUxcJvFwa3o4sPrAQkc%2BrmSkQ2evme5lOkggOCPqIDwbt%2BTQ7iYyeUW6Tjz5f3OZRhqZ4LBGK81vsvyraqYO8ur95POGXEk3MB6cgoMoMo3n8EV7tLlmf5r3t%2B4gxCWoEDeMVPuL7trSHbJ5TRp%2BkKdC%2B2%2FR5urBKAHYzkcBdQIi4M6wAySwGhilgNC5BEns9uj7hGfImlsDgvmuJpLT4HkEidV%2F8H). Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *rnaseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *DEreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *DEreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\\n\\n\\n### The pipelines includes\\n- quality control of rawdata with FastQC and MultiQC\\n- Read mapping to the reference genome using STAR\\n- generation of bigWig tracks for visualisation of alignment with deeptools\\n- Characterization of insert size for paired-end libraries\\n- Read quantification with featureCounts (Subread) \\n- Library complexity assessment with dupRadar\\n- RNA class representation\\n- Check for strand specificity\\n- Visualization of gene body coverage\\n- Illustration of sample relatedness with MDS plots and heatmaps\\n- Differential Expression Analysis for depicted group comparisons with DESeq2\\n- Enrichment analysis for DE results with clusterProfiler and ReactomePA\\n- Additional DE analysis including multimapped reads\\n\\n\\n### Pipeline parameter settings\\n- targets.txt: tab-separated txt-file giving information about the analysed samples. The following columns are required \\n  - sample: sample identifier for use in plots and and tables\\n  - file: read counts file name (a unique sub-string of the file name is sufficient, this sub-string is grebbed against the count file names produced by the pipeline) \\n  - group: variable for sample grouping (e.g. by condition)\\n  - replicate: replicate number of samples belonging to the same group\\n- contrasts.txt: indicate intended group comparisions for differential expression analysis, e.g. *KOvsWT=(KO-WT)* if targets.txt contains the groups *KO* and *WT*. Give 1 contrast per line.  \\n- essential.vars.groovy: essential parameter describing the experiment including: \\n  - ESSENTIAL_PROJECT: your project folder name\\n  - ESSENTIAL_STAR_REF: path to STAR indexed reference genome\\n  - ESSENTIAL_GENESGTF: genome annotation file in gtf-format\\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single read (\"no\") design\\n  - ESSENTIAL_STRANDED: strandness of library (no|yes|reverse)\\n  - ESSENTIAL_ORG: UCSC organism name\\n  - ESSENTIAL_READLENGTH: read length of library\\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \\n\\n\\n## Programs required\\n- Bedtools\\n- DEseq2\\n- deeptools\\n- dupRadar (provided by another project from imbforge)\\n- FastQC\\n- MultiQC\\n- Picard\\n- R packages DESeq2, clusterProfiler, ReactomePA\\n- RSeQC\\n- Samtools\\n- STAR\\n- Subread\\n- UCSC utilities\\n'\n",
      " '# ChIP-Seq pipeline\\nHere we provide the tools to perform paired end or single read ChIP-Seq analysis including raw data quality control, read mapping, peak calling, differential binding analysis and functional annotation. As input files you may use either zipped fastq-files (.fastq.gz) or mapped read data (.bam files). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes.\\n\\n\\n## Pipeline Workflow\\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_ChIPseq_pipeline.html#R7R1Zc6M489e4KvNgF4fPx0kyzmSPTCbJfLOzLykMsq0NBgLYOX79p5NTYLC5kkyyNRuEAKnV6m712VPPNs8Xruas%2F7YNYPYUyXjuqec9RZGHitLD%2F0nGC22ZDGe0YeVCg3UKG27hK2CNEmvdQgN4sY6%2BbZs%2BdOKNum1ZQPdjbZrr2k%2FxbkvbjH%2FV0VYg1XCra2a69Sc0%2FDVrlcez8MZXAFdr9umpMqE3Fpr%2BsHLtrcW%2BZ9kWoHc2Gn8Nm6O31gz7KdKkfumpZ65t%2B%2FSvzfMZMDFYOcToc%2FOMu8GQXWD5RR44%2F%2B%2FFO%2F%2Fxv8eH1%2FXZ5Hopg0tf6geD8184LICBQMMubddf2yvb0swvYespmS%2FAr5XQ1X%2FbjcP7rzQHtYRP%2FWXbqOFcxt2A77%2Bwlde2vo2a1v7GZHfBM%2FT%2FwS8cjNjVr8id82f2LXLxwi8s332JPIQvf0XvhY%2BRK%2F6c52uuz0Yyxtf21tXBNXDhBvjAvXU0HVqr4FEj0hU9uQJ%2BRlfPd%2B2HAH%2FQ%2Bp6ml4itGv0kbdrpV99vf27%2F2J6%2F%2Fnt3eTf%2F5e3%2B7asM8egHczoO2QLiVYt8gqHABbDRQN0X1MEFpubDXRzhNbZvVkG%2FEIHQHwyHxPj0Vfpjub6a%2BuZufbO4%2Bno7v3%2F92ucbeqeZW%2FapnjI20QxODbjDXzThyiI3xo9bjPqnLtlWwSX6a8X%2BTx5buMkWNDDyLt6awN8IVpnaApinwRY9s03bJZ3UOfkRLtrStjh2yHjNDc1bE2SXY6iPr5bQNKMvlfBv8FJ%2Bh9AE9XTlagZEiJBo1u0N1BkGrUzN8zg2cXIhkc%2F6aPFsDLj%2BTMrDrB1wffCciwns7nSsDpQRfYqRbbR806DtKUIIJ2xZ1xEaOJSGg9HxaCTEavlD49GXMf6tA4%2FgBjGyz55DeahcDSIpUzmGRfJISmHQUJqlMWjM0eoY%2FHn96Z%2FfSf%2F8%2B8f85ou7kY27i5fLvno0V%2Bs8D6uQ1xRkNeqwalZDHv3sutpLpINjQ8v3Im%2B%2Bxg0hug2nihDd5hn91Uluf%2FQHHUGIbcFUCiHg%2FGasX%2Fxzvr4Bfz3qy18vd9%2FOXvvy6L1jYIyctYKOajfRkZ2FstAx1X%2BYj76jmZTX%2F2j0zVuDLO6b4KzJBsz5YrjPeTK%2B0fcI9n5GHeSh8yxg2GdK73OUSdPXpXh31Z9NzuZsfXndvwWP6Ilr6AATIjaLhnaaOwahlJExAQEoqWxz5AyOg0OfbCnoawv0NNrBfOrocA%2BW0IJEDEXog%2F%2FZM8dqFsRzNKtIm4alCxcsY19a%2Bz5WIXzG%2B0WZr6CPRLmBazoDC9EWZQ43i6WN6Az68%2BriFs9VWdnooo%2FX1rQXeJXADis75hwQiBTMMWp4GDPm%2Bho66K8BvztAEqK9e0nPJrsjh6AmQBHRPLObG8azTqOwaWuGhxU0GOBYHWMbWxMvnqRZBkZxz0OcCWom5VIaOc9XtIsr2IDPQN%2F6ZLiRDYhQFb8WWiuyNU888An9jzQ6GnQBmReZ3YlDbhnAw6ek4rPKPgJVck5IcDI1fUyQBQdNmZ8dKj9mDgVsLgECdHxy8J%2FoeOWggxaePBXCuLAz5Q2BfnAoBFcuny0MQzkFMkUAMd5WTvJJiR79hHpgEn%2BBvVx6mIomFqEC8aNd6Zn%2F%2FSsmSe%2BTnmOycyhKN3B%2Bk5WiEnPVAnPG5vvhAffb4j%2BsZECEGOtOBDKkgDwu7GdMHYlGFdPHhe0awO2jZixrYBBxxCfUU0JYPgruRDXxn4lsooIh%2Fg16OJphBO9W8thPK6IEXT5v4D8LtFjxm3kig4AfcVkuDfFscCZZFdhEByXpaLG9QPsd3pJSvQKoR3op2aD3F7bxkmpMcS7f4C2X17n8LeyYfpS3wOi7ECJt8PwhAZmFBIPI22ChL%2BDG9Ihz5xD9TkXz8LSNk5qD8N11zerq%2BsddhZOynK2ftz6Nz63iRWPza33dMBV1Dp0Fe5gNXrKXkRlVMXDcKCIRhIASMheVZfOlDCXC%2FphYEpE4sFAGdc38zCwNPhYjTrndwQRL%2FBkb9VqaRL%2BOlfi9mEBRhcycUK3LM4EAKJKZuVBQgm2jywjnLiGuKdPf4lpxcY0fMt6MuOYIRYcNGj60qHQgOaGERtv7ZLfge%2BPIPYT0fp%2FtoM9EPrDoqXufLu46PASnDuuF1XBOsm2d6nXQVCkpwDenYqHyNKIq64zmKlCADHaa62XqrbK6pUVQJnXnLECG2iq1LvWsQoJROa69gwbYpwoScavo5E4D3dIVejAGpBhABN%2Blj%2FWxNRYuoR7%2FunRiO%2F7gU9FhiIC5lwEq%2BQywDKt7WkMfYHcY%2FOyTS9yAqmd%2Fw6FI%2FyFgf7PyjgmHcz%2BRsaRRLRL%2FYGfVSMP4KtagRhI6t8lyaiHenVzSlCtbzNx7uPCjFrbujjsi%2FQSyxVzz%2FO9nhKoXETwet4h%2B%2BniEuo3WzDb5acjVnsiwiX0CDxbx5oce07cneP56uxggmpHD7xmv36DBYbYxD%2BwcuBvW76Mbj%2FqANgfsey9roOuUyRpE9CzBLph26C9yda6OM5gE8ZJiL5YqPz6NuOmAn5%2FG0xTlk8dymvSNy3smFWEgQmclfqbLIFPAXNhPrVOo%2BijCsCBF4F5lrft7xK1YQ3b6rcofQ4gie9y0f6MIs%2BfNOoki6rQJFMkXdj4IilS88mlvLDlxIpETRlEmVtGnwjU%2FGoVG1aLQXOS02KoaT4qh0eQgcVndKy5X6KZYmChVjppFfR6Eiyz%2F9o0%2BcpWFYG3quFJskdXjF7nSsK44j5h0X0lfeHerHRE5xnF%2BobCDT6YXcrI%2FO2%2FXyl%2F4mb6bWHkYUu5jOfW5yRelTB1B0KTz32yYj6CJ%2Fspo1gCC5p%2FEfyNoPQLS5F1g6LhiRYAQQ4f5ioC3gaFyHEOVfYy9AyhauXjXEooqx6BoYVX5wn7yIVAK68p5S8R38QZoRg8bdx0HmyX2WJaz4hBo04lHXJGwVdWhgSzYLf5T1vN7dPIH2%2BFD1XxohWeQKq%2BfH0awpE79fMXuTDMljsBjtgNj%2BniBKbImfXyu6UkQ%2FbbfM4GEdDD%2FBOkA15dvW5%2B6AeIcJqhDuIUi%2B0d61PM2Vnu%2BLhSN8WYLpgFp1A2NXQnHXcR54XhnvGgMPo%2FJn2sbaGL8%2BwrMHcBv7TXhyjAZTmOor0rjNOrz7XEk6hfE80nH8FxKoboLvK2JOWIXkf0EIRmaGzA%2BEcaiPYTo3qxv0zlcLoEb%2BjDRoZjQ8xsfyjX9tmZZPGNJx3Z3DTt5OBZkTBHtZB75Xf1OnnZ%2BJ5v2qqPbGI2sz7mUbpsmkhZIJCXlWEDT1z3uq%2FcBkHmsFkTmKtjSn7uLe%2Bfs7od%2FtQHj6832yr%2B%2Fajd3SxNOW4efV%2Fnpf%2B95ddaaOSJ33J2hT2mJ2gUOQpuOkqi4C3nN7uI48SI%2BpAYQmXtraL3cswwG97QdHR3vcU92mqWNg5uNkR596n5m%2BGLSsbrLMx6QPujWYA1MB7je4EY8dXHHvBDOdhzi8RohmhTsBbxjXOx7GEtBEJVvCweltTMhxLPh8oWwMh%2FzcH3r%2BXQ%2B3M0%2BTAfhI%2FTy1ohA4CaI3TQtADBHKTyxd39gncgFJQNZqiAt4Fyo%2FT7eobt97XdJaaE25ffw7bpiX1qoJ9snBVXMeIo4axF%2BEEMHP8sJ3FjbYESwFh7dOvEQ49TXaXIV0UeZmFOl8jjTsZtOBc%2BjtPJ42JRzd2TLJFJcAvLTS6a4RHfUsTpTjV4kcWVNvuAqv47Ss5EoTWU9umdxyMrwvZ9%2BEvSs%2B6mYh6M3SyedtWYhkYcoBIsSyu9nWOJCI4C6R6kfpY70X0wcoWVQk5y%2BRoKUjoAPPT%2FonaCVXhat3EMoj4l8Cc1rEQCUp5Kj31QS%2B64LLHQNUkmhHMj1nd2UAw90b5y0JgiOCxK46h11jiVw4NkHLOFTEdr2hXan0Xf41cBakaMhWlg0wN2KiITaagOsiJDYDaJGpT464fK0bNw8LYtRpvKErQJalvQ2GEuTNC1r0NtAHLfzFqL%2FqnPJrijeh5OivTSL56Zv2XNroiY9sY7K4lzcE0vbnGEtE6l%2FU5BK6og6%2BiQBle1uEBnwiDlsAVdPEBNIH8l8D02ELqOx62zs5QnepHmCV4P0Neb5UFuiWD8t9eHp9PHu35%2FX9zfWn456ubv%2FXVyhFKGapgmVEKqV06mjNK0812s3JewDBewmA4hE657rhdW2a3EiIGi4z7U40X%2FEMKZuhoaP8vcR96LCTA1Res2FHkleT20%2BO%2BjhHB2vzE%2BpGYVEfPjl%2Bdq0W4L8coR%2FRYL8mPz0qAkrKuCTn3oE%2FGHL7sRila7SJr%2F8rdLNyRPxFlW6C70wzbs%2Bu0FvO7V93wQW0LFj5pkNlkuoQ8DSEzRA8Rblsw%2FR9fnoqtfhSGBwb91A9e5T8785ajYq6hXYPWoGHQQIqoQtStQc08ZvpSnO%2BcPNkLJwtKUp2kjqNU7RaiBJI255aYkkiYs9vvtae5XKPoVjXqcNUYuCGolO2%2FzegkaicEqTjmok5HIpTZrSSGw03Sse6syCs3TNNIkTRSOMiwyxvBT%2BPmKM1anAz6vtGGNulupMRETays3tPF0MiFjA1c%2BYMYpFazGVnhcLPexuqHEF2J700h6NBTbuuuK3FpOtcWFf%2FeOATf%2F7zcyf6Rfj%2Frs%2FIFbJlKeFmXJbwljusA8iX7VXC7kBJBxaxzbsC2DZsRJeOby5fbpGw55Ov%2F28u%2FxC%2BDPaFc%2FE%2FL6KzSMZKMXvSjjdN46WQnx%2FSyHQdBAOmQC0cTmLaBQ42h36A%2FZCoKWh8mOI2q4Iwjz%2FTVsnoycxQXgO%2B4qhHFSio2MVN%2BLC00SYpUJw4Jfl2mjNSEBrEoCrt75GQO0%2Bbn0NYaR2q7EKcb1LwNxL6V2CRGQNcPqZwMtWqOjgWv%2BOsPpZdjakztdxDXh9JASZoFL%2BoeajlVClIInnbEL%2FO0FIbgBaVpxFAidDuXlo7YMFffdTeL2EFklFE%2BTvoaVPNIJuosoo1CNDQ0%2B9eNAjgAaON4gsSpEqlAWLcPbKVsQs%2Bf4PMrBeqVKclVbWrEn4mfFSMLkhz8IS9XWFPKdlndZO028gbbcsFw7W6xSTDcbdij6QVx7OT5CiPRkaPlt2UR9Iqlz1wmyDJ6%2FQcUhitpbPlOisy2IlbWuHz47kLEyVlY4GKXNNhTB1J4cGLd0sD27kAYHxYPXa7XEqB4zzTSuAlTjLkkVHT1GFzFlNpIznUO%2BwZQMnFY6ldukQJQtTTxRWkb1p9E3aLxBYBCKXwH4xqk3iOt6A0cHg4ialsJlcUAjjeeG645BGcnXdsDxgRW36DCCYyVP1MWHvaL8%2FGPaT1QsTauFRV2Hsz8sozrON4RmUNvnPovB%2Fuyb%2FybBdk%2F9X6Y%2Fl%2Bmrqm7v1zeLq6%2B38%2FvUrD%2FAS8MUUamHaK%2BQtSZXSUMRC%2FgIrEuUeoC59XYqFLDJZCGa38UWKe0FbaPF7cZdp1sSxgTFo9TTJjTbQMAjFFGFFnIpWwV24byyXjtQUXgxF0pFSHi%2BKIUG6VvgC7%2B5eJMNsT1GfP%2FdoemmTnA9IPjrbwYcHzfyUWq3DdmUV0E0WKRTsuqEAusO6oHuMVXqcFD0jSlgOzbCRwFV0LImlEoskTgwXp3UR80C5sox%2BrgrkmsRdj7nabS9yVSAXCpGrSTNkLnZ%2FXCukECxv0D7F1ONzamjJ2Y0fzRKVC4wPa2jp7MBatABVwGFmk8PEF3WYTRuP4jBt1mOB1AYwtxMatE46yX0AvVgSOQuLP%2BOakDO7xEigXb%2F0ETT1qMt4qHgny7n1MEZJDNeCFNvv9vQ5ies25aIUprbjZ1o3f06jXYleClBHRwNBjOR%2FEjlqEL%2BM97xkcSlWFmiSFOGSVWBPEbrTv%2FuEwAdqnoXAKqp4bsr6v0%2Fv3AFn%2BeuojsJawtXW5U7PXbSPxQuDoFOLlfxqzcVCKJBwO%2B42oJehK3jSnCfsxGEoKv4hmlKz3vtNA9JxgbZBpwUvE4qCHnkgbMcFIQ48kddBqqFmCHP1Y9xARKrdZII6cTcPzN3x%2Fkg1GGBJSdqT7T6YNislbGkrIn%2FumcP%2BdSvR4ALKtLwS3z2pboAIENrWJKUv2AgYLD1z68a0QDjtOy18Y0dlc8nb%2FxFSWgM8Ax3JEW5snXEEjRlbwELzwp7E1CNYdDeM0wnKzQYEIniQVmqKhfCcELPsAwAYJ0PAeEgSgtbK%2BzTYM7zE0TJXasu3o3bA1TXhNjQapuN8hH5D8kgpLQUVMZkKoagIzpx1qdzz5MkPrHKfC%2Bsw5eesfpO5OJo8phTODVp5CpaDMnEkk1dPWMadrEwcqWTX41ECC%2BvJxMGqtxKz61%2B43mzx5KDOC1kPjxROJqVqaU2AQB3KGrlWVOnVlrZjAz1satEsYG89IieTWZGTRODRowh9erKR7c259IzkOIOaSIJQjAZdesR0MM2PWqWD0qAUJUQXQXq6JHWU26OOohgOIfh5cvSOZKjiBfXePzYobaNDrttpV9ChYwnLKkSHhOiktowNeTJW88iQO%2BxCR5mta76c4sRGeIL7eG2IMfSK50U%2F789Kwr44cx5ORoNR%2FOgxkwaiWMk0f1YntWUIfIenkrr89vNKULzVBIGjSaymTmcSBBpwucRFEwsfR%2FADOIERzjTTixZcDOPj95w%2FqigzG2qM%2BQTywgmyEarZo0d19QmqTu%2BdDFIYqTMRyRSeacqTTMGZZh%2FCdtZdsmZzSYD7xPYRwXfixoZpoXcfbAH%2FWeD3ndErz3jy0Tw7b%2BEGmppLyNYDCLiMR0GFjQ8LomdhCUTwaGi8A8aQMGcq8x9aYt3MSWCjoAFXGPoZ5FKyLfMFK%2FilNLWNLEqF%2FpiBAeEaDZ7iJ8uG0gsSrLCkKonp5YxPZJWoa8xndDhk4DhjW4%2FlcGlktL1SjqX5tWJzeVBnU5EocWahKoqAWYgtNNJkUD4G4je%2FqIRfEEc5dH8fx8js95tn5NClvXT%2BjJB65stEAEwoFmU7uB13QALsgE79z2877%2BcdGv%2FJn9%2F6P%2B%2F2WoDroLUXCNmdXpAVE%2B9Zlm0yxiE7RlXza6O%2FFaqqDAtmy5g0aFRot15Yh8tZCKFVNKtiU6XLC6qq5KPXuHOqqpo0VUVXPTcZTcc0Vcq4XCkLhVVuqltThejwwn7yIVC2FnzcgsIaKxdsEDlHb9%2BgAw7s4wQ7lDW7QDNqUFflJcBITiJPa5WNXM1qrfJKbBaoRFeDdkoYDde2vV0%2Bnmx%2BINY4KUgkeWGgrvDG4%2BWfD80biy57R3mjOixXePpI3lgMJysoMde9nGD7zPaJQpgtEKbKw7QOw9BkmpvRHukt2b8hO%2BPN5nzrFHd6DGQ2Y%2BuYJOTTqE9gE9gXUVf0r7tBn89NVJaNQs3KaOhlUOf0unPymiq3W%2FpczMqlNuW1dhh3jGoeTiNnRbl452r6nmqb72eFqZAoxpzYmcJEqHup0THe1pQKoY895pYpz16jt%2BdZnajpK1KCytzjoC3KMTqectQocCndIiYVy0dpgSaRRXE8m8Rfwaqi06dyJKMJjzLjTEtNZCthNdOTLyodsCJNhQPOHFeiv9qQvu1vrC0rQSy97WajuRhXFen7WS%2FIE12%2Feo2SSaLdO4BQUkf9N0cpkzLWeKp0T8YaZVeOoAb6dPJKl4y0XF6lmO05QZkjUCfYfhrY8CPC75z8CKlcNMXSOEn14v7NCVn7yxj%2FpmRtlltm5WoGBKF3H2uOCvIrU%2FO8pFAvVYM%2Bs6RmQ1SIWFRNRpUqSOIkRpZ9yRJLZDolT23NZIsZhr5rT%2Fh0SYqGCHMJOabtR9OMmCnbduRlgAXxc8uChx3dPT%2BebyrvDZhAhpWCf5zdYgLKy4NKCM%2BeaHaqoKQwCQBkFYUJyWVj0LHlmpSyzNkveSOBFvoSCT2n701PpfQrNV0HnkchY8KFq7kU1LgqxjOF%2BwL%2Fc312g%2F%2B0fd8EFtCJJ5cNlkuow3hu2ryPYZHDcqG%2B3pBn%2BDoyyEaHcnlNpoc20Qq79RV6u%2B7ankfG5VK5hZYPZd9ILYqz1iw%2F8KMrigzEfc0IH5K4Iw3NHvA%2FYOGPGlBDNGQTzo2WGcFHFIQCpuYUXZ1YIdQYtJZA87cuUcMgFHDhYsu63c17UR8X09ZhMDzug5dAxD2jwL13EDylxlDA37wUWqLGNF3Yw0OOzyDGJIUj8ofFtDpRLlEFM%2BCnrCDZpCDhgixgBsqs%2FFkd69Vs24%2FKq2i%2B679tA4uiX%2F4P). Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *chipseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *ChIPreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *ChIPreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\\n\\n\\n### The pipelines includes\\n- raw data quality control with FastQC, BamQC and MultiQC\\n- mapping reads or read pairs to the reference genome using bowtie2 (default) or bowtie1\\n- filter out multimapping reads from bowtie2 output with samtools (optional)\\n- identify and remove duplicate reads with Picard MarkDuplicates (optional) \\n- generation of bigWig tracks for visualisation of alignment with deeptools bamCoverage. For single end design, reads are extended to the average fragment size\\n- characterization of insert size using Picard CollectInsertSizeMetrics (for paired end libraries only)\\n- characterize library complexity by PCR Bottleneck Coefficient using the GenomicAlignments R-package (for single read libraries only) \\n- characterize phantom peaks by cross correlation analysis using the spp R-package (for single read libraries only)\\n- peak calling of IP samples vs. corresponding input controls using MACS2\\n- peak annotation using the ChIPseeker R-package (optional)\\n- differential binding analysis using the diffbind R-package (optional). For this, input peak files must be given in *NGSpipe2go/tools/diffbind/targets_diffbind.txt* and contrasts of interest in *NGSpipe2go/tools/diffbind/contrasts_diffbind.txt* (see below)\\n\\n\\n### Pipeline-specific parameter settings\\n- targets.txt: tab-separated txt-file giving information about the analysed samples. The following columns are required: \\n  - IP: bam file name of IP sample\\n  - IPname: IP sample name to be used in plots and tables \\n  - INPUT: bam file name of corresponding input control sample\\n  - INPUTname: input sample name to be used in plots and tables \\n  - group: variable for sample grouping (e.g. by condition)\\n\\n- essential.vars.groovy: essential parameter describing the experiment including: \\n  - ESSENTIAL_PROJECT: your project folder name\\n  - ESSENTIAL_BOWTIE_REF: full path to bowtie2 indexed reference genome (bowtie1 indexed reference genome if bowtie1 is selected as mapper)\\n  - ESSENTIAL_BOWTIE_GENOME: full path to the reference genome FASTA file\\n  - ESSENTIAL_BSGENOME: Bioconductor genome sequence annotation package\\n  - ESSENTIAL_TXDB: Bioconductor transcript-related annotation package\\n  - ESSENTIAL_ANNODB: Bioconductor genome annotation package\\n  - ESSENTIAL_BLACKLIST: files with problematic \\'blacklist regions\\' to be excluded from analysis (optional)\\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single read (\"no\") design\\n  - ESSENTIAL_READLEN: read length of library\\n  - ESSENTIAL_FRAGLEN: mean length of library inserts and also minimum peak size called by MACS2\\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\\n  - ESSENTIAL_USE_BOWTIE1: if true use bowtie1 for read mapping, otherwise bowtie2 by default\\n\\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \\n\\nIf differential binding analysis is selected it is required additionally:\\n\\n- contrasts_diffbind.txt: indicate intended group comparisions for differential binding analysis, e.g. *KOvsWT=(KO-WT)* if targets.txt contains the groups *KO* and *WT*. Give 1 contrast per line.  \\n- targets_diffbind.txt: \\n  - SampleID: IP sample name (as IPname in targets.txt)\\n  - Condition: variable for sample grouping (as group in targets.txt)\\n  - Replicate: number of replicate\\n  - bamReads: bam file name of IP sample (as IP in targets.txt but with path relative to project directory)\\n  - ControlID: input sample name (as INPUTname in targets.txt)\\n  - bamControl: bam file name of corresponding input control sample (as INPUT in targets.txt but with path relative to project directory)\\n  - Peaks: peak file name opbatined from peak caller (path relative to project directory)\\n  - PeakCaller: name of peak caller (e.g. macs)\\n\\n## Programs required\\n- Bedtools\\n- Bowtie2\\n- deepTools\\n- encodeChIPqc (provided by another project from imbforge)\\n- FastQC\\n- MACS2\\n- MultiQC\\n- Picard\\n- R with packages ChIPSeeker, diffbind, GenomicAlignments, spp and genome annotation packages\\n- Samtools\\n- UCSC utilities\\n'\n",
      " '# DNA-Seq pipeline\\nHere we provide the tools to perform paired end or single read DNA-Seq analysis including raw data quality control, read mapping, variant calling and variant filtering. \\n\\n\\n## Pipeline Workflow\\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=NGSpipe2go_DNAseq_pipeline.html#R7R1bd5s489f4nObBPoDvj3ESO%2BnXZpuk3Wz3pUcG2WaDgQJ2Lr%2F%2B0wgJIxAY29jGabZ7TowAIc2MRnNXrXkxfxl5yJ19dQxs1TTFeKk1L2uaprY0rQb%2FK8Zr2NLtdsOGqWca7KFVw4P5hlmjwloXpoF94cHAcazAdMVG3bFtrAdCG%2FI851l8bOJY4lddNMWphgcdWenWR9MIZqxV7fRXN66xOZ2xT%2Fc0Nr8x0p%2BmnrOw2fdsx8bhnTni3bA5%2BjNkOM%2BxpuZVrXnhOU4Q%2Fpq%2FXGALwMohFr43zLgbDdnDdlDkhaV%2Be%2FfwuPi8uHz79%2FvN9%2BFPf%2FlvXeUYWCJrwYBR0zoW6XFgmEsAr2VObXqj83sBYx14FA7RJfk1ZX%2Fpa2Mv2ULGRPvirRQawSsH%2FiyYW%2BSXSu5ZaIytQQTTC8dyPPpQc0j%2FI4%2F4gec8RVgiUBxMHDtgJKV2YNzIn2GD9Uj7ia4mpmXFOr3qwL%2BoU36HIrE5mHrIMAlwE826Mzd1cqnAIxbyffY7wq8STTKOHIavJfYC%2FBJrYsgaYWeOA%2B%2BVPMLuaj32CltTmsJQ9byi0JbCCHQWI84OfxCxVTGN%2Bl6RB%2FnBKEROLf9i21w%2BXM8nj8355L63%2FPEjqNc7Kdxhg6wjdul4wcyZOjayrlatMQQAXP5bzF3%2B%2FBS5pGX11hfHcRme%2FsNB8MpwihaBQ5piVIJfzOAf6LDRZlc%2FWffw%2B%2FIlfvHKL2wCgNhLcPmT9wcXq9foFX8vTW4hDGDimQuRNfnOwtPZU%2F3P6rBl%2FfwyuhtMHr%2BPTHXx2Ku3GatD3hQHOXBvygnFwxYKzKU4jl2Qfq18nsxue4G1nN2Pb68fhr%2FerusfHGIvHMJzAoI8BwBX75fEMnrtrsAy6n0txTLUnpZmGS1F2516rkZfR%2BdB%2F9v8WRuN0C%2F%2F4n%2Bvn%2BvdnVlGaQyC35EziKyFvmZdS3fVZj%2B9sKUPthQpNznAUn97DC6%2FK%2F%2F8%2B3l4f%2BXNVeP76PUm5DOngKwsbr7i35tx862QXBDHzValcNxW3zuSsW2cg05ALnXgvsCLoXFoWnw4aToQtoQDEEXzWESRN2zJFg9bp0AtfFOHG3Wf4vucPKC23JecHT9quNBq54PL2%2FP6A%2F4NMDZdbJlk1yTtg7g4MC4kNISjE5tDoWTHAe827TrFrBmgMXk7NkmiIOOJaZt03ycosddMpRzg%2By6Cr%2BlMElk9qPt%2BPfAWto4CGFv8ss4oWNKpEht02HUhtFRyLmGnCKQhD0%2BEDmdBAMaHc1hY2nBqBkTybHiW27Chq6E5H08c0i35eTt6AAxrU4dc1IF2LWcMEi1egplkyNHvk9%2BE8n0g%2FKFhI%2FKjwe81iDjrLF%2FTY8x8jsMaSeC%2FAVqSknel1pDlIMMHKwtMH2wqjrGwAI4Ksg1YY75POLSJrJBbI8IMsXfKE8YvWF8EdIYxpkEIDbo17SllJ598fEb%2B0EYXmR6moKAA%2BeTSWwb2QSXbBhDlLNPUJ7MVulIsJYqg9nQ6aUMJ12LjWk9k%2FCp9P21J9tMECIgy6MJPoiy6RG2EyYcyFJdKerwhMjy2pODK3dALw1BNgUyTQIy3FZZK2Pe%2BOSYlK%2FaxutYWENYVO3AmEx%2BYbAIJ0aC3x4vWO6bwy3%2F%2FFAThdcKvIPquJOEDaDicatdLswcSZn%2F42Ptr%2FB94AMjmAJYgUVh1pZxrToZPRC3KthTgWqG1RQnb6wFgEe51YvfIqgnqzLAF93QCPLq1rBNwv63YtmxHKibWusm2WeqpraZq4UkQ3uzJuXdcvJMJEceQlKJNvrFEnp8pKGU9toWclAL%2FfoAd9m3yBtdzlqaBC4g1Zh4RDSI56Za8KACDDibnu%2BFrdd%2FFujkhWrPwdeVT3giyGxw3aMjePCs6IRlatLWMXotxILYzxJg%2BN1mHOBo4ZJ%2BcWNRiMKFGgjj3fp6ZAX5wEWWVzx51WpQhtCS9O7IdWCa0KK2N%2BSa5jLHOTfZLmWHgoIIM%2F2BlJZmWgMYEbkqRZPqfvd7T01B%2Fsr4P6uhypGL1S11tv3dJRrDIlWq4LWyk61REromkhiHyg7sLysiLiBS%2FF4TRBTBCnWhjngNU5UxgwOiZDpuq1zBYsus%2BcdWsrN080tbhSdAIyeB%2F642wOdqg17LyZtz7mWblMvaTYO9km9eJ8vyFXl02OxlMnXoJWceKSPcl8Ps2d9Vzht9qphiV2pFwqs7mSmoRfi%2FlKmnu%2Fs58A6XykVZBPtKqHB8ZPJ4XZiKCiSxsuieMgwq6rkvWVUyoXO%2FZT%2FX1yceRMQufZb22fx4V6RvjZ7Q5j2odiEeVbDjj8QGcJ%2FW6R%2BVJubuwxDm1Xi%2BjVkqmnSlb6Pd%2FLQJ3AasJQgrJA6tFE1sxym89bykdT6EPyRiWVzQNM7SfhyZlGLfyiQkV3Kg%2BOv%2F%2BPwpuz0RU1MUAeRqrcpa7zhP6WYz%2BgSJNHVnnjOapySW5AtJKWDxsh4fxDNHctIBcr7G1xNDrQbS1bqsnrJRmP21jlqpr3BZdvsu2W7FloaRWhof9hRX41VwbS30SWw%2FPJsFiRPTQQsjVop6V8etqUVwj13KCVxdfwF1qFbm%2BOIuWzg%2FbnJjYGGGbPkTv%2Fxid5U12H1P7%2B%2B7hy1%2BX0LHueNj%2FWLRcOe92Gu1iy7YfPVn%2Bwu1VfuFazrSiq5aMrM7XrO6QNagHdJGG%2BxlGOqxivu1Vi8j3QNDtfkFy3kJeKxbeyT2o1TdDlan58bDNtZpf%2F1CaX0G3dHYod1VEaQ%2B7hCAqyn1EB9menWGQAAWKaQSRoT8z7ddfYUTQr7CZqIy%2F4EFyl8kuYXvjfm5IZB7JI5n%2BsaRTqdrTbtCnyN3GDFtE9PIb95nzlz%2Bb5yg8iNj2jYyIMKNoEcBS8Zx5TYwFgs2vUZP48b46hjkBXkLxSh9cCbb6wg9oV4jw2Vff9Gsx594qoCogiPVnZH36IfvRFBtjYNU5vrqq7K8HEiI73X6xPVfVSth0pVbajzD9TfbqTnqvlnvUDrRVFzbS3s8vF4ypFrHTfkXeE%2BWbc7KeSNfGwrVMiH81agd39Xhz8nV%2FYzNqiKuTN6M2O12Jnnl05456VL5x0i7jdlGBv3ou479DmeeKyPwbuHzawEDGD7ffahpYhqfMokW1blt3PAPZBGD0Hg70xiE4S%2BSgYVIcmKU35jBtpXaKHCbpPG42laNzGEk%2B88lH1zY3FkzSCe07iCqFHcq9Y5kVpGjvf6C9bLTnlQs4PNalRQlaO2O91GIQIh1o2xGCtm14%2FablIHZb%2F2F%2B%2BQ6Ip6%2Beex56jT3gQmijH%2Bs5EevY4lsJtz4z1WpY8Pmm2k0QXTiCgnGRxYNbkI%2FvsU727bHHSiwUlHtWb4GkA%2F2QP3c0iM6kLgfI2FQG519rkefwgHKPF5%2FT5rrVqYaoiHHS7WY6vveQcs%2B4uzBGzu0%2FLp7X7%2B77QV8fdervPha3TF2qV5jLHWu7yx32Vr6Tvacd3eMJJqgAnUyB8IM5LqrpVcOnAgGJwF4N%2FELtVVNhDklXBIvGuMeM2DRlTBZTmBxLWGKDKq%2B%2B7TaW%2BuQs%2FPaT7TzbtXhYE2Pgm0DkwHlAzIRvOTpl%2BdEGtC7Vaat0moplx4gWtbYkq0PtS2qfqWp%2FX%2Bu%2FLVn%2FCcDtNxUm4kB%2FbiqMVBFR04j4KEu3ix7Sl7hMpJDfVQ3ZaUH2s%2BOCs6o5jJ0XKGEAMeuUIY8dz8BenTRHTJutFVriALx67ehOvNRnGDXVxC34Fz3hIsOI%2BtYK1cMRPfKUmPIjHyTB9LzMTHq%2B2ZNJVnPA8%2FhgidxhWUzcFzpUUk9Fc449pWVPPBg7xmuqMbU9BQZvCUEiBjDDNk%2BomqAujP0KneSS4Abu%2FX6yzcA7W11PTJsm10YBqmFWFKJEJ0uaCr3jMd%2B5H2DXj%2Be1rkacnkPuNpx%2BDBrTEMmB0Yb9%2FyEDg0YZtUF7uGLKjVnYv1DU7ybqnPRbaamoJ9nh%2ByVEIsiLyZ1MecdtNd%2Btisltv%2FWqqqQm7Alox9G4jxJaeGMXiSxEzwYK0Jr99UhaMM2NXSmlyqc303WxcfDUgZQ2aqN5mOJHNsMlaJ20ct0kXXxKsLGuCRI74Ph9NHctrDbu1QaFcWP6Vu1xaluMs5wN7EhB7alCGBKlVVoHY1%2FFu3j13spESacTNCDzVojKrBArG6N5jJFRwZrlZYDQDLwtyhuG8JEoTI3uVXMH4lTojRs7PGXiHlOwzYHC6A3mlxHcIR%2BZidH6KZoT0uvtvn7k9phjyoSiN6R7WHdIOdYXfgBKseLhRwwto2H09zkJGzFBLFqWkcled4xVcUvCWp6M0ELPg98P6Fnl%2BQAwkY0dq%2F34%2Bjwlx6poYpcqkwd0rMrPnsjciVMb3m6FS7%2FgKa2gGhFxRsXUceZOAhu8iCTpkRHx8yVYE6cGJhI0B8lNaW4aBuWSMqoQOWcpG4p4iIQmqaYq20%2FKSHeQEsExE93NULMcOgmxrJKO1KpLQCUQZ9IEpvLA2DjXakmos1VCHQYpdWZnc0dK201AwKnHt8iVPkfxufBprQBGbFHO1bvlMD0xmllt94ohcW8sJq3xXYapBFQEYaFfBldTZB4C6hB4zygTnbtqpy8rpSBFWgkeeWnU1e4BqO8z6koKrKJKxUcp50jEi5VyJut8Yk4XHo%2FDqaLd5aBZ2iFAoB0ea4SXmbWZ5Q8dPeH6OBBzPYzmY8jizgKX5IkKwCoh1IjAk9moj1EsnFYPyARs4m62uJwCc3Z9a3qiD820d7wnOLCEgtRGUyrSrbHhb1BHe32Dx8Ih%2FQ2%2Bm1vUe7MBEkCghQXs3GQjYND3rYUnhJeAEdcEu7DtxMVdxV%2F%2FEUwL0tFTUtgRjbxbiJK0BJQXmteqeJfsbrI2Omy%2BfHFGL4aFKIQwzU%2FUqPWEqb17BRifiBamPfXPpBXRs9W1XDEo3%2FxUgbCFhIOnyesvr%2FXwdDYXK4pYmqRQ1CRqXEK6LC26M09A%2BwjuFJ0JaTScmJDf3VzKz0ou3NS9IKWzXlFFoHccwT%2BSqFl5twHs81%2BgIF3RLC7dcV%2FpDP0nypahlh1h0rVYFCFr5Ma8%2Fbsa5qYP0ZLIxs7Cp9IgnRwVjCOXgyZ1OmRj8eR8Dm1VtN51Osf1OUhL2h25us5GPIdcfMOeGR7RkuBDqsiHtk5t3craUDSKjTs7Dx%2FGJsX97gfcVwP3O%2Be3l4v6vI2oPMxvl9ScyC%2FtsOPRt0xSLiZsSoMls6TNhWe9DjykPwE01zFp8TD71BnzGyC6OFdv9UQQttqSKvKSXK0mP4m%2B9AJpR7UMV7hAWl7ds%2FXhJkc7x9xEw7vl893NTXPYt97uH2%2FsN62EnOs9lp84cPWJfaKdR0Mem08nikm0WJ5D1YpP0KjFeMhi8doToYSeqC%2BhIPgij4V0Hd9kNqb96yyxwhPRdPKio7LJ7OQUlWRwVIsXgKtStS21QoGWlXKA5hVPWs%2Fvugfa5jat4AdHTvwYFeYnF2E2Y%2BxEi4UfZnKkT6k4IC9h49HpZPK5SRYST46bJKv3taXBE8fnJ39mLf%2FCBtEqM4br4odCZjOG1Ok2x%2BEL1xebF7c6UXtogjG0%2Bq0jMwZpdFz6EGBqt66tvKJZtouN4F0CQLs9MRmmIxHbZF6y1ubQLAa6E6xRwZLjh2GlhZwgiT%2BtFEUuMP7YMguVHVjtePUfSuBk%2FY64NXTSWRjSEOlma3dOJj%2FpKTsXZ328WSeZhhFbuhywsVAeALFs3aI5YMIe%2B%2FDHjYdn8n0m8Uz6WsklLWno5gYjTFBSBYJvkjuiPGFCk2yJJSRMyAlJkxBSAQFiq1CbfFL%2Bc2NtpNVctHzXN7bGzvPRdb8SjedlFvEufBh4Re3pzbaSIKnd%2FJ5Sn83u7vU4ylimzOnR3fZUJjvM6KSorN%2BLU9m%2BvTbFQgx3r2H1DspHCkfnSGlnLcVKgdssSLDCwSwHP8tg94NLKudMPlQNi52wHtpUD29CZcUCJkgXdZZV4vL69DDeENaxkBYj2KR%2FZsx9gCO7a1D1Jn6Awdq6BMIgsizByiYHg4sfOrwl%2BBcd46%2FChx5k02HJZuFCOltKtZCso%2BJmYVU7slk4u%2Fpvjm0zrTZ7dKCbVS4QrIwJJh1DBV3kg8j8ybOpa1pzSP%2BTcrx4EYNOchMUQ%2BniKdqk06sO%2FKtl5HNPPWSYBNmJZkJnUD6Sst8pFJPkrHiGDGoVUGpCzJ5SDjklaybw0BmxbKjEBKB223vab3OMSVm0k2kUom8trGSLtcqLQ8%2Bkx7D2ozR3H7xe8aReK5WSFesMsww%2FVsKMkCNBlx%2BI9R3yevg7OoEAnF20k5z38ibGjkFI%2B%2FMLjYO9nXL65bxMGvPGQ3rQF1ZYIyG11bBCT5MFEYtoaiRZUQH2QiDSKnErj2QOOygwo%2FiJh%2FEDH9KYKvgF6bTXsKbdKz%2BwHWjnug%2B78Y5WU%2BQd%2FXRsr9QWpm7hUoNQM8cJ4todmdbsqwPBb82r%2FwM%3D). In case of paired end reads, corresponding fastq files should be named using *.R1.fastq.gz* and *.R2.fastq.gz* suffixes. Specify the desired analysis details for your data in the *essential.vars.groovy* file (see below) and run the pipeline *dnaseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). A markdown file *variantreport.Rmd* will be generated in the output reports folder after running the pipeline. Subsequently, the *variantreport.Rmd* file can be converted to a final html report using the *knitr* R-package.\\nGATK requires chromosomes in bam files to be karyotypically ordered. Best you use an ordered genome fasta file as reference for the pipeline (assigned in *essential.vars.groovy*, see below).\\n\\n\\n### The pipelines includes\\n- quality control of rawdata with FastQC\\n- Read mapping to the reference genome using BWA\\n- identify and remove duplicate reads with Picard MarkDuplicates\\n- Realign BAM files at Indel positions using GATK\\n- Recalibrate Base Qualities in BAM files using GATK\\n- Variant calling using GATK UnifiedGenotyper and GATK HaplotypeCaller\\n- Calculate VQSLOD scores for further filtering variants using GATK VariantRecalibrator and ApplyRecalibration\\n- Calculate the basic properties of variants as triplets for \"all\", \"known\" ,\"novel\" variants in comparison to dbSNP using GATK VariantEval\\n\\n\\n### Pipeline parameter settings\\n- essential.vars.groovy: essential parameter describing the experiment including: \\n  - ESSENTIAL_PROJECT: your project folder name\\n  - ESSENTIAL_BWA_REF: path to BWA indexed reference genome\\n  - ESSENTIAL_CALL_REGION: bath to bed file containing region s to limit variant calling to (optional)\\n  - ESSENTIAL_PAIRED: either paired end (\"yes\") or single end (\"no\") design\\n  - ESSENTIAL_KNOWN_VARIANTS: dbSNP from GATK resource bundle (crucial for BaseQualityRecalibration step)\\n  - ESSENTIAL_HAPMAP_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\\n  - ESSENTIAL_OMNI_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\\n  - ESSENTIAL_MILLS_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\\n  - ESSENTIAL_THOUSAND_GENOMES_VARIANTS: variants provided by the GATK bundle (essential for Variant Score Recalibration)\\n  - ESSENTIAL_THREADS: number of threads for parallel tasks\\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \\n\\n\\n## Programs required\\n- Bedtools\\n- BWA\\n- FastQC\\n- GATK\\n- Picard\\n- Samtools\\n'\n",
      " '# scRNA-Seq pipelines\\n\\nHere we forge the tools to analyze single cell RNA-Seq experiments. The analysis workflow is based on the Bioconductor packages [*scater*](https://bioconductor.org/packages/devel/bioc/vignettes/scater/inst/doc/overview.html) and [*scran*](https://bioconductor.org/packages/devel/bioc/vignettes/scran/inst/doc/scran.html) as well as the Bioconductor workflows by Lun ATL, McCarthy DJ, & Marioni JC [*A step-by-step workflow for low-level analysis of single-cell RNA-seq data.*](http://doi.org/10.12688/f1000research.9501.1) F1000Res. 2016 Aug 31 [revised 2016 Oct 31];5:2122 and Amezquita RA, Lun ATL et al. [*Orchestrating Single-Cell Analysis with Bioconductor*](https://osca.bioconductor.org/index.html) Nat Methods. 2020 Feb;17(2):137-145.\\n\\n## Implemented protocols\\n - MARS-Seq (massively parallel single-cell RNA-sequencing): The protocol is based on the publications of Jaitin DA, et al. (2014). *Massively parallel single-cell RNA-seq for marker-free decomposition of tissues into cell types.* Science (New York, N.Y.), 343(6172), 776–779. https://doi.org/10.1126/science.1247651 and Keren-Shaul H., et al. (2019). *MARS-seq2.0: an experimental and analytical pipeline for indexed sorting combined with single-cell RNA sequencing.* Nature Protocols. https://doi.org/10.1038/s41596-019-0164-4. The MARS-Seq library preparation protocol is given [here](https://github.com/imbforge/NGSpipe2go/blob/master/resources/MARS-Seq_protocol_Step-by-Step_MML.pdf). The sequencing reads are demultiplexed according to the respective pool barcodes before they are used as input for the analysis pipeline.  \\n- Smart-seq2: Libraries are generated using the [Smart-seq2 kit](http://www.nature.com/nmeth/journal/v10/n11/full/nmeth.2639.html). \\n\\n## Pipeline Workflow\\nAll analysis steps are illustrated in the pipeline [flowchart](https://www.draw.io/?lightbox=1&highlight=0000ff&edit=_blank&layers=1&nav=1&title=scRNA-Seq#R7R3ZcpvK8mtUlTxIxSIh6dF2oiyVODm2Uzk5LykEI4kYAWGxrXz9nZ6FdUBIQoDjm%2BTeIwYYZqZ7eu%2BegXq1fXrn697ms2sie6BI5tNAfTNQFFmdTPB%2FoGVHW6bKjDasfctkDyUNt9YfxBol1hpZJgoyD4aua4eWl200XMdBRphp033ffcw%2BtnLt7Fc9fY0KDbeGbhdbv1tmuGGtsjZPbrxH1nrDPj1TpvTGUjfu174bOex7jusgemer827YHIONbrqPqSb17UC98l03pL%2B2T1fIhmXlK0bfW5TcjYfsIyes88J76eNqcz0L7YfNzfL6%2Fe3i55%2F3Qw6AB92O2FoMFM3GHV6a1gOsrm2tHXJD%2Bx3BUC99sgzxJf61Zv8lry39fAseEumLt5LFCHd87Tfh1sa%2FZHzP1pfIvoyX9Mq1XZ88pC7IH%2FxIEPrufQwkvIiXK9cJGUbJGoxbDzbIZD2SfuKrlWXbqU4R%2BRN3yu8QGKqXhru1DHwp4d9rWw8C9juGo0T6D%2FXQcmGFhnMpnl4aKgxQD8gP0VOqiUHpHXK3KPR3%2BBF2dzbR6CtsMw1nDAsfU6g5ZW2bFFaOJZVtCbYd1nHfCV7gHww1xGjyYFz%2Fc%2Fs9%2Bhi9%2BfPf3Ye7xY%2Fg4b%2Bh%2FKLx5K0Gf8vwZO3rpoWhfQz6NIAtykzKYEtMU1PYMpYE2KLxB0%2FBlj%2Ffwzd30r%2F%2FfVzcvPW3snn3bvdhOJELwEMmprfs0vXDjbt2Hd1%2Bm7SmIAALkzzzyXU9BpZfKAx3DIR6FLq4KYUU6MkK%2F4XXRxN29SN1580T65lc7PiFg%2Bebegkuf6TvJa%2BRq%2BQ98wLYDr40ALYAaWhcWDYfThEBMwhXCvnAjXwDVexFtsdD3V%2BjsOo59iAsfSUi%2BcjGNOwhywpFSEFexfPWd6kHPNdywiDV81doSPBzPJtk8VMZZ9lU7vnJbFb1PP5BR5CgZzyVWhj75e7L%2Fa%2FN7Ofvf%2B9CZfHz%2BrusLYez54KwRaw6PyKNZ00j0km8SN3DiRIgctYDlH8YEDBc4AdkzXsq50vwcK1exsJerpTBxeVWtxwAiuUh28LcADdeptkc%2FcY%2Bflh7JJXzCTzdybfpwAp8tMr0uglDkLYvADrKYm2FmMuOfNsbORg7lIW1Xa5cjCn45%2FW7W5iasnbxxRD%2Fb2m7S%2BDe6AH0ggWfN6YKi8C4ub4I0G%2F4udX9EP8c8fsjzL7dh1167JLBWGXSiFF%2BGPqRY%2Bgh3m2FeVb0Gq%2FrIlkAKfUvhocuEEVE61YCurzg0gU2VvbSDbwxYILGwV3e6UnQbh6sYvLQCJEZEmJshfoSv50iM1j%2FRivLsYhCokivlnDndaXA%2Fezmbbu6GRBeE3jIINwGa%2BEwUdC5XTOyAQ2f%2F%2FwQxnIntHRgh7pjxpMbwrStFRY4CdPXMU9F%2FlETbgAHn5ARhbDeKRwsGUm5MncGHUgTqEBc20mrQPK4ARVILDMJhJTcEmBF0IOfWFH0sMoIk6cCI5fuZrwhtjyNhctVKSXVXkO5sGSKYMV422E6Q0HIHypZpWCa7cBdrQJgQjkgHCboiw0Zky4lff77R0bq36eaDtKKaaKnlqimTWoIslJX12xJQ%2FgWIP%2FL8heYgDGpBCtQPQ1g6T4BRbOcNaVpS9c3kT%2FEzSAaEEmBIT6heBLG8kl8J23jvSCihIrG8Dd%2BwtNNM%2B5b6Z8kRuEXjMKnjEHucOkr29FJEpeAI3GBpgi%2FcuDkmRXaZmeIUQdL8gaBTqpDqfBUDMPUU0o5IMOla%2B4KjQURKzRjPUXfejYaVAljycPF13mLxRsMSu14t5KDRYFUX1at%2FqGxOOaKWSx133DN5qcR9yu98tHvyPIB%2BzBwcf%2FS54ub2%2BEtIHL57GoO5OD5eq5rNz5Z2qn04U3ldF93Ml9MnxuELuuOTdWJtkvkdzIvsNU2PivSqfTKxry%2Fo2kZTWInnxbuNNo6HQOMEO7m5iZhfWkhk%2F%2BDn9IrA%2FMNn2zDR%2FhSJ3MEocJrbo6sOwn8EWtni5ywi0mNRqPmpqQ7IDW64QZ0XMnUQ%2F1cU4JGEUsn0hMRS9KabLWOoaSEX6aUpPQNUMksQ7cvmMc0BCXikvtPbbSCz7j4qZVNHEwr4lNKqxOPGytEt1iqgR4ffd2Lx3WaFq1kdDJ5pglUQpFOKM8PFuTxZUqWP0CBUzp11Tw3BY6D9NkocJ5Q%2FN%2Fi4VsOlfAlL9HZaPuQ7CC4p6XuYaQPh2xXXRAZ36GWsrJ4BN7wNTGnigxsy1pUx8u3bQpPHTVVSh7g5kysZl6mNK9ulM3Ycjl60P2gEaN%2FWZcnKaAFEJ0HIHkhy3cfLKLopC28IkSrZG%2BXsUH4Gr%2BYWRZmoyj9Ln0N5iq0JEuvXC8cHS4PpZZzL4dUqjnkqbyQKfyfyOuNxdlkueNYntbjjvIRQVnHc0eRc7xVuzP%2FYG8Nz%2BMsGM9geBZGmMjqy5Nbmgp4qh3xpPVEiollhIUehP9cEZJcR4D4HWHSF%2B6IDs7VU3cFA9YfybCJbxAGi%2Fnu%2FYB50nL8fBMtR3hvV%2FByxse3eHBA8Rex2xQeA88dvvHbGNHmmNvupeoUTqVUXUR3cpQ%2BS7hVrYS%2Bk0hO1rGURfgGCP1EygWgjYuUXtYEJEo73JlYh9ALQyy5ylVCTpC9dB9boCRKNSk5H0kY1yQJE6lpknBkEGTWQT3WpBxWnBbUKMSReDVfNo40DPpiwKqcg62a82ZTJGVvJUA%2FFYfU6bRRHBKKLT1yl%2B%2FBozKxRe1MbKlLo8Z9CdTO0ygljV%2F78XE8OT8%2Bys8mtaDRSO3aqNSWBFyAlqb52q%2FVxtve6xvVfHKt%2B2t1qPWHehzKhFoAljrvxb6fqrl9n%2BUr%2B59vOEHj9uHi8Z186%2F78cjn%2F8k19%2BrrZ3g97pD3%2FvXxo2gt8PJUPTWYn8aHauv3t3cVNbc3%2BBqvuxGjseRBkdE4dPsm3wBA%2FXIkfpyBzTiW%2B4QjguZxFmo6VdvHaMmk6ZZ0lfhMa3CR9TfwpOVoHvqzUQjNXVtYQAguZdy1vLdMkpDAHk6Tlhq2EOijJbiUJzsQBTskU2714WJNL%2FA%2Bv3hUw1wke5hW%2BlpNr%2FA8e98Mr18Fd6xYBLsII%2B4iCcFBMyG0ADdQZfDxrvSnggdBMr44PxoN6MtALlVi1ukyncQPNacmFWmGL1ncPkkQJ5iSUjnBFf4lCLwK2A%2FUrYINfFJgOvvnbqGI53fmeY7%2BiG0%2FDIhyJxTUm467jPzw9YCZNXnj%2B%2F0LfWjbg%2B3tkPyDoddBGZM2Uc6MKk7KQKh3BnWri%2BbRneC4VUN1HQWSHQT%2BR3cAEOo3i4DS3gELbOwJWDFVFirYWVLGh6WpE%2FjPR0Iw8G%2BMdPF2dmtdqfMKL35Iaz3XZuyUPD3aruSVnvd%2BStrvu6X7EIxvyvWi4to0l81RIPdIN2I%2BcQ%2FULwc%2BAzBO1Rf4ilnqVLqXerkx%2FXJjdK%2FXOO7PTVo67N6SnKPX6CFTJnlKfVsMumUixiFdkEWwsZ%2FczMHxHD9Dvn%2FQGVu5%2FhiTRaBEYI9o2utmap0Vi5rsqDbDMByv2eXHII%2FjGaINsD%2FnB6ObkRRJ2WViYfsWqfsXjxUQv3mywJX0wQqYrTBAmO0qNOI4O%2Feya1gpoFsEM8mAiChtREEJXmr4FRuosAyrHJZGnwLlpoYMQ40ewwcQgoLROkRyETJRGN1F2Yy%2BYeVvS6nxWj8HLSgMcXlzk7fREjF%2FR1uPPM3NjazGOcf029uGkeNtZnTFn8vUXnXS84h93zk%2FqBaU06B%2BxnBXy0ROmfhbJRavrKjGBGmwptQG7MaEJPFadRkeSmEjbWvo6Q%2BSz%2B1JykznYrTJuKzbyvLGQqtKtW%2BX%2B92P4aG23MlI%2Baab0dbwcXg6nz0XVODayOkN7yvSVHEURAL5xIqNpWa5UiHyj5LBAZIrUapyteKtMzxNCN82hMx9w6bjyocCnhdzVJp1BtASDoYJl1nDnoeAo2klMlKQ2E0nrwaQMZKudh9qhmPk5HE4yJ38FyRxLPQwf597O%2Fjslj63Q26hZp7Yzs%2FGQ45OMcMqzib%2FrQ650bSirPYmT4gSDG31Z2fjSOKnc89q4VWb2k7rMDgyYIi%2BRiCmw3mMNIGXVT9xrZ%2BRn6fq1dCKHMzLtWTKyfEjVRCnWg2iTkYnjwV9gOYimTBHTmvRuJveC3k3lrEljMt8jvOeeH8%2FboXdLfXsFxkRy5ExNamdgwkJqLy2t9Xdr3UaSJx6mwYZ5OEWbPkuKlhfNJ1q3ormQolUndv5fMhceTZCmYcJVbUswr00lLkzz2%2BcPl7Ts4R3esBLkiv%2FOBl6IiEa6Lr1uEh8J7mjAnSdJIUXmu7kh6wMfIGndBLN1k5gEiBn1Rq6MAcqWwW9H0MLTwrIdm0noknEfTqNm7dOo3LlBqwn8HRSPCML3luTvIB91je9o5A%2FsY%2Fh%2BCNtm0lQNk5xIpwhKfHVuzv3rJbr%2BmnOztVEm8tHmXDWLZi2Zc%2FmA%2B2bOXSMHXbrmDouFSm2R0OcKMBMlMbl%2BIFQbeoMPImcNrvU2bLnwSSiOiAejHEyIJ9KgdUJ8BmFxPOtWWBRb%2BP5631ej0mLtzNx%2BWvg0tZ8WPiysHWbcw9hHKtXS0DAeS8%2FKGxUE2cCzrTA8d9pkSv7k8zlc5uwgeTInJVaJmmXiabkIegaZU5O6lTkrTVy9CYAtxt6Hvm7c9zT%2BlVqtYMpskMxC%2F2AFUNEsoEfk9j7VpAFszwfqjQUmprNF4gstHz2qnNB%2FCWFWW0JoWkA4KX6fD%2Fso8nX2gso3CGLnHAN4%2BTvkuJlzSiqkhO7pGpVPSKEHDAG8JZ6I%2B3GdmUQ%2B0n2F9DDyc6fxkW%2Bsw1XbMd2FKO2kDjAL0LZdmowYkCkCTlcVWx4cWQ64weq%2BTcgkSoZIT%2BW60dRSA1RavIOLpSEKC3fewr0xDXm5hXvFKnaPHDIxzzzIISO3yEDngiAacYJgvzLg5uV1H3p%2FplwSaRMnZxFMqlYVXtr5a3RJssUpBuT8LbxT2flb1GOWy3HjKUz3mJXDWUj8GvN2Urs%2FrlNA%2FWk6QTZRJWdq19DxW7vACsg6Iy84%2FbScc53k8%2F%2BBnX5Q0HM492fOSxrE4lCxZpI8E7D22fR0aix0AT7%2F7PH97rs6x6%2Fy02rrqaNd8c54mJ1Yzz44dbLH9cfc4WE9sp7xuIy4lswfy%2FOQ%2BbpqZK3oio6%2BpaG3mIc9gE5IjpWnpj1PZ4dWIibC1M3NbVPXdV1bHt3IIxpBsv7T51EqR4zyWZtKc0q4ILq4ZR2c1%2BPtsRMA6odmEuF7RMaW%2BjYhYi%2FC1J9NWlTnNUtNnq%2FoTqdZXU3Y%2BqdtGvu5Y7KDGpWNhANMp4eFA0xnsxyKnSnhB2qf3LACLHVDAhi0QNag1mkiZWDKc2%2B6j84gqUkCE2oh14fVb4E5HBwGME%2Bjx%2FOJeMp76qfjolm8TU%2F9e%2BnjanM9C%2B2Hzc3y%2Bv3t4uef98NyR30BuYAPCPlc3mY1FrGzT2gNom2CvLS7AjtblrIzVrU5BSRhheV0VAZrKhR6Li3sLMKKLEVvhNNlCYlAVBvLFab%2FUxidEAmKpxwuYX8PUodbxgU5oa8VFUxsolTV8KoctxubWOlZdgvynZXegWPBSvPakY2v9CmOZS0vEqcsvnyFk0ay1iJtKVMxKlWJKgFY56LvkfLuIba%2FMyCXzGsE7cMudrBP89jVptOzEr1frs9TuCzP0BnGbO8L6tap2I4vze9VuRgv1ovT24F16F5qgMXMtWzmz0SQM8EjJDMcRjkTh%2BmyzL1FPRMLN2fa62Wg2wsw2OWRUyz%2FiLBTOxN2HlrxHSTfIR8ZQVHKkyp8DHV6EXfyIcSANNIR54k3grwWBSQfjaF5sUhrAZfLhIfCgDTxiN7QxEli%2F0E0XtGEoxSAd4sCLkh8RTXrL4yxEt9P1%2BjZnhBvnP3K%2FMb1rT94fnr8%2FVjGUJvZJAUlYVzcJMpsXtwkZ7Nq%2F%2FX5wEWbdrow7rratpC2cv%2BY2x%2FvrA9vl7e3T%2B7O3X388eXbsK6Rm5sQuq7ykjtdcpo9VXbv87ymx7mN3OyQCWKQ%2BATHYtQu9eJ6OwKP4B4%2BQU7UoPUZYjmBNXJx4YwW760VgA6iO8iN4JrNiljTYru3IrR8lyPbszN851N9px2XbBSegNrxMXIHEUJ88ZVUSibW64aPRT2eFAqSe6pIZvuhyULAF21izxPwchbwyplcvXUhX%2BkSbh%2F0wjjIjqu0ngv0x0L%2B0KDKuqggXPu2QiqrBlnLMB759u4S8m5hOvu4ZYId9Cqkqbnqm%2BH8wD1Xn72Op2ruUNaxoO6Qohb5qzptwPMgrhspdbu1zn4qa1UVtP26QGd1jIXD7lMd4x6f%2Fn4ayM9zMMnBMU6T3LklMyWNPnufV7STDn%2BvZ5GY9Acdz3K2XV1EElNWpReIlLcLqPuqxeafnyinIFJtO8JVFOqm7oW17Qe8JV2Y0Udb9wGsoKQrYpVluWyspk7ccRflHA02xao4unKUe3bWhHwYndrHKrN8l%2FY41j3EysS2r8Hu8eCklxf2nq1woyoyl%2B67C3wXBazkVjm1CoQeX8ZBJ6nCVwvyR8g508ut5eWzrFqVq7P1VoO%2FgxJnUb5gF2vGVM0yGBtfswMoCUvf6CbBAimrvzUU6jmf5tigKAJxKnD%2FqHzJGwcuZ8yl0UgHxBKStyI732LHns1%2F8m5EcoCS7dL8bohDxzwtQo5BE%2BNYkU6bnj5asfvtQiZX0nIHtIT0d0Xoj%2BfRzmm%2BePbIeILT%2BFcQZl2zVf0X5pSfkkGhwGbBvvvPFSVrmIKT4Rw5uWvX3%2BLP%2F9Ez2YPx2X0SCaGq19UHE3IQV8lKsIR681cUhOSIQZqCXzhjFE97BRuULuuD7lsQSUNSLS3HsEfJEhg7g0SCeRs9QK9rjusNySyV0BNeVJ%2BPjRUJ%2BHp1QcEaDm%2Bv38Zj%2Fvb54msChVpfubKjgMOC%2Bossuh67ZPhBtBx6rhfZvKLPIeu6o8zvPi4ym9Rx46tDh1Cz2wvHAfKE9g7vYJx6Y61IRSkoUkQX3kdBwHEiLq9AyjFAkMKQFGIi3JqNBAi%2Fd%2BgAcGORdOTDCnJc56%2FJAZhP8vpRkTEImb7aRBaA0ELbIzX8SKtQI2e%2FdFApfDzOWl20ec5OW79SeNYw3FalcD7gvlUK%2FxzZoYU5b%2B2suWiLSTZhoTHDRu0diFUzSW6vf6VV5b5A82rtmLJqtjOBfNz1CQpyp7ECXdb8PNRDKFw%2BRtt6U3SlapD9rPj5NZ2l5aysdeQXJNIeGXNo3U7%2BBZDlNz5aZT5QIKGYio982xs5kIFTSkaHCSk1qXa1oAsC7fDYiF4mVTZTySFM608aMdYPQz9yIOTVLE5D3CFfW73Oep9%2FdbMG6HZXHKsJ%2BhY0v0aWW9Bb1VqLJp4vHdv2gpDjMxtZjFxPPUC6OAK%2BpIHqhMC5Xf%2FedokdZ4u1xzXRf1PjFemB5d0e0YCVV8KSggO%2B%2B6q5AeKF0LHYCQyHjYCD3I78TFYbCJgklxor9oMk4F8K9n8EUBQr6sjAYoOftdW5BmjxKQDWmtfr2JYiuptUOY4TwuPtGr%2BIBcGAFT6mDelSyLSWY2JMoAiOf3x2A2poChHYG%2B4RAvxNFjFA5KyKpK5jyUxa3QZnTEw2fde746ISDK48Nf40yVvJCN4TVZSKK6oXNW0ge0csZosM07mVPCl9ec96lcuFpYv43NOX8SVmMWH6cYxgm8%2BuCSr12%2F8B). Specify desired analysis details for your data in the respective *essential.vars.groovy* file (see below) and run the selected pipeline *marsseq.pipeline.groovy* or *smartsseq.pipeline.groovy* as described [here](https://gitlab.rlp.net/imbforge/NGSpipe2go/-/blob/master/README.md). The analysis allows further parameter fine-tuning subsequent the initial analysis e.g. for plotting and QC thresholding. Therefore, a customisable *sc.report.Rmd* file will be generated in the output reports folder after running the pipeline. Go through the steps and modify the default settings where appropriate. Subsequently, the *sc.report.Rmd* file can be converted to a final html report using the *knitr* R-package.\\n\\n### The pipelines includes:\\n- FastQC, MultiQC and other tools for rawdata quality control\\n- Adapter trimming with Cutadapt\\n- Mapping to the genome using STAR\\n- generation of bigWig tracks for visualisation of alignment\\n- Quantification with featureCounts (Subread) and UMI-tools (if UMIs are used for deduplication)\\n- Downstream analysis in R using a pre-designed markdown report file (*sc.report.Rmd*). Modify this file to fit your custom parameter and thresholds and render it to your final html report. The Rmd file uses, among others, the following tools and methods:\\n  - QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\\n  - Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\\n  - Differential expression analysis: the [scde](http://bioconductor.org/packages/release/bioc/html/scde.html) package.\\n  - Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\\n\\n### Pipeline parameter settings\\n- essential.vars.groovy: essential parameter describing the experiment \\n  - project folder name\\n  - reference genome\\n  - experiment design\\n  - adapter sequence, etc.\\n- additional (more specialized) parameter can be given in the var.groovy-files of the individual pipeline modules \\n- targets.txt: comma-separated txt-file giving information about the analysed samples. The following columns are required \\n  - sample: sample identifier. Must be a unique substring of the input sample file name (e.g. common prefixes and suffixes may be removed). These names are grebbed against the count file names to merge targets.txt to the count data.\\n  - plate: plate ID (number) \\n  - row: plate row (letter)\\n  - col: late column (number)\\n  - cells: 0c/1c/10c (control wells)\\n  - group: default variable for cell grouping (e.g. by condition)\\n  \\n  for pool-based libraries like MARSseq required additionally:\\n  - pool: the pool ID comprises all cells from 1 library pool (i.e. a set of unique cell barcodes; the cell barcodes are re-used in other pools). Must be a unique substring of the input sample file name. For pool-based design, the pool ID is grebbed against the respective count data filename instead of the sample name as stated above.\\n  - barcode: cell barcodes used as cell identifier in the count files. After merging the count data with targets.txt, the barcodes are replaced with sample IDs given in the sample column (i.e. here, sample names need not be a substring of input sample file name).\\n\\n### Programs required\\n- FastQC\\n- STAR\\n- Samtools\\n- Bedtools\\n- Subread\\n- Picard\\n- UCSC utilities\\n- RSeQC\\n- UMI-tools\\n- R\\n\\n## Resources\\n- QC: the [scater](http://bioconductor.org/packages/release/bioc/html/scater.html) package.\\n- Normalization: the [scran](http://bioconductor.org/packages/release/bioc/html/scran.html) package.\\n- Trajectory analysis (pseudotime): the [monocle](https://bioconductor.org/packages/release/bioc/html/monocle.html) package.\\n- A [tutorial](https://scrnaseq-course.cog.sanger.ac.uk/website/index.html) from Hemberg lab\\n- Luecken and Theis 2019 [Current best practices in single‐cell RNA‐seq analysis: a tutorial](https://www.embopress.org/doi/10.15252/msb.20188746)\\n\\n\\n'\n",
      " nan\n",
      " '### Workflow for Metagenomics binning from assembly\\n\\n**Minimal inputs are: Identifier, assembly (fasta) and a associated sorted BAM file**\\n\\n**Summary**\\n  - MetaBAT2 (binning)\\n  - MaxBin2 (binning)\\n  - SemiBin (binning)\\n  - DAS Tool (bin merging)\\n  - EukRep (eukaryotic classification)\\n  - CheckM (bin completeness and contamination)\\n  - BUSCO (bin completeness)\\n  - GTDB-Tk (bin taxonomic classification)\\n\\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\n  Tools: https://gitlab.com/m-unlock/cwl<br>\\n  Workflows: https://gitlab.com/m-unlock/cwl/workflows<br>\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\\n'\n",
      " 'Abstract CWL Automatically generated from the Galaxy workflow file: CLM-FATES_ ALP1 simulation (5 years)'\n",
      " 'RNA-RNA interactome analysis using ChiRA tools suite. The aligner used is BWA-MEM.'\n",
      " 'RNA-RNA interactome analysis using ChiRA tools suite. The aligner used is CLAN.'\n",
      " 'This WF is based on the official Covid19-Galaxy assembly workflow as available from https://covid19.galaxyproject.org/genomics/2-assembly/ . It has been adapted to suit the needs of the analysis of metagenomics sequencing data. Prior to be submitted to INDSC databases, these data need to be cleaned from contaminant reads, including reads of possible human origin. \\n\\nThe assembly of the SARS-CoV-2 genome is performed using both the Unicycler and the SPAdes assemblers, similar to the original WV.\\n\\nTo facilitate the deposition of raw sequencing reads in INDSC databases, different fastq files are saved during the different steps of the WV. Which reflect different levels of stringency/filtration:\\n\\n(1) Initially fastq are filtered to remove human reads. \\n(2) Subsequently, a similarity search is performed against the reference assembly of the SARS-CoV-2 genome, to retain only SARS-CoV-2 like reads. \\n(3) Finally, SARS-CoV-2 reads are assembled, and the bowtie2 program is used to identify (and save in the corresponding fastq files) only reads that are completely identical to the final assembly of the genome.\\n\\nAny of the fastq files produced in (1), (2) or (3) are suitable for being submitted in  raw reads repositories. While the files filtered according to (1) are richer and contain more data, including for example genomic sequences of different microbes living in the oral cavity; files filtered according to (3) contain only the reads that are completely identical to the final assembly. This should guarantee that any re-analysis/re-assembly of these always produce consistent and identical results. File obtained at (2) include all the reads in the sequencing reaction that had some degree of similarity with the reference SARS-CoV-2 genome, these may include subgenomic RNAs, but also polymorphic regions/variants in the case of a coinfection by multiple SARS-CoV-2 strains. Consequently, reanalysis of these data is not guarateed to produce identical and consistent results, depending on the parameters used during the assembly. However, these data contain more information.\\n\\nPlease feel free to comment,  ask questions and/or add suggestions\\n\\n'\n",
      " 'Scipion is a workflow engine mostly for Cryo-Electron Microscopy image processing. In this extremely simple workflow, we load the Relion 3.0 tutorial data and process it to 2.9A resolution.'\n",
      " 'Continuous flexibility analysis of SARS-CoV-2 Spike prefusion structures'\n",
      " 'Workflow to build different indices for different tools from a genome and transcriptome. \\n\\nThis workflow expects an (annotated) genome in GBOL ttl format.\\n\\nSteps:\\n  - SAPP: rdf2gtf (genome fasta)\\n  - SAPP: rdf2fasta (transcripts fasta)\\n  - STAR index (Optional for Eukaryotic origin)\\n  - bowtie2 index\\n  - kallisto index\\n'\n",
      " 'Workflow for NonSpliced RNAseq data with multiple aligners.\\n\\nSteps:  \\n    - workflow_quality.cwl:\\n        - FastQC (control)\\n        - fastp (trimming)\\n    - bowtie2 (read mapping)\\n    - sam_to_sorted-bam\\n    - featurecounts (transcript read counts)\\n    - kallisto (transcript [pseudo]counts)\\n'\n",
      " 'Workflow for Spliced RNAseq data\\n**Steps:**\\n\\n* workflow_quality.cwl:\\n\\t* FastQC (Read Quality Control)\\n\\t* fastp (Read Trimming)\\n* STAR (Read mapping)\\n* featurecounts (transcript read counts)\\n* kallisto (transcript [pseudo]counts)\\n'\n",
      " 'CWL version of the md_list.cwl workflow for HPC.\\n'\n",
      " 'Galaxy version of pre-processing of reads from COVID-19 samples. \\nQC + human read cleaning\\nBased on https://github.com/Finn-Lab/Metagen-FastQC/blob/master/metagen-fastqc.sh'\n",
      " 'Non-functional workflow to get a global view of possibilities for plant virus classification.'\n",
      " 'Metagenomic dataset taxonomic classification using kraken2'\n",
      " 'Mapping against all plant virus then make contig out of the mapped reads then blast them.'\n",
      " 'Just the cleaning then assembly of all reads. TO explore further follow one of the paths described in \"Global view\" (WF 0) '\n",
      " 'A workflow for mapping and consensus generation of SARS-CoV2 whole genome amplicon nanopore data implemented in the Nextflow framework. Reads are mapped to a reference genome using Minimap2 after trimming the amplicon primers with a fixed length at both ends of the amplicons using Cutadapt. The consensus is called using Pysam based on a majority read support threshold per position of the Minimap2 alignment and positions with less than 30x coverage are masked using ‘N’.'\n",
      " 'A pipeline for mapping, calling, and annotation of SARS-CoV2 variants.'\n",
      " 'Rare disease researchers workflow is that they submit their raw data (fastq), run the mapping and variant calling RD-Connect pipeline and obtain unannotated gvcf files to further submit to the RD-Connect GPAP or analyse on their own.\\n\\nThis demonstrator focuses on the variant calling pipeline. The raw genomic data is processed using the RD-Connect pipeline ([Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/27604516)) running on the standards (GA4GH) compliant, interoperable container orchestration platform.\\n\\nThis demonstrator will be aligned with the current implementation study on [Development of Architecture for Software Containers at ELIXIR and its use by EXCELERATE use-case communities](docs/Appendix%201%20-%20Project%20Plan%202018-biocontainers%2020171117.pdf) \\n\\nFor this implementation, different steps are required:\\n\\n1. Adapt the pipeline to CWL and dockerise elements \\n2. Align with IS efforts on software containers to package the different components (Nextflow) \\n3. Submit trio of Illumina NA12878 Platinum Genome or Exome to the GA4GH platform cloud (by Aspera or ftp server)\\n4. Run the RD-Connect pipeline on the container platform\\n5. Return corresponding gvcf files\\n6. OPTIONAL: annotate and update to RD-Connect playground instance\\n\\nN.B: The demonstrator might have some manual steps, which will not be in production. \\n\\n## RD-Connect pipeline\\n\\nDetailed information about the RD-Connect pipeline can be found in [Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/?term=27604516)\\n\\n![alt text](https://raw.githubusercontent.com/inab/Wetlab2Variations/eosc-life/docs/RD-Connect_pipeline.jpg)\\n\\n## The applications\\n\\n**1\\\\. Name of the application: Adaptor removal**\\nFunction: remove sequencing adaptors   \\nContainer (readiness status, location, version): [cutadapt (v.1.18)](https://hub.docker.com/r/cnag/cutadapt)  \\nRequired resources in cores and RAM: current container size 169MB  \\nInput data (amount, format, directory..): raw fastq  \\nOutput data: paired fastq without adaptors  \\n\\n**2\\\\. Name of the application: Mapping and bam sorting**\\nFunction: align data to reference genome  \\nContainer : [bwa-mem (v.0.7.17)](https://hub.docker.com/r/cnag/bwa) / [Sambamba (v. 0.6.8 )](https://hub.docker.com/r/cnag/sambamba)(or samtools)  \\nResources :current container size 111MB / 32MB  \\nInput data: paired fastq without adaptors  \\nOutput data: sorted bam  \\n\\n**3\\\\. Name of the application: MarkDuplicates**  \\nFunction: Mark (and remove) duplicates  \\nContainer: [Picard (v.2.18.25)](https://hub.docker.com/r/cnag/picard)\\nResources: current container size 261MB  \\nInput data:sorted bam  \\nOutput data: Sorted bam with marked (or removed) duplicates  \\n\\n**4\\\\. Name of the application: Base quality recalibration (BQSR)**  \\nFunction: Base quality recalibration  \\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\\nResources: current container size 270MB  \\nInput data: Sorted bam with marked (or removed) duplicates  \\nOutput data: Sorted bam with marked duplicates & base quality recalculated  \\n\\n**5\\\\. Name of the application: Variant calling**  \\nFunction: variant calling  \\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\\nResources: current container size 270MB  \\nInput data:Sorted bam with marked duplicates & base quality recalculated  \\nOutput data: unannotated gvcf per sample  \\n\\n**6\\\\. (OPTIONAL)Name of the application: Quality of the fastq**  \\nFunction: report on the sequencing quality  \\nContainer: [fastqc 0.11.8](https://hub.docker.com/r/cnag/fastqc)\\nResources: current container size 173MB  \\nInput data: raw fastq  \\nOutput data: QC report \\n\\n## Licensing\\n\\nGATK declares that archived packages are made available for free to academic researchers under a limited license for non-commercial use. If you need to use one of these packages for commercial use. https://software.broadinstitute.org/gatk/download/archive '\n",
      " 'Rare disease researchers workflow is that they submit their raw data (fastq), run the mapping and variant calling RD-Connect pipeline and obtain unannotated gvcf files to further submit to the RD-Connect GPAP or analyse on their own.\\n\\nThis demonstrator focuses on the variant calling pipeline. The raw genomic data is processed using the RD-Connect pipeline ([Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/27604516)) running on the standards (GA4GH) compliant, interoperable container orchestration platform.\\n\\nThis demonstrator will be aligned with the current implementation study on [Development of Architecture for Software Containers at ELIXIR and its use by EXCELERATE use-case communities](docs/Appendix%201%20-%20Project%20Plan%202018-biocontainers%2020171117.pdf) \\n\\nFor this implementation, different steps are required:\\n\\n1. Adapt the pipeline to CWL and dockerise elements \\n2. Align with IS efforts on software containers to package the different components (Nextflow) \\n3. Submit trio of Illumina NA12878 Platinum Genome or Exome to the GA4GH platform cloud (by Aspera or ftp server)\\n4. Run the RD-Connect pipeline on the container platform\\n5. Return corresponding gvcf files\\n6. OPTIONAL: annotate and update to RD-Connect playground instance\\n\\nN.B: The demonstrator might have some manual steps, which will not be in production. \\n\\n## RD-Connect pipeline\\n\\nDetailed information about the RD-Connect pipeline can be found in [Laurie et al., 2016](https://www.ncbi.nlm.nih.gov/pubmed/?term=27604516)\\n\\n![alt text](https://raw.githubusercontent.com/inab/Wetlab2Variations/eosc-life/docs/RD-Connect_pipeline.jpg)\\n\\n## The applications\\n\\n**1\\\\. Name of the application: Adaptor removal**\\nFunction: remove sequencing adaptors   \\nContainer (readiness status, location, version): [cutadapt (v.1.18)](https://hub.docker.com/r/cnag/cutadapt)  \\nRequired resources in cores and RAM: current container size 169MB  \\nInput data (amount, format, directory..): raw fastq  \\nOutput data: paired fastq without adaptors  \\n\\n**2\\\\. Name of the application: Mapping and bam sorting**  \\nFunction: align data to reference genome  \\nContainer : [bwa-mem (v.0.7.17)](https://hub.docker.com/r/cnag/bwa) / [Sambamba (v. 0.6.8 )](https://hub.docker.com/r/cnag/sambamba)(or samtools)  \\nResources :current container size 111MB / 32MB  \\nInput data: paired fastq without adaptors  \\nOutput data: sorted bam  \\n\\n**3\\\\. Name of the application: MarkDuplicates**  \\nFunction: Mark (and remove) duplicates  \\nContainer: [Picard (v.2.18.25)](https://hub.docker.com/r/cnag/picard)\\nResources: current container size 261MB  \\nInput data:sorted bam  \\nOutput data: Sorted bam with marked (or removed) duplicates  \\n\\n**4\\\\. Name of the application: Base quality recalibration (BQSR)**  \\nFunction: Base quality recalibration  \\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\\nResources: current container size 270MB  \\nInput data: Sorted bam with marked (or removed) duplicates  \\nOutput data: Sorted bam with marked duplicates & base quality recalculated  \\n\\n**5\\\\. Name of the application: Variant calling**  \\nFunction: variant calling  \\nContainer: [GATK (v.3.6-0)](https://hub.docker.com/r/cnag/gatk)\\nResources: current container size 270MB  \\nInput data:Sorted bam with marked duplicates & base quality recalculated  \\nOutput data: unannotated gvcf per sample  \\n\\n**6\\\\. (OPTIONAL)Name of the application: Quality of the fastq**  \\nFunction: report on the sequencing quality  \\nContainer: [fastqc 0.11.8](https://hub.docker.com/r/cnag/fastqc)\\nResources: current container size 173MB  \\nInput data: raw fastq  \\nOutput data: QC report \\n\\n## Licensing\\n\\nGATK declares that archived packages are made available for free to academic researchers under a limited license for non-commercial use. If you need to use one of these packages for commercial use. https://software.broadinstitute.org/gatk/download/archive '\n",
      " 'COVID-19: variation analysis reporting\\n--------------------------------------\\n\\nThis workflow takes VCF datasets of variants produced by any of the\\n\"*-variant-calling\" workflows in\\nhttps://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling\\nand generates tabular reports of variants by samples and by variant, along with\\nan overview plot of variants and their allele-frequencies across all samples.\\n'\n",
      " 'COVID-19: variation analysis on ARTIC PE data\\n---------------------------------------------\\n\\nThe workflow for Illumina-sequenced ampliconic data builds on the RNASeq workflow\\nfor paired-end data using the same steps for mapping and variant calling, but\\nadds extra logic for trimming amplicon primer sequences off reads with the ivar\\npackage. In addition, this workflow uses ivar also to identify amplicons\\naffected by primer-binding site mutations and, if possible, excludes reads\\nderived from such \"tainted\" amplicons when calculating allele-frequencies\\nof other variants.\\n'\n",
      " 'COVID-19: variation analysis on ARTIC ONT data\\n----------------------------------------------\\n\\nThis workflow for ONT-sequenced ARTIC data is modeled after the alignment/variant-calling steps of the [ARTIC pipeline](https://artic.readthedocs.io/en/latest/). It performs, essentially, the same steps as that pipeline’s minion command, i.e. read mapping with minimap2 and variant calling with medaka. Like the Illumina ARTIC workflow it uses ivar for primer trimming. Since ONT-sequenced reads have a much higher error rate than Illumina-sequenced reads and are therefor plagued more by false-positive variant calls, this workflow does make no attempt to handle amplicons affected by potential primer-binding site mutations.\\n'\n",
      " 'COVID-19: variation analysis on WGS SE data\\n-------------------------------------------\\n\\nThis workflows performs single end read mapping with bowtie2 followed by\\nsensitive variant calling across a wide range of AFs with lofreq and variant\\nannotation with snpEff 4.5covid19.\\n'\n",
      " 'COVID-19: variation analysis on WGS PE data\\n-------------------------------------------\\n\\nThis workflows performs paired end read mapping with bwa-mem followed by\\nsensitive variant calling across a wide range of AFs with lofreq and variant\\nannotation with snpEff 4.5covid19.\\n'\n",
      " 'A porting of the Trinity RNA assembly pipeline, https://trinityrnaseq.github.io, that uses Nextflow to handle the underlying sub-tasks.\\nThis enables additional capabilities to better use HPC resources, such as packing of tasks to fill up nodes and use of node-local disks to improve I/O.\\nBy design, the pipeline separates the workflow logic (main file) and the cluster-specific configuration (config files), improving portability.\\n\\nBased on a pipeline by Sydney Informatics Hub: https://github.com/Sydney-Informatics-Hub/SIH-Raijin-Trinity'\n",
      " 'Workflow for tracking objects in Cell Profiler:\\nhttps://training.galaxyproject.org/training-material/topics/imaging/tutorials/object-tracking-using-cell-profiler/tutorial.html'\n",
      " 'Workflow to take DataOne data packages (raw datasets + metadata written in Ecological Metadata Standard) as input and create a DwC occurence.csv file almost ready to put in a Dawrin core Archive using eml-annotations at the attribute level'\n",
      " '# COnSensus Interaction Network InFErence Service\\nInference framework for reconstructing networks using a consensus approach between multiple methods and data sources.\\n\\n![alt text](https://github.com/PhosphorylatedRabbits/cosifer/raw/master/docs/_static/logo.png)\\n\\n## Reference\\n[Manica, Matteo, Charlotte, Bunne, Roland, Mathis, Joris, Cadow, Mehmet Eren, Ahsen, Gustavo A, Stolovitzky, and María Rodríguez, Martínez. \"COSIFER: a python package for the consensus inference of molecular interaction networks\".Bioinformatics (2020)](https://doi.org/10.1093/bioinformatics/btaa942).'\n",
      " '# COnSensus Interaction Network InFErence Service\\nInference framework for reconstructing networks using a consensus approach between multiple methods and data sources.\\n\\n![alt text](https://raw.githubusercontent.com/PhosphorylatedRabbits/cosifer/master/docs/_static/logo.png)\\n\\n## Reference\\n[Manica, Matteo, Charlotte, Bunne, Roland, Mathis, Joris, Cadow, Mehmet Eren, Ahsen, Gustavo A, Stolovitzky, and María Rodríguez, Martínez. \"COSIFER: a python package for the consensus inference of molecular interaction networks\".Bioinformatics (2020)](https://doi.org/10.1093/bioinformatics/btaa942).'\n",
      " '# Protein MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Lysozyme** protein (PDB code 1AKI).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " \"CWL version of the md_list.py workflow for HPC. This performs a system setup and runs a molecular dynamics simulation on the structure passed to this workflow. This workflow uses the md\\\\_gather.cwl sub-workflow to gather the outputs together to return these.\\nTo work with more than one structure this workflow can be called from either the md\\\\_launch.cwl workflow, or the md\\\\_launch\\\\_mutate.cwl workflow. These use scatter for parallelising the workflow. md\\\\_launch.cwl operates on a list of individual input molecule files. md\\\\_launch\\\\_mutate.cwl operates on a single input molecule file, and a list of mutations to apply to that molecule. Within that list of mutations, a value of 'WT' will indicate that the molecule should be simulated without any mutation being applied.\\n\"\n",
      " 'A workflow querying on an endpoint of a graph database by a file containing a SPARQL query.'\n",
      " 'This workflow is used to process timeseries from meteorological stations in Finland but can be applied to any timeseries according it follows the same format.\\n\\nTake a temperature timeseries from any meteorological station. Input format is csv and it must be standardized with 6 columns:\\n\\n1. Year\\t(ex: 2021)\\n2. month\\t(ex: 1)\\n3. day\\t(ex: 15) \\n4. Time\\t(ex: 16:56)\\n5. Time zone\\t(such as UTC)\\n6. Air temperature (degC)'\n",
      " 'Variant Interpretation Pipeline (VIP) that annotates, filters and reports prioritized causal variants in humans, see https://github.com/molgenis/vip for more information.'\n",
      " ' Joint multi-omics dimensionality reduction approaches for CAKUT data using peptidome and proteome data\\n \\n **Brief description**\\n In (Cantini et al. 2020), Cantini et al. evaluated 9 representative joint dimensionality reduction (jDR) methods for multi-omics integration and analysis and . The methods are Regularized Generalized Canonical Correlation Analysis (RGCCA), Multiple co-inertia analysis (MCIA), Multi-Omics Factor Analysis (MOFA), Multi-Study Factor Analysis (MSFA), iCluster, Integrative NMF (intNMF), Joint and Individual Variation Explained (JIVE), tensorial Independent Component Analysis (tICA), and matrix-tri-factorization (scikit-fusion) (Tenenhaus, Tenenhaus, and Groenen 2017; Bady et al. 2004; Argelaguet et al. 2018; De Vito et al. 2019; Shen, Olshen, and Ladanyi 2009; Chalise and Fridley 2017; Lock et al. 2013; Teschendorff et al. 2018; Žitnik and Zupan 2015).\\n\\nThe authors provided their benchmarking procedure, multi-omics  mix  (momix), as Jupyter Notebook on GitHub (https://github.com/ComputationalSystemsBiology/momix-notebook) and project environment through Conda. In momix, the factorization methods are called from an R script, and parameters of the methods are also set in that script. We did not modify the parameters of the methods in the provided script. We set factor number to 2.\\n'\n",
      " '# Protein-ligand Docking tutorials using BioExcel Building Blocks (biobb)\\n\\nThis tutorials aim to illustrate the process of **protein-ligand docking**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular examples used are based on the **Mitogen-activated protein kinase 14** (p38-α) protein (PDB code [3HEC](https://www.rcsb.org/structure/3HEC)), a well-known **Protein Kinase enzyme**,\\n in complex with the FDA-approved **Imatinib** (PDB Ligand code [STI](https://www.rcsb.org/ligand/STI), DrugBank Ligand Code [DB00619](https://go.drugbank.com/drugs/DB00619)) and **Dasatinib** (PDB Ligand code [1N1](https://www.rcsb.org/ligand/1N1), DrugBank Ligand Code [DB01254](https://go.drugbank.com/drugs/DB01254)), small **kinase inhibitors** molecules used to treat certain types of **cancer**.\\n\\nThe tutorials will guide you through the process of identifying the **active site cavity** (pocket) without previous knowledge, and the final prediction of the **protein-ligand complex**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# AMBER Protein MD Setup tutorials using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)** wrapping the **Ambertools MD package**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'ORSON combine state-of-the-art tools for annotation processes within a Nextflow pipeline: sequence similarity search (PLAST, BLAST or Diamond), functional annotation retrieval (BeeDeeM) and functional prediction (InterProScan). When required, BUSCO completness evaluation and eggNOG Orthogroup annotation can be activated. While ORSON results can be analyzed through the command-line, it also offers the possibility to be compatible with BlastViewer or Blast2GO graphical tools.\\n\\n'\n",
      " 'Downloads fastq files for sequencing run accessions provided in a text file using fasterq-dump. Creates one job per listed run accession.'\n",
      " 'COVID-19: consensus construction\\n--------------------------------\\n\\nThis workflow aims at generating reliable consensus sequences from variant\\ncalls according to transparent criteria that capture at least some of the\\ncomplexity of variant calling.\\n\\nIt takes a collection of VCFs (with DP and DP4 INFO fields) and a collection of\\nthe corresponding aligned reads (for the purpose of calculating genome-wide\\ncoverage) such as produced by any of the variant calling workflows in\\nhttps://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling\\nand generates a collection of viral consensus sequences and a multisample FASTA\\nof all these sequences.\\n\\nEach consensus sequence is guaranteed to capture all called, filter-passing (as\\nper the FILTER column of the VCF input) variants found in the VCF of its sample\\nthat reach a user-defined consensus allele frequency threshold.\\n\\nFilter-failing variants and variants below a second user-defined minimal\\nallele frequency threshold will be ignored.\\n\\nGenomic positions of filter-passing variants with an allele frequency in\\nbetween the two thresholds will be hard-masked (with N) in the consensus\\nsequence of their sample.\\n\\nGenomic positions with a coverage (calculated from the read alignments input)\\nbelow another user-defined threshold will be hard-masked, too, unless they are\\nconsensus variant sites.\\n'\n",
      " 'microPIPE was developed to automate high-quality complete bacterial genome assembly using Oxford Nanopore Sequencing in combination with Illumina sequencing.\\n\\nTo build microPIPE we evaluated the performance of several tools at each step of bacterial genome assembly, including basecalling, assembly, and polishing. Results at each step were validated using the high-quality ST131 Escherichia coli strain EC958 (GenBank: HG941718.1). After appraisal of each step, we selected the best combination of tools to achieve the most consistent and best quality bacterial genome assemblies.\\n\\nThe workflow below summarises the different steps of the pipeline (with each selected tool) and the approximate run time (using GPU basecalling, averaged over 12 E. coli isolates sequenced on a R9.4 MinION flow cell). Dashed boxes correspond to optional steps in the pipeline.\\n\\nMicropipe has been written in Nextflow and uses Singularity containers. It can use both GPU and CPU resources.\\n\\nFor more information please see our publication here: https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-021-07767-z\\n\\nInfrastructure\\\\_deployment\\\\_metadata: Zeus (Pawsey)'\n",
      " 'This repository contains the workflow used to find and characterize the HI sources in the data cube of the SKA Data Challenge 2. It was developed to process a simulated [SKA data cube](https://sdc2.astronomers.skatelescope.org/sdc2-challenge/data) data cube, but can be adapted for clean HI data cubes from other radio observatories.\\n\\nThe workflow is managed and executed using snakemake workflow management system. It uses [https://spectral-cube.readthedocs.io/en/latest/](http://) based on [https://dask.org/](http://) parallelization tool and [https://www.astropy.org/](http://) suite to divide the large cube in smaller pieces. On each of the subcubes, we execute [https://github.com/SoFiA-Admin/SoFiA-2](http://) for masking the subcubes, find sources and characterize their properties. Finally, the individual catalogs are cleaned, concatenated into a single catalog, and duplicates from the overlapping regions are eliminated. Some diagnostic plots are produced using Jupyter notebook.\\n\\nThe documentation can be found in the [Documentation page](https://hi-friends-sdc2.readthedocs.io/en/latest/index.html). The workflow and the results can be cited in the [Zenodo record](https://doi.org/10.5281/zenodo.5167659).'\n",
      " 'This is a Galaxy workflow that uses to convert the16S BIOM file to table and figures. It is part of the metaDEGalaxy workflow MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data.'\n",
      " \"Germline-ShortV @ NCI-Gadi is an implementation of the BROAD Institute's best practice workflow for germline short variant discovery. This implementation is optimised for the National Compute Infrastucture's Gadi HPC, utilising scatter-gather parallelism to enable use of multiple nodes with high CPU or memory efficiency. This workflow requires sample BAM files, which can be generated using the [Fastq-to-bam @ NCI-Gadi](https://workflowhub.eu/workflows/146) pipeline. Germline-ShortV can be applied to model and non-model organisms (including non-diploid organisms). \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)\"\n",
      " '# SLURM HPC Cromwell implementation of GATK4 germline variant calling pipeline\\nSee the [GATK](https://gatk.broadinstitute.org/hc/en-us) website for more information on this toolset \\n## Assumptions\\n- Using hg38 human reference genome build\\n- Running using HPC/SLURM scheduling. This repo was specifically tested on Pawsey Zeus machine, primarily running in the `/scratch` partition. \\n- Starting from short-read Illumina paired-end fastq files as input\\n\\n### Dependencies\\nThe following versions have been tested and work, but GATK and Cromwell are regularly updated and so one must consider whether they would like to use newer versions of these tools. \\n- BWA/0.7.15\\n- GATK v4.0.6.0\\n- SAMtools/1.5\\n- picard/2.9\\n- Python/2.7\\n- Cromwell v61\\n\\n## Quick start guide\\n### Installing and preparing environment for GATK4 with Cromwell\\n\\n1. Clone repository\\n```\\ngit clone https://github.com/SarahBeecroft/slurmCromwellGATK4.git\\ncd slurmCromwellGATK4\\nchmod +x *.sh\\n```\\n\\n2. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) if you haven’t already. This is best placed in your `/group` directory to avoid filling your small `/home` directory, or being purged is placed in the `/scratch` directory.\\n\\n3. Create Conda environment using the supplied conda environment file\\n\\n```\\nconda env create --file gatk4_pipeline.yml\\n```\\n\\n3. Download the necessary .jar files\\n    - The Cromwell workfow orchestration engine can be downloaded from https://github.com/broadinstitute/cromwell/releases/ \\n    - GATK can be downloaded from https://github.com/broadinstitute/gatk/releases. Unzip the file with `unzip` \\n    - Picard can be downloaded from https://github.com/broadinstitute/picard/releases/\\n\\n\\n4. If you do not have the resource bundle files already, these need to be downloaded. In future they will be cached on Pawsey systems. The bundle data should be download from the [Google Cloud bucket](https://console.cloud.google.com/storage/browser/genomics-public-data/references/hg38/v0;tab=objects?_ga=2.98248159.1769807612.1582055494-233304531.1578854612&pli=1&prefix=&forceOnObjectsSortingFiltering=false) and not from the FTP site, which is missing various files. Refer to this handy [blog post](https://davetang.org/muse/2020/02/21/using-google-cloud-sdk-to-download-gatk-resource-bundle-files/) on how to download the resource files using Google Cloud SDK. There is a Slurm script (download_bundle.slurm) that can be used to download all hg38 files from the Google Cloud bucket. The files were downloaded in /scratch/pawsey0001/sbeecroft/hg38/v0, which needs to be moved before the data becomes purged after 30 days. Note that Homo_sapiens_assembly38.dbsnp138.vcf.gz was from the FTP bundle as this file could not be downloaded using the Conda version of Google Cloud SDK.\\n\\nNote that the `hg38_wgs_scattered_calling_intervals.txt` will need to be to generated using the following:\\n\\n```\\ncd <your_resource_dir>\\nfind `pwd` -name \"scattered.interval_list\" -print | sort > hg38_wgs_scattered_calling_intervals.txt\\n```\\n\\nThese files are required for Multisample_Fastq_to_Gvcf_GATK4.\\n\\n```\\nHomo_sapiens_assembly38.dict\\nHomo_sapiens_assembly38.fasta\\nHomo_sapiens_assembly38.fasta.fai\\nHomo_sapiens_assembly38.fasta.64.alt\\nHomo_sapiens_assembly38.fasta.64.amb\\nHomo_sapiens_assembly38.fasta.64.ann\\nHomo_sapiens_assembly38.fasta.64.bwt\\nHomo_sapiens_assembly38.fasta.64.pac\\nHomo_sapiens_assembly38.fasta.64.sa\\nHomo_sapiens_assembly38.fasta.amb\\nHomo_sapiens_assembly38.fasta.ann\\nHomo_sapiens_assembly38.fasta.bwt\\nHomo_sapiens_assembly38.fasta.pac\\nHomo_sapiens_assembly38.fasta.sa\\nHomo_sapiens_assembly38.dbsnp138.vcf.gz (needs to be gunzipped)\\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\\nMills_and_1000G_gold_standard.indels.hg38.vcf.gz\\nMills_and_1000G_gold_standard.indels.hg38.vcf.gz.tbi\\nHomo_sapiens_assembly38.dbsnp138.vcf\\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\\nHomo_sapiens_assembly38.known_indels.vcf.gz\\nHomo_sapiens_assembly38.known_indels.vcf.gz.tbi\\n```\\n\\nThese files are required for Multisample_jointgt_GATK4.\\n\\n```\\nwgs_evaluation_regions.hg38.interval_list\\nhg38.custom_100Mb.intervals\\nHomo_sapiens_assembly38.dbsnp138.vcf\\nHomo_sapiens_assembly38.dbsnp138.vcf.idx\\n1000G_phase1.snps.high_confidence.hg38.vcf.gz\\n1000G_phase1.snps.high_confidence.hg38.vcf.gz.tbi\\n1000G_omni2.5.hg38.vcf.gz\\n1000G_omni2.5.hg38.vcf.gz.tbi\\nAxiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz\\nAxiom_Exome_Plus.genotypes.all_populations.poly.hg38.vcf.gz.tbi\\nhapmap_3.3.hg38.vcf.gz\\nhapmap_3.3.hg38.vcf.gz.tbi\\n```\\n\\n\\n5. Set up the config files. Files that you need to edit with the correct paths to your data/jar files or other specific configurations are:\\n    - `Multisample_Fastq_to_Gvcf_GATK4_inputs_hg38.json`\\n    - `Multisample_jointgt_GATK4_inputs_hg38.json`\\n        - both json files will need the correct paths to your reference file locations, and the file specifying your inputs i.e. `samples.txt` or `gvcfs.txt`\\n    - `samples.txt`\\n    - `gvcfs.txt`\\n        - These are the sample input files (tab seperated)\\n        - The format for samples.txt is sampleID, sampleID_readgroup, path_to_fastq_R1_file, path_to_fastq_R2_file,\\n        - The format for gvcfs.txt is sample ID, gvcf, gvcf .tbi index file\\n        - Examples are included in this repo\\n        - NOTE: Having tabs, not spaces, is vital for parsing the file. Visual studio code tends to introduce spaces, so if you are having issues, check the file with another text editor such as sublime. \\n    - `launch_cromwell.sh`\\n    - `launch_jointgt.sh`\\n        - These are the scripts which launch the pipeline. \\n        - `launch_cromwell.sh` launches the fastq to gvcf stage\\n        - `launch_jointgt.sh` launched the gvcf joint genotyping to cohort vcf step. This is perfomed when you have run all samples through the fastq to gvcf stage.\\n        - Check the paths and parameters make sense for your machine\\n    - `slurm.conf`\\n        - the main options here relate to the job scheduler. If you are running on Zeus at Pawsey, you should not need to alter these parameters.\\n    - `cromwell.options`\\n        - `cromwell.options` requires editing to provide the directory where you would like the final workflow outputs to be written\\n    - `Multisample_Fastq_to_Gvcf_GATK4.wdl`\\n    - `ruddle_fastq_to_gvcf_single_sample_gatk4.wdl`\\n        - The paths to your jar files will need to be updated\\n        - The path to your conda `activate` binary will need to be updated (e.g. `/group/projectID/userID/miniconda/bin/activate`)\\n\\n6. Launch the job using `sbatch launch_cromwell.sh`. When that has completed successfully, you can launch the second stage of the pipeline (joint calling) with `sbatch launch_jointgt.sh`.\\n\\n### Overview of the steps in `Multisample_Fastq_to_Gvcf_GATK4.wdl`\\nThis part of the pipeline takes short-read, Illumina paired-end fastq files as the input. The outputs generated are sorted, duplicate marked bam files and their indices, duplicate metric information, and a GVCF file for each sample. The GVCF files are used as input for the second part of the pipeline (joint genotyping).\\n\\n```\\nFastqToUbam\\nGetBwaVersion\\nSamToFastqAndBwaMem\\nMergeBamAlignment\\nSortAndFixTags\\nMarkDuplicates\\nCreateSequenceGroupingTSV\\nBaseRecalibrator\\nGatherBqsrReports\\nApplyBQSR\\nGatherBamFiles\\nHaplotypeCaller\\nMergeGVCFs\\n```\\n\\n### Overview of the steps in `Multisample_jointgt_GATK4.wdl`\\nThis part of the pipeline takes GVCF files (one per sample), and performs joint genotyping across all of the provided samples. This means that old previously generated GVCFs can be joint-called with new GVCFs whenever you need to add new samples. The key output from this is a joint-genotyped, cohort-wide VCF file.\\n\\n```\\nGetNumberOfSamples\\nImportGVCFs\\nGenotypeGVCFs\\nHardFilterAndMakeSitesOnlyVcf\\nIndelsVariantRecalibrator\\nSNPsVariantRecalibratorCreateModel\\nSNPsVariantRecalibrator\\nGatherTranches\\nApplyRecalibration\\nGatherVcfs\\nCollectVariantCallingMetrics\\nGatherMetrics\\nDynamicallyCombineIntervals\\n```\\n'\n",
      " 'Description: Trinity @ NCI-Gadi contains a staged [Trinity](https://github.com/trinityrnaseq/trinityrnaseq/wiki) workflow that can be run on the National Computational Infrastructure’s (NCI) Gadi supercomputer. Trinity performs de novo transcriptome assembly of RNA-seq data by combining three independent software modules Inchworm, Chrysalis and Butterfly to process RNA-seq reads. The algorithm can detect isoforms, handle paired-end reads, multiple insert sizes and strandedness. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)'\n",
      " \"Fastq-to-BAM @ NCI-Gadi is a genome alignment workflow that takes raw FASTQ files, aligns them to a reference genome and outputs analysis ready BAM files. This workflow is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes on NCI Gadi to run all stages of the workflow in parallel, either massively parallel using the scatter-gather approach or parallel by sample. It consists of a number of stages and follows the BROAD Institute's best practice recommendations. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)\"\n",
      " '# Local Cromwell implementation of GATK4 germline variant calling pipeline\\nSee the [GATK](https://gatk.broadinstitute.org/hc/en-us) website for more information on this toolset \\n## Assumptions\\n- Using hg38 human reference genome build\\n- Running \\'locally\\' i.e. not using HPC/SLURM scheduling, or containers. This repo was specifically tested on Pawsey Nimbus 16 CPU, 64GB RAM virtual machine, primarily running in the `/data` volume storage partition. \\n- Starting from short-read Illumina paired-end fastq files as input\\n\\n### Dependencies\\nThe following versions have been tested and work, but GATK and Cromwell are regularly updated and so one must consider whether they would like to use newer versions of these tools. \\n- BWA/0.7.15\\n- GATK v4.0.6.0\\n- SAMtools/1.5\\n- picard/2.9\\n- Python/2.7\\n- Cromwell v61\\n\\n## Quick start guide\\n### Installing and preparing environment for GATK4 with Cromwell\\n\\n1. Clone repository\\n```\\ngit clone https://github.com/SarahBeecroft/cromwellGATK4.git\\ncd cromwellGATK4\\nchmod 777 *.sh\\n```\\n\\n2. Install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) if you haven’t already. Create Conda environment using the supplied conda environment file\\n\\n```\\nconda env create --file gatk4_pipeline.yml\\n```\\n\\n3. Download the necessary .jar files\\n    - The Cromwell workfow orchestration engine can be downloaded from https://github.com/broadinstitute/cromwell/releases/ \\n    - GATK can be downloaded from https://github.com/broadinstitute/gatk/releases. Unzip the file with `unzip` \\n    - Picard can be downloaded from https://github.com/broadinstitute/picard/releases/\\n\\n\\n4. Upload the resource bundle file from IRDS using rclone or filezilla and unpack it with `tar xzvf resource.tar.gz`. Note that the `hg38_wgs_scattered_calling_intervals.txt` will need to be to generated using the following:\\n\\n```\\ncd <your_resource_dir>\\nfind `pwd` -name \"scattered.interval_list\" -print | sort > hg38_wgs_scattered_calling_intervals.txt\\n```\\n\\n5. Set up the config files. Files that you need to edit with the correct paths to your data/jar files or other specific configurations are:\\n    - `Multisample_Fastq_to_Gvcf_GATK4_inputs_hg38.json`\\n    - `Multisample_jointgt_GATK4_inputs_hg38.json`\\n        - both json files will need the correct paths to your reference file locations, and the file specifying your inputs i.e. `samples.txt` or `gvcfs.txt`\\n    - `samples.txt`\\n    - `gvcfs.txt`\\n        - These are the sample input files (tab seperated)\\n        - The format for samples.txt is sampleID, sampleID_readgroup, path_to_fastq_R1_file, path_to_fastq_R2_file,\\n        - The format for gvcfs.txt is sample ID, gvcf, gvcf .tbi index file\\n        - Examples are included in this repo\\n        - NOTE: Having tabs, not spaces, is vital for parsing the file. Visual studio code tends to introduce spaces, so if you are having issues, check the file with another text editor such as sublime. \\n    - `launch_cromwell.sh`\\n    - `launch_jointgt.sh`\\n        - These are the scripts which launch the pipeline. \\n        - `launch_cromwell.sh` launches the fastq to gvcf stage\\n        - `launch_jointgt.sh` launched the gvcf joint genotyping to cohort vcf step. This is perfomed when you have run all samples through the fastq to gvcf stage.\\n        - Check the paths and parameters make sense for your machine\\n    - `local.conf`\\n        - the main tuneable parameters here are:\\n        \\t- `concurrent-job-limit = 5` this is the max number of concurrent jobs that can be spawned by cromwell. This depends on the computational resources available to you. 5 was determined to work reasonably well on a 16 CPU, 64GB RAM Nimbus VM (Pawsey). \\n        \\t- `call-caching enabled = true`. Setting this parameter to `false` will disable call caching (i.e. being able to resume if the job fails before completion). By default, call caching is enabled. \\n    - `cromwell.options`\\n        - `cromwell.options` requires editing to provide the directory where you would like the final workflow outputs to be written\\n    - `Multisample_Fastq_to_Gvcf_GATK4.wdl`\\n    - `ruddle_fastq_to_gvcf_single_sample_gatk4.wdl`\\n        - The paths to your jar files will need to be updated\\n        - The path to your conda `activate` binary will need to be updated (e.g. `/data/miniconda/bin/activate`)\\n\\n6. Launch the job within a `screen` or `tmux` session, using `./launch_cromwell.sh`. When that has completed successfully, you can launch the second stage of the pipeline (joint calling) with `./launch_jointgt.sh`. Ensure you pipe the stdout and stderr to a log file using (for example) `./launch_cromwell.sh &> cromwell.log`\\n\\n### Overview of the steps in `Multisample_Fastq_to_Gvcf_GATK4.wdl`\\nThis part of the pipeline takes short-read, Illumina paired-end fastq files as the input. The outputs generated are sorted, duplicate marked bam files and their indices, duplicate metric information, and a GVCF file for each sample. The GVCF files are used as input for the second part of the pipeline (joint genotyping).\\n\\n```\\nFastqToUbam\\nGetBwaVersion\\nSamToFastqAndBwaMem\\nMergeBamAlignment\\nSortAndFixTags\\nMarkDuplicates\\nCreateSequenceGroupingTSV\\nBaseRecalibrator\\nGatherBqsrReports\\nApplyBQSR\\nGatherBamFiles\\nHaplotypeCaller\\nMergeGVCFs\\n```\\n\\n### Overview of the steps in `Multisample_jointgt_GATK4.wdl`\\nThis part of the pipeline takes GVCF files (one per sample), and performs joint genotyping across all of the provided samples. This means that old previously generated GVCFs can be joint-called with new GVCFs whenever you need to add new samples. The key output from this is a joint-genotyped, cohort-wide VCF file. This file can be used for a GEMINI database after normalisation with VT and annotation with a tool such as VEP or SNPEFF. \\n\\nThe file `hg38.custom_100Mb.intervals` is required for this step of the pipeline to run. This is included in the git repo for convenience, but should be moved to your resource directory with all the other resource files. \\n\\n```\\nGetNumberOfSamples\\nImportGVCFs\\nGenotypeGVCFs\\nHardFilterAndMakeSitesOnlyVcf\\nIndelsVariantRecalibrator\\nSNPsVariantRecalibratorCreateModel\\nSNPsVariantRecalibrator\\nGatherTranches\\nApplyRecalibration\\nGatherVcfs\\nCollectVariantCallingMetrics\\nGatherMetrics\\nDynamicallyCombineIntervals\\n```\\n'\n",
      " \"Somatic-ShortV @ NCI-Gadi is a variant calling pipeline that calls somatic short variants (SNPs and indels) from tumour and matched normal BAM files following [GATK's Best Practice Workflow](https://gatk.broadinstitute.org/hc/en-us/articles/360035894731-Somatic-short-variant-discovery-SNVs-Indels-). This workflow is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes on NCI Gadi to run all stages of the workflow in parallel. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)\"\n",
      " \"Flashlite-Trinity contains two workflows that run Trinity on the [University of Queensland's HPC, Flashlite](https://rcc.uq.edu.au/flashlite).  Trinity performs de novo transcriptome assembly of RNA-seq data by combining three independent software modules Inchworm, Chrysalis and Butterfly to process RNA-seq reads. The algorithm can detect isoforms, handle paired-end reads, multiple insert sizes and strandedness. Users can run Flashlite-Trinity on single samples, or smaller samples requiring <500Gb of memory or staged Trinity which is recommended for global assemblies with multiple sample inputs. Both implementations make use of Singularity containers to install software. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: FlashLite (QRISCloud)\"\n",
      " \"Flashlite-Juicer is a PBS implementation of [Juicer](https://github.com/aidenlab/juicer) for University of Queensland's Flashlite HPC.\\n\\nInfrastructure\\\\_deployment\\\\_metadata: FlashLite (QRISCloud)\"\n",
      " \"The Flashlite-Supernova pipeline runs Supernova to generate phased whole-genome de novo assemblies from a Chromium prepared library on [University of Queensland's HPC, Flashlite](https://rcc.uq.edu.au/flashlite). \\n\\nInfrastructure\\\\_deployment\\\\_metadata: FlashLite (QRISCloud)\"\n",
      " \"RNASeq-DE @ NCI-Gadi processes RNA sequencing data (single, paired and/or multiplexed) for differential expression (raw FASTQ to counts). This pipeline consists of multiple stages and is designed for the National Computational Infrastructure's (NCI) Gadi supercompter, leveraging multiple nodes to run each stage in parallel. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)\"\n",
      " \"Bootstrapping-for-BQSR @ NCI-Gadi is a pipeline for bootstrapping a variant resource to enable GATK base quality score recalibration (BQSR) for non-model organisms that lack a publicly available variant resource. This implementation is optimised for the National Compute Infrastucture's Gadi HPC. Multiple rounds of bootstrapping can be performed. Users can use [Fastq-to-bam @ NCI-Gadi](https://workflowhub.eu/workflows/146) and [Germline-ShortV @ NCI-Gadi](https://workflowhub.eu/workflows/143) to produce required input files for Bootstrapping-for-BQSR @ NCI-Gadi. \\n\\nInfrastructure\\\\_deployment\\\\_metadata: Gadi (NCI)\\n\\n\"\n",
      " 'Workflow for quality assessment of paired reads and classification using NGTax 2.0 and functional annotation using picrust2. \\nIn addition files are exported to their respective subfolders for easier data management in a later stage.\\nSteps:  \\n    - FastQC (read quality control)\\n    - NGTax 2.0\\n    - Picrust 2\\n    - Export module for ngtax\\n'\n",
      " '# COVID-19 sequence analysis on Illumina Amplicon PE data\\n\\nThis workflow implements an [iVar](https://github.com/andersen-lab/ivar) based analysis similar to\\nthe one in [ncov2019-artic-nf](https://github.com/connor-lab/ncov2019-artic-nf), [covid-19-signal](https://github.com/jaleezyy/covid-19-signal/) and the Thiagen [Titan workflow](https://github.com/theiagen/public_health_viral_genomics). These workflows (written in  Nextflow, Snakemake and WDL) are widely in use in [COG UK](https://www.cogconsortium.uk/), [CanCOGeN](https://www.genomecanada.ca/en/cancogen) and some US state public health laboratories.\\n\\nThis workflow is also the subject of a Galaxy Training Network tutorial (currently a [Work in Progress](https://github.com/galaxyproject/training-material/pull/2633)).\\nIt differs from [this workflow](https://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-artic-variant-calling) in\\nthat it does not use `lofreq` and is aimed at rapid analysis of majority variants and lineage/clade assignment with `pangolin` and `nextclade`.\\n\\nTODO:\\n\\n1. Add support for QC using negative and positive controls\\n2. Integrate with phylogeny tools including IQTree and UShER (and possibly more).\\n'\n",
      " 'SAMBA is a FAIR scalable workflow integrating, into a unique tool, state-of-the-art bioinformatics and statistical methods to conduct reproducible eDNA analyses using Nextflow. SAMBA starts processing by verifying integrity of raw reads and metadata. Then all bioinformatics processing is done using commonly used procedure (QIIME 2 and DADA2) but adds new steps relying on dbOTU3 and microDecon to build high quality ASV count tables. Extended statistical analyses are also performed. Finally, SAMBA produces a full dynamic HTML report including resources used, commands executed, intermediate results, statistical analyses and figures.\\n\\nThe SAMBA pipeline can run tasks across multiple compute infrastructures in a very portable manner. It comes with singularity containers making installation trivial and results highly reproducible.'\n",
      " 'Cryo-EM processing workflow'\n",
      " 'BioTranslator performs sequentially pathway analysis and gene prioritization: A specific operator is executed for each task to translate the input gene set into semantic terms and pinpoint the pivotal-role genes on the derived semantic network. The output consists of the set of statistically significant semantic terms and the associated hub genes (the gene signature), prioritized according to their involvement in the underlying semantic topology.'\n",
      " 'ASPICov was developed to provide a rapid, reliable and complete analysis of NGS SARS-Cov2 samples to the biologist. This broad application tool allows to process samples from either capture or amplicon strategy and Illumina or Ion Torrent technology. To ensure FAIR data analysis, this Nextflow pipeline follows nf-core guidelines and use Singularity containers. \\n\\nAvailability and Implementation:\\xa0https://gitlab.com/vtilloy/aspicov\\n\\nCitation: Valentin Tilloy, Pierre Cuzin, Laura Leroi, Emilie Guérin, Patrick Durand, Sophie Alain\\n\\t\\t\\t\\t\\t\\t\\tASPICov: An automated pipeline for identification of SARS-Cov2 nucleotidic variants\\n\\t\\t\\t\\t\\t\\t\\tPLoS One 2022 Jan 26;17(1):e0262953: https://pubmed.ncbi.nlm.nih.gov/35081137/'\n",
      " 'This workflow is based on the idea of comparing different gene sets through their semantic interpretation. In many cases, the user studies a specific phenotype (e.g. disease) by analyzing lists of genes resulting from different samples or patients. Their pathway analysis could result in different semantic networks, revealing mechanistic and phenotypic divergence between these gene sets. The workflow of BioTranslator Comparative Analysis compares quantitatively the outputs of pathway analysis, based on the topology of the underlying ontological graph, in order to derive a semantic similarity value for each pair of the initial gene sets. The workflow is available in a Galaxy application and can be used for 14 species. The algorithm accepts as input a batch of gene sets, such as BioTranslator, for the same organism. It performs pathway analysis according to the user-selected ontology and then it compares the derived semantic networks and extracts a matrix with their distances, as well as a respective heatmap.'\n",
      " 'Galaxy workflow example that illustrate the process of setting up a simulation system containing a protein, step by step, using the [BioExcel Building Blocks](/projects/11) library (biobb). The particular example used is the Lysozyme protein (PDB code 1AKI). This workflow returns a resulting protein structure and simulated 3D trajectories.\\n\\nDesigned for running on the https://dev.usegalaxy.es Galaxy instance.'\n",
      " '# Structural DNA helical parameters from MD trajectory tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the [NAFlex](https://mmb.irbbarcelona.org/NAFlex) server and in particular in its [Nucleic Acids Analysis section](https://mmb.irbbarcelona.org/NAFlex/help.php?id=tutorialAnalysisNA).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **extracting structural and dynamical properties** from a **DNA MD trajectory helical parameters**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Drew Dickerson Dodecamer** sequence -CGCGAATTCGCG- (PDB code [1BNA](https://www.rcsb.org/structure/1BNA)). The trajectory used is a  500ns-long MD simulation taken from the [BigNASim](https://mmb.irbbarcelona.org/BIGNASim/) database ([NAFlex_DDD_II](https://mmb.irbbarcelona.org/BIGNASim/getStruc.php?idCode=NAFlex_DDD_II) entry).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'Analysis of RNA-seq data starting from BAM and focusing on mRNA, lncRNA and miRNA'\n",
      " 'This PyCOMPSs workflow tutorial aims to illustrate the process of setting up a simulation system containing a protein, step by step, using the BioExcel Building Blocks library (biobb) in PyCOMPSs for execution on HPC. Three variants of the MD Setup workflows are included, supporting a list of structures, a list of mutations, or a cumulative set of mutations. '\n",
      " \"This is an experimental KNIME workflow of using the BioExcel building blocks to implement the Protein MD Setup tutorial for molecular dynamics with GROMACS.\\n\\nNote that this workflow won't import in KNIME without the [experimental KNIME nodes](https://bioexcel.eu/research/projects/biobb_knime/) for BioBB - contact Adam Hospital for details.\"\n",
      " 'This notebook is about pre-processing the Auditory Brainstem Response (ABR) raw data files provided by [Ingham et. al](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000194) to create a data set for Deep Learning models.\\n\\nThe unprocessed ABR data files are available at [Dryad](https://datadryad.org/stash/dataset/doi:10.5061/dryad.cv803rv).\\n\\nSince the ABR raw data are available as zip-archives, these have to be unzipped and the extracted raw data files parsed so that the time series corresponding to the ABR audiograms can be saved in a single csv file.\\n\\nThe final data set contains the ABR time series, an individual mouse identifier, stimulus frequency, stimulus sound pressure level (SPL) and a manually determined hearing threshold. For each mouse there are different time series corresponding to six different sound stimuli: broadband click, 6, 12, 18, 24, and 30 kHz, each of which was measured for a range of sound pressure levels. The exact range of sound levels can vary between the different mice and stimuli. \\n\\nThe following is done: \\n\\n* The zip archives are unpacked.\\n* The extracted ABR raw data files are parsed and collected in one csv file per archive.\\n* The csv files are merged into a data set of time series. Each time series corresponds to an ABR audiogram measured for a mouse at a specific frequency and sound level.\\n* The mouse phenotyping data are available in Excel format. The individual data sheets are combined into one mouse phenotyping data set, maintaining the mouse pipeline and the cohort type mapping. In addition, the hearing thresholds are added to the ABR audiogram data set.\\n* The data sets are curated: \\n\\n\\t* there is a single curve per mouse, stimulus frequency and sound level,\\n\\t* each sound level is included in the list of potential sound pressure levels,\\n\\t* for each mouse for which an ABR audiogram has been measured, mouse phenotyping data are also provided.'\n",
      " '# BAM-to-FASTQ-QC\\n\\n## General recommendations for using BAM-to-FASTQ-QC\\nPlease see the [`Genome assembly with hifiasm on Galaxy Australia`](https://australianbiocommons.github.io/how-to-guides/genome_assembly/hifi_assembly) guide.\\n\\n## Acknowledgements\\n\\nThe workflow & the [doc_guidelines template used](https://github.com/AustralianBioCommons/doc_guidelines) are supported by the Australian BioCommons via Bioplatforms Australia funding, the Australian Research Data Commons (https://doi.org/10.47486/PL105) and the Queensland Government RICF programme. Bioplatforms Australia and the Australian Research Data Commons are enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\\n'\n",
      " '# PacBio HiFi genome assembly using hifiasm v2.1\\n\\n## General usage recommendations\\nPlease see the [Genome assembly with hifiasm on Galaxy Australia](https://australianbiocommons.github.io/how-to-guides/genome_assembly/hifi_assembly) guide.\\n\\n## See [change log](./change_log.md)\\n\\n## Acknowledgements\\n\\nThe workflow & the [doc_guidelines template used](https://github.com/AustralianBioCommons/doc_guidelines) are \\nsupported by the Australian BioCommons via Bioplatforms Australia funding, the Australian Research Data Commons \\n(https://doi.org/10.47486/PL105) and the Queensland Government RICF programme. Bioplatforms Australia and the \\nAustralian Research Data Commons are enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\\n\\n\\n\\n'\n",
      " 'Data QC step, can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Reports statistics from sequencing reads.\\n* Inputs: long reads (fastq.gz format), short reads (R1 and R2) (fastq.gz format).\\n* Outputs: For long reads: a nanoplot report (the HTML report summarizes all the information). For short reads: a MultiQC report.\\n* Tools used: Nanoplot, FastQC, MultiQC.\\n* Input parameters: None required.\\n* Workflow steps: Long reads are analysed by Nanoplot; Short reads (R1 and R2) are analysed by FastQC; the resulting reports are processed by MultiQC.\\n* Options: see the tool settings options at runtime and change as required. Alternative tool option: fastp\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\\n'\n",
      " 'Kmer counting step, can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Estimates genome size and heterozygosity based on counts of kmers\\n* Inputs: One set of short reads: e.g. R1.fq.gz\\n* Outputs: GenomeScope graphs\\n* Tools used: Meryl, GenomeScope\\n* Input parameters: None required\\n* Workflow steps: The tool meryl counts kmers in the input reads (k=21), then converts this into a histogram. GenomeScope: runs a model on the histogram; reports estimates. k-mer size set to 21. \\n* Options: Use a different kmer counting tool. e.g. khmer.\\n\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " 'Trim and filter reads; can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Trims and filters raw sequence reads according to specified settings. \\n* Inputs: Long reads (format fastq); Short reads R1 and R2 (format fastq) \\n* Outputs: Trimmed and filtered reads: fastp_filtered_long_reads.fastq.gz (But note: no trimming or filtering is on by default), fastp_filtered_R1.fastq.gz, fastp_filtered_R2.fastq.gz\\n* Reports: fastp report on long reads, html; fastp report on short reads, html\\n* Tools used: fastp (Note. The latest version (0.20.1) of fastp has an issue displaying plot results. Using version 0.19.5 here instead until this is rectified). \\n* Input parameters: None required, but recommend removing the long reads from the workflow if not using any trimming/filtering settings. \\n\\nWorkflow steps:\\n\\nLong reads: fastp settings: \\n* These settings have been changed from the defaults (so that all filtering and trimming settings are now disabled). \\n* Adapter trimming options: Disable adapter trimming: yes\\n* Filter options: Quality filtering options: Disable quality filtering: yes\\n* Filter options: Length filtering options: Disable length filtering: yes\\n* Read modification options: PolyG tail trimming: Disable\\n* Output options: output JSON report: yes\\n\\nShort reads: fastp settings:\\n* adapter trimming (default setting: adapters are auto-detected)\\n* quality filtering (default: phred quality 15), unqualified bases limit (default = 40%), number of Ns allowed in a read (default = 5)\\n* length filtering (default length = min 15)\\n* polyG tail trimming (default = on for NextSeq/NovaSeq data which is auto detected)\\n* Output options: output JSON report: yes\\n\\nOptions:\\n* Change any settings in fastp for any of the input reads. \\n* Adapter trimming: input the actual adapter sequences. (Alternative tool for long read adapter trimming: Porechop.) \\n* Trimming n bases from ends of reads if quality less than value x  (Alternative tool for trimming long reads: NanoFilt.)\\n* Discard post-trimmed reads if length is < x (e.g. for long reads, 1000 bp)\\n* Example filtering/trimming that you might do on long reads: remove adapters (can also be done with Porechop), trim bases from ends of the reads with low quality (can also be done with NanoFilt), after this can keep only reads of length x (e.g. 1000 bp) \\n\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " 'Assembly with Flye; can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Assembles long reads with the tool Flye\\n* Inputs: long reads (may be raw, or filtered, and/or corrected); fastq.gz format\\n* Outputs: Flye assembly fasta; Fasta stats on assembly.fasta; Assembly graph image from Bandage; Bar chart of contig sizes; Quast reports of genome assembly\\n* Tools used: Flye, Fasta statistics, Bandage, Bar chart, Quast\\n* Input parameters: None required, but recommend setting assembly mode to match input sequence type\\n\\nWorkflow steps:\\n* Long reads are assembled with Flye, using default tool settings. Note: the default setting for read type (\"mode\") is nanopore raw. Change this at runtime if required. \\n* Statistics are computed from the assembly.fasta file output, using Fasta Statistics and Quast (is genome large: Yes; distinguish contigs with more that 50% unaligned bases: no)\\n* The graphical fragment assembly file is visualized with the tool Bandage. \\n* Assembly information sent to bar chart to visualize contig sizes\\n\\nOptions\\n* See other Flye options. \\n* Use a different assembler (in a different workflow). \\n* Bandage image options - change size (max size is 32767), labels - add (e.g. node lengths). You can also install Bandage on your own computer and donwload the \"graphical fragment assembly\" file to view in greater detail. \\n\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\\n\\n'\n",
      " 'Assembly polishing; can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Polishes (corrects) an assembly, using long reads (with the tools Racon and Medaka) and short reads (with the tool Racon). (Note: medaka is only for nanopore reads, not PacBio reads). \\n* Inputs:  assembly to be polished:  assembly.fasta; long reads - the same set used in the assembly (e.g. may be raw or filtered) fastq.gz format; short reads, R1 only, in fastq.gz format\\n* Outputs: Racon+Medaka+Racon polished_assembly. fasta; Fasta statistics after each polishing tool\\n* Tools used: Minimap2, Racon, Fasta statistics, Medaka\\n* Input parameters:  None required, but recommended to set the Medaka model correctly (default = r941_min_high_g360). See drop down list for options. \\n\\nWorkflow steps:\\n\\n-1-  Polish with long reads: using Racon\\n* Long reads and assembly contigs => Racon polishing (subworkflow): \\n* minimap2 : long reads are mapped to assembly => overlaps.paf. \\n* overaps, long reads, assembly => Racon => polished assembly 1\\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\\n* using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\\n* using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\\n* Racon long-read polished assembly => Fasta statistics\\n* Note: The Racon tool panel can be a bit confusing and is under review for improvement. Presently it requires sequences (= long reads), overlaps (= the paf file created by minimap2), and target sequences (= the contigs to be polished) as per \"usage\" described here https://github.com/isovic/racon/blob/master/README.md\\n* Note: Racon: the default setting for \"output unpolished target sequences?\" is No. This has been changed to Yes for all Racon steps in these polishing workflows.  This means that even if no polishes are made in some contigs, they will be part of the output fasta file. \\n* Note: the contigs output by Racon have new tags in their headers. For more on this see https://github.com/isovic/racon/issues/85.\\n\\n-2-  Polish with long reads: using Medaka\\n* Racon polished assembly + long reads => medaka polishing X1 => medaka polished assembly\\n* Medaka polished assembly => Fasta statistics\\n\\n-3-  Polish with short reads: using Racon\\n* Short reads and Medaka polished assembly =>Racon polish (subworkflow):\\n* minimap2: short reads (R1 only) are mapped to the assembly => overlaps.paf. Minimap2 setting is for short reads.\\n* overlaps + short reads + assembly => Racon => polished assembly 1\\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\\n* Racon short-read polished assembly => Fasta statistics\\n\\nOptions\\n* Change settings for Racon long read polishing if using PacBio reads:  The default profile setting for Racon long read polishing: minimap2 read mapping is \"Oxford Nanopore read to reference mapping\", which is specified as an input parameter to the whole Assembly polishing workflow, as text: map-ont. If you are not using nanopore reads and/or need a different setting, change this input. To see the other available settings, open the minimap2 tool, find \"Select a profile of preset options\", and click on the drop down menu. For each described option, there is a short text in brackets at the end (e.g. map-pb). This is the text to enter into the assembly polishing workflow at runtime instead of the default (map-ont).\\n* Other options: change the number of polishes (in Racon and/or Medaka). There are ways to assess how much improvement in assembly quality has occurred per polishing round (for example, the number of corrections made; the change in Busco score - see section Genome quality assessment for more on Busco).\\n* Option: change polishing settings for any of these tools. Note: for Racon - these will have to be changed within those subworkflows first. Then, in the main workflow, update the subworkflows, and re-save. \\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " 'Assembly polishing subworkflow: Racon polishing with long reads\\n\\nInputs: long reads and assembly contigs\\n\\nWorkflow steps:\\n* minimap2 : long reads are mapped to assembly => overlaps.paf. \\n* overaps, long reads, assembly => Racon => polished assembly 1\\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\\n* using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3\\n* using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4\\n* Racon long-read polished assembly => Fasta statistics\\n* Note: The Racon tool panel can be a bit confusing and is under review for improvement. Presently it requires sequences (= long reads), overlaps (= the paf file created by minimap2), and target sequences (= the contigs to be polished) as per \"usage\" described here https://github.com/isovic/racon/blob/master/README.md\\n* Note: Racon: the default setting for \"output unpolished target sequences?\" is No. This has been changed to Yes for all Racon steps in these polishing workflows.  This means that even if no polishes are made in some contigs, they will be part of the output fasta file. \\n* Note: the contigs output by Racon have new tags in their headers. For more on this see https://github.com/isovic/racon/issues/85.\\n\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " 'Assembly polishing subworkflow: Racon polishing with short reads\\n\\nInputs: short reads and assembly (usually pre-polished with other tools first, e.g. Racon + long reads; Medaka)\\n\\nWorkflow steps: \\n* minimap2: short reads (R1 only) are mapped to the assembly => overlaps.paf. Minimap2 setting is for short reads.\\n* overlaps + short reads + assembly => Racon => polished assembly 1\\n* using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2\\n* Racon short-read polished assembly => Fasta statistics\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " \"Assess genome quality; can run alone or as part of a combined workflow for large genome assembly. \\n\\n* What it does: Assesses the quality of the genome assembly: generate some statistics and determine if expected genes are present; align contigs to a reference genome.\\n* Inputs: polished assembly;  reference_genome.fasta (e.g. of a closely-related species, if available). \\n* Outputs:  Busco table of genes found; Quast HTML report, and link to Icarus contigs browser,  showing contigs aligned to a reference genome\\n* Tools used: Busco, Quast\\n* Input parameters: None required\\n\\nWorkflow steps: \\n\\nPolished assembly => Busco\\n* First: predict genes in the assembly: using Metaeuk\\n* Second: compare the set of predicted genes to the set of expected genes in a particular lineage. Default setting for lineage: Eukaryota\\n\\nPolished assembly and a reference genome => Quast\\n* Contigs/scaffolds file: polished assembly\\n* Type of assembly: Genome\\n* Use a reference genome: Yes\\n* Reference genome: Arabidopsis genome\\n* Is the genome large (> 100Mbp)? Yes. \\n* All other settings as defaults, except second last setting: Distinguish contigs with more than 50% unaligned bases as a separate group of contigs?: change to No\\n\\nOptions\\n\\nGene prediction: \\n* Change tool used by Busco to predict genes in the assembly: instead of Metaeuk, use Augustus. \\n* To do this: select: Use Augustus; Use another predefined species model; then choose from the drop down list.\\n* Select from a database of trained species models. list here:  https://github.com/Gaius-Augustus/Augustus/tree/master/config/species\\n* Note: if using Augustus: it may fail if the input assembly is too small (e.g. a test-size data assembly). It can't do the training part properly. \\n\\nCompare genes found to other lineage: \\n* Busco has databases of lineages and their expected genes. Option to change lineage. \\n* Not all lineages are available - there is a mix of broader and narrower lineages. - list of lineages here: https://busco.ezlab.org/list_of_lineages.html. \\n* To see the groups in taxonomic hierarchies: Eukaryotes:  https://busco.ezlab.org/frames/euka.htm\\n* For example,  if you have a plant species from Fabales, you could set that as the lineage. \\n* The narrower the taxonomic group, the more total genes are expected. \\n\\n\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)\\n\"\n",
      " 'Combined workflow for large genome assembly\\n\\nThe tutorial document for this workflow is here: https://doi.org/10.5281/zenodo.5655813\\n\\n\\nWhat it does:  A workflow for genome assembly, containing subworkflows:\\n* Data QC\\n* Kmer counting\\n* Trim and filter reads\\n* Assembly with Flye\\n* Assembly polishing\\n* Assess genome quality\\n\\nInputs: \\n* long reads and short reads in fastq format\\n* reference genome for Quast\\n\\nOutputs: \\n* Data information - QC, kmers\\n* Filtered, trimmed reads\\n* Genome assembly, assembly graph, stats\\n* Polished assembly, stats\\n* Quality metrics - Busco, Quast\\n\\nOptions\\n* Omit some steps - e.g. Data QC and kmer counting\\n* Replace a module with one using a different tool - e.g. change assembly tool\\n\\nInfrastructure_deployment_metadata: Galaxy Australia (Galaxy)'\n",
      " 'MetaDEGalaxy: Galaxy workflow for differential abundance analysis of 16s metagenomic data'\n",
      " \"A hecatomb is a great sacrifice or an extensive loss. Heactomb the software empowers an analyst to make data driven decisions to 'sacrifice' false-positive viral reads from metagenomes to enrich for true-positive viral reads. This process frequently results in a great loss of suspected viral sequences / contigs.\\n\\nFor information about installation, usage, tutorial etc please refer to the documentation: https://hecatomb.readthedocs.io/en/latest/\\n\\n### Quick start guide\\n\\nInstall Hecatomb from Bioconda\\n```bash\\n# create an env called hecatomb and install Hecatomb in it\\nconda create -n hecatomb -c conda-forge -c bioconda hecatomb\\n\\n# activate conda env\\nconda activate hecatomb\\n\\n# check the installation\\nhecatomb -h\\n\\n# download the databases - you only have to do this once\\nhecatomb install\\n\\n# Run the test dataset\\nhecatomb run --test\\n```\"\n",
      " 'This is a genomics pipeline to do a single germline sample variant-calling, adapted from GATK Best Practice Workflow.\\n\\nThis workflow is a reference pipeline for using the Janis Python framework (pipelines assistant).\\n- Alignment: bwa-mem\\n- Variant-Calling: GATK HaplotypeCaller\\n- Outputs the final variants in the VCF format.\\n\\n**Resources**\\n\\nThis pipeline has been tested using the HG38 reference set, available on Google Cloud Storage through:\\n\\n- https://console.cloud.google.com/storage/browser/genomics-public-data/references/hg38/v0/\\n\\nThis pipeline expects the assembly references to be as they appear in that storage     (\".fai\", \".amb\", \".ann\", \".bwt\", \".pac\", \".sa\", \"^.dict\").\\nThe known sites (snps_dbsnp, snps_1000gp, known_indels, mills_indels) should be gzipped and tabix indexed.\\n\\n\\nInfrastructure_deployment_metadata: Spartan (Unimelb)'\n",
      " '# Purge-duplicates-from-hifiasm-assembly\\n\\n## General recommendations for using `Purge-duplicates-from-hifiasm-assembly`\\n\\nPlease see the [`Genome assembly with hifiasm on Galaxy Australia`](https://australianbiocommons.github.io/how-to-guides/genome_assembly/hifi_assembly) guide.\\n\\n## Acknowledgements\\n\\nThe workflow & the [doc_guidelines template used](https://github.com/AustralianBioCommons/doc_guidelines) are \\nsupported by the Australian BioCommons via Bioplatforms Australia funding, the Australian Research Data Commons \\n(https://doi.org/10.47486/PL105) and the Queensland Government RICF programme. Bioplatforms Australia and the \\nAustralian Research Data Commons are enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\\n\\n\\n\\n'\n",
      " '# Summary\\nThis notebook demonstrates how to retrieve metadata associated to the paper [A SARS-CoV-2 cytopathicity dataset generated by high-content screening of a large drug repurposing collection](https://doi.org/10.1038/s41597-021-00848-4) and available in IDR at [idr0094-ellinger-sarscov2](https://idr.openmicroscopy.org/search/?query=Name:idr0094).\\nOver 300 compounds were used in this investigation. This notebook allows the user to calculate the half maximal inhibitory concentration (IC50) for each compound. IC50 is a measure of the potency of a substance in inhibiting a specific biological or biochemical function. IC50 is a quantitative measure that indicates how much of a particular inhibitory substance (e.g. drug) is needed to inhibit, in vitro, a given biological process or biological component by 50%.\\nUser can download the IC50 for each compound used in that study\\n\\nThe notebook can be launched in [My Binder](https://mybinder.org/v2/gh/IDR/idr0094-ellinger-sarscov2/master?urlpath=notebooks%2Fnotebooks%2Fidr0094-ic50.ipynb%3FscreenId%3D2603).\\n\\nA shiny app is also available for dynamic plotting of the IC50 curve for each compound.\\nThis R shiny app can be launched in [My Binder](https://mybinder.org/v2/gh/IDR/idr0094-ellinger-sarscov2/master?urlpath=shiny/apps/)\\n\\n\\n# Inputs\\nParameters needed to configure the workflow:\\n\\n**screenId**: Identifier of a screen in IDR.\\n\\n# Ouputs\\nOutput file generated:\\n\\n**ic50.csv**: Comma separate value file containing the IC50 for each compound.\\n\\n'\n",
      " 'Exome Alignment Workflow\\n' 'Exome SAMtools Workflow'\n",
      " \"`atavide` is a complete workflow for metagenomics data analysis, including QC/QA, optional host removal, assembly and cross-assembly, and individual read based annotations. We have also built in some advanced analytics including tools to assign annotations from reads to contigs, and to generate metagenome-assembled genomes in several different ways, giving you the power to explore your data!\\n\\n`atavide` is 100% snakemake and conda, so you only need to install the snakemake workflow, and then everything else will be installed with conda.\\n\\nSteps:\\n1. QC/QA with [prinseq++](https://github.com/Adrian-Cantu/PRINSEQ-plus-plus)\\n2. optional host removal using bowtie2 and samtools, [as described previously](https://edwards.flinders.edu.au/command-line-deconseq/). To enable this, you need to provide a path to the host db and a host db.\\n\\nMetagenome assembly\\n1. pairwise assembly of each sample using [megahit](https://github.com/voutcn/megahit)\\n2. extraction of all reads that do not assemble using samtools flags\\n3. assembly of all unassembled reads using [megahit](https://github.com/voutcn/megahit)\\n4. compilation of _all_ contigs into a single unified set using [Flye](https://github.com/fenderglass/Flye)\\n5. comparison of reads -> contigs to generate coverage\\n\\nMAG creation\\n1. [metabat](https://bitbucket.org/berkeleylab/metabat/src/master/)\\n2. [concoct](https://github.com/BinPro/CONCOCT)\\n3. Pairwise comparisons using [turbocor](https://github.com/dcjones/turbocor) followed by clustering\\n\\nRead-based annotations\\n1. [Kraken2](https://ccb.jhu.edu/software/kraken2/)\\n2. [singlem](https://github.com/wwood/singlem)\\n3. [SUPER-focus](https://github.com/metageni/SUPER-FOCUS)\\n4. [FOCUS](https://github.com/metageni/FOCUS)\\n\\nWant something else added to the suite? File an issue on github and we'll add it ASAP!\\n\\n### Installation\\n\\nYou will need to install\\n1. The NCBI taxonomy database somewhere\\n2. The superfocus databases somewhere, and set the SUPERFOCUS_DB environmental variable\\n\\nEverything else should install automatically.\"\n",
      " '# Summary\\nThis notebook shows how to integrate genomic and image data resources. This notebook looks at the question **Which diabetes related genes are expressed in the pancreas?** \\n\\nSteps:\\n\\n* Query humanmine.org, an integrated database of Homo sapiens genomic data using the intermine API to find the genes.\\n* Using the list of found genes, search in the Image Data Resource (IDR) for images linked to the genes, tissue and disease.\\n* \\nWe use the intermine API and the IDR API\\n\\nThe notebook can be launched in [My Binder](https://mybinder.org/v2/gh/IDR/idr-notebooks/master?urlpath=notebooks%2Fhumanmine.ipynb)\\n\\n# Inputs\\nParameters needed to configure the workflow:\\n\\n* TISSUE = \"Pancreas\" \\n* DISEASE = \"diabetes\"\\n\\n# Ouputs\\n* List of genes found using HumanMine\\n* List of images from IDR for one of the gene found'\n",
      " \"## Description\\n\\nThe workflow takes an input file with Cancer Driver Genes predictions (i.e. the results provided by a participant), computes a set of metrics, and compares them against the data currently stored in OpenEBench within the TCGA community. Two assessment metrics are provided for that predictions. Also, some plots (which are optional) that allow to visualize the performance of the tool are generated. The workflow consists in three standard steps, defined by OpenEBench. The tools needed to run these steps are containerised in three Docker images, whose recipes are available in the [TCGA_benchmarking_dockers](https://github.com/inab/TCGA_benchmarking_dockers ) repository and the images are stored in the [INB GitLab container registry](https://gitlab.bsc.es/inb/elixir/openebench/workflows/tcga_benchmarking_dockers/container_registry) . Separated instances are spawned from these images for each step:\\n1. **Validation**: the input file format is checked and, if required, the content of the file is validated (e.g check whether the submitted gene IDs exist)\\n2. **Metrics Generation**: the predictions are compared with the 'Gold Standards' provided by the community, which results in two performance metrics - precision (Positive Predictive Value) and recall(True Positive Rate).\\n3. **Consolidation**: the benchmark itself is performed by merging the tool metrics with the rest of TCGA data. The results are provided in JSON format and SVG format (scatter plot).\\n\\n![OpenEBench benchmarking workflow](https://raw.githubusercontent.com/inab/TCGA_benchmarking_workflow/1.0.8/workflow_schema.jpg)\\n\\n## Data\\n\\n* [TCGA_sample_data](./TCGA_sample_data) folder contains all the reference data required by the steps. It is derived from the manuscript:\\n[Comprehensive Characterization of Cancer Driver Genes and Mutations](https://www.cell.com/cell/fulltext/S0092-8674%2818%2930237-X?code=cell-site), Bailey et al, 2018, Cell [![doi:10.1016/j.cell.2018.02.060](https://img.shields.io/badge/doi-10.1016%2Fj.cell.2018.02.060-green.svg)](https://doi.org/10.1016/j.cell.2018.02.060) \\n* [TCGA_sample_out](./TCGA_sample_out) folder contains an example output for a worklow run, with two cancer types / challenges selected (ACC, BRCA). Results obtained from the default execution should be similar to those ones available in this directory. Results found in [TCGA_sample_out/results](./TCGA_sample_out/results) can be visualized in the browser using [`benchmarking_workflows_results_visualizer` javascript library](https://github.com/inab/benchmarking_workflows_results_visualizer).\\n\\n## Requirements\\nThis workflow depends on three tools that have to be installed before you can run it:\\n* [Git](https://git-scm.com/downloads): Used to download the workflow from GitHub.\\n* [Docker](https://docs.docker.com/get-docker/): The Docker Engine is used under the hood to execute the containerised steps of the benchmarking workflow.\\n* [Nextflow](https://www.nextflow.io/): Is the technology used to write and execute the benchmarking workflow. Note that it depends on Bash (>=3.2) and Java (>=8 , <=17). We provide the script [run_local_nextflow.bash](run_local_nextflow.bash) that automates their installation for local testing.\\n\\nCheck that these tools are available in your environment:\\n```\\n# Git\\n> which git\\n/usr/bin/git\\n> git --version\\ngit version 2.26.2\\n\\n# Docker\\n> which docker\\n/usr/bin/docker\\n> docker --version\\nDocker version 20.10.9-ce, build 79ea9d308018\\n\\n# Nextflow\\n> which nextflow\\n/home/myuser/bin/nextflow\\n> nextflow -version\\n\\n      N E X T F L O W\\n      version 21.04.1 build 5556\\n      created 14-05-2021 15:20 UTC (17:20 CEST)\\n      cite doi:10.1038/nbt.3820\\n      http://nextflow.io\\n```\\nIn the case of docker, apart from being installed the daemon has to be running. On Linux distributions that use `Systemd` for service management, which includes the most popular ones as of 2021 (Ubuntu, Debian, CentOs, Red Hat, OpenSuse), the `systemctl` command can be used to check its status and manage it:\\n\\n```\\n# Check status of docker daemon\\n> sudo systemctl status docker\\n● docker.service - Docker Application Container Engine\\n   Loaded: loaded (/usr/lib/systemd/system/docker.service; disabled; vendor preset: disabled)\\n   Active: inactive (dead)\\n     Docs: http://docs.docker.com\\n\\n# Start docker daemon\\n> sudo systemctl start docker\\n```\\n\\n### Download workflow\\nSimply clone the repository and check out the latest tag (currently `1.0.8`):\\n\\n```\\n# Clone repository\\n> git clone https://github.com/inab/TCGA_benchmarking_dockers.git\\n\\n# Move to new directory\\ncd TCGA_benchmarking_workflow/\\n\\n# Checkout version 1.0.8\\n> git checkout 1.0.8 -b 1.0.8\\n```\\n\\n## Usage\\nThe workflow can be run workflow in two different ways:\\n* Standard: `nextflow run main.nf -profile docker`\\n* Using the bash script that installs Java and Nextflow:`./run_local_nextflow.bash run main.nf -profile docker`.\\n\\nArguments specifications:\\n```\\nUsage:\\nRun the pipeline with default parameters:\\nnextflow run main.nf -profile docker\\n\\nRun with user parameters:\\nnextflow run main.nf -profile docker --predictionsFile {driver.genes.file} --public_ref_dir {validation.reference.file} --participant_name {tool.name} --metrics_ref_dir {gold.standards.dir} --cancer_types {analyzed.cancer.types} --assess_dir {benchmark.data.dir} --results_dir {output.dir}\\n\\nMandatory arguments:\\n\\t--input                 List of cancer genes prediction\\n\\t--community_id          Name or OEB permanent ID for the benchmarking community\\n\\t--public_ref_dir        Directory with list of cancer genes used to validate the predictions\\n\\t--participant_id        Name of the tool used for prediction\\n\\t--goldstandard_dir      Dir that contains metrics reference datasets for all cancer types\\n\\t--challenges_ids        List of types of cancer selected by the user, separated by spaces\\n\\t--assess_dir            Dir where the data for the benchmark are stored\\n\\nOther options:\\n\\t--validation_result     The output directory where the results from validation step will be saved\\n\\t--augmented_assess_dir  Dir where the augmented data for the benchmark are stored\\n\\t--assessment_results    The output directory where the results from the computed metrics step will be saved\\n\\t--outdir                The output directory where the consolidation of the benchmark will be saved\\n\\t--statsdir              The output directory with nextflow statistics\\n\\t--data_model_export_dir The output dir where json file with benchmarking data model contents will be saved\\n\\t--otherdir              The output directory where custom results will be saved (no directory inside)\\nFlags:\\n\\t--help                  Display this message\\n```\\n\\nDefault input parameters and Docker images to use for each step can be specified in the [config](./nextflow.config) file.\\n\\n**NOTE: In order to make your workflow compatible with the [OpenEBench VRE Nextflow Executor](https://github.com/inab/vre-process_nextflow-executor), please make sure to use the same parameter names in your workflow.**\\n\"\n",
      " 'Virtual screening of the SARS-CoV-2 main protease with rDock and pose scoring'\n",
      " '# Protein-ligand complex parameterization\\n\\nParameterizes an input protein (PDB) and ligand (SDF) file prior to molecular\\ndynamics simulation with GROMACS.\\n\\nThis is a simple workflow intended for use as a subworkflow in more complex\\nMD workflows. It is used as a subworkflow by the GROMACS MMGBSA and dcTMD\\nworkflows. \\n'\n",
      " 'MMGBSA simulation and calculation'\n",
      " \"# GROMACS dcTMD free energy calculation\\n\\nPerform an ensemble of targeted MD simulations of a user-specified size using\\nthe GROMACS PULL code and calculate dcTMD free energy and friction profiles\\nfor the resulting dissocation pathway. Note that pathway separation is not\\nperformed by the workflow; the user is responsible for checking the ensemble themselves.\\n\\nThe input protein (PDB) and ligand (SDF) files provided are parameterized by\\nthe 'Protein-ligand complex parameterization' subworkflow.\\n\\nNote that the workflow uses a MDP file for configuring the TMD simulations; this\\nis packaged alongside the workflow as `tmd.mdp`.\\n\\n## Citations\\n* Steffen Wolf and Gerhard Stock (2018), Targeted Molecular Dynamics Calculations of Free Energy Profiles Using a Nonequilibrium Friction Correction, J. Chem. Theory Comput. doi:10.1021/acs.jctc.8b00835\\n* Steffen Wolf, Benjamin Lickert, Simon Bray and Gerhard Stock (2020), Multisecond ligand dissociation dynamics from atomistic simulations, Nat. Commun. doi:10.1038/s41467-020-16655-1\\n\"\n",
      " '<br>\\n\\n<img src=\"https://github.com/marbatlle/multiAffinity/raw/main/docs/img/multiAffinty-logo.png\" alt=\"drawing\" width=\"400\"/>\\n\\n<br>\\n\\nMultiAffinity enables the study of how gene dysregulation propagates on a multilayer network on a disease of interest, uncovering key genes. Find the detailed documentation for the tool [here](https://marbatlle.github.io/multiAffinity/).\\n\\n![alt](https://github.com/marbatlle/multiAffinity/raw/main/docs/img/multiAffinity_workflow.png)'\n",
      " 'This workflow extracts 5 different time periods e.g. January- June 2019, 2020 and 2021, July-December 2019 and 2020 over a single selected location. Then statistics (mean, minimum, maximum) are computed. The final products are maximum, minimum and mean.'\n",
      " 'Abstract CWL Automatically generated from the Galaxy workflow file: GTN \\'Pangeo 101 for everyone - Introduction to Xarray\\'.\\n\\nIn this tutorial, we analyze particle matter < 2.5 μm/m3 data from Copernicus Atmosphere Monitoring Service to understand Xarray Galaxy Tools:\\n- Understand how an Xarray dataset is organized;\\n- Get metadata from Xarray dataset such as variable names, units, coordinates (latitude, longitude, level), etc;\\n- Plot an Xarray dataset on a geographical map and learn to customize it;\\n- Select/Subset an Xarray dataset from coordinates values such as time selection or a subset over a geographical area;\\n- Mask an Xarray dataset with a Where statement, for instance to only see PM2.5 > 30 μm/m and highlight on a map regions with \"high\" values;\\n- Convert an Xarray dataset to Tabular data (pandas dataframe);\\n- Plot tabular data to visualize the forecast PM2.5 over a single point (here Naples) using a scatterplot and/or climate stripes.'\n",
      " '### - deprecated - \\n\\nWorkflow for sequencing with ONT Nanopore, from basecalling to assembly.\\n  - Guppy (basecalling of raw reads)\\n  - MinIONQC (quality check)\\n  - FASTQ merging from multi into one file\\n  - Kraken2 (taxonomic classification)\\n  - Krona (classification visualization)\\n  - Flye (de novo assembly)\\n  - Medaka (assembly polishing)\\n  - QUAST (assembly quality reports)\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\n  Tools: https://git.wur.nl/unlock/cwl/-/tree/master/cwl<br>\\n  Workflows: https://git.wur.nl/unlock/cwl/-/tree/master/cwl/workflows<br>\\n\\n'\n",
      " '#### - Deprecated -\\n#### See our updated hybrid assembly workflow: https://workflowhub.eu/workflows/367\\n#### And other workflows: https://workflowhub.eu/projects/16#workflows\\n# \\n**Workflow for sequencing with ONT Nanopore data, from basecalled reads to (meta)assembly and binning**\\n- Workflow Nanopore Quality\\n- Kraken2 taxonomic classification of FASTQ reads\\n- Flye (de-novo assembly)\\n- Medaka (assembly polishing)\\n- metaQUAST (assembly quality reports)\\n\\n**When Illumina reads are provided:** \\n  - Workflow Illumina Quality: https://workflowhub.eu/workflows/336?version=1\\t\\n  - Assembly polishing with Pilon<br>\\n  - Workflow binnning https://workflowhub.eu/workflows/64?version=11\\n      - Metabat2\\n      - CheckM\\n      - BUSCO\\n      - GTDB-Tk\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\n  Tools: https://git.wur.nl/unlock/cwl/-/tree/master/cwl<br>\\n  Workflows: https://git.wur.nl/unlock/cwl/-/tree/master/cwl/workflows<br>'\n",
      " '# Automatic Ligand parameterization tutorial using BioExcel Building Blocks (biobb)\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **ligand parameterization** for a **small molecule**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Sulfasalazine** protein (3-letter code SAS), used to treat rheumatoid arthritis, ulcerative colitis, and Crohn\\'s disease.\\n\\n**OpenBabel and ACPype** packages are used to **add hydrogens, energetically minimize the structure**, and **generate parameters** for the **GROMACS** package. With *Generalized Amber Force Field (GAFF) forcefield and AM1-BCC* charges.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'This workflow performs the process of protein-ligand docking, step by step, using the BioExcel Building Blocks library (biobb).'\n",
      " '# Protein Ligand Complex MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/complex/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation system** containing a **protein in complex with a ligand**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **T4 lysozyme** L99A/M102Q protein (PDB code 3HTB), in complex with the **2-propylphenol** small molecule (3-letter Code JZ4). \\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Protein-ligand Docking tutorials using BioExcel Building Blocks (biobb)\\n\\nThis tutorials aim to illustrate the process of **protein-ligand docking**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular examples used are based on the **Mitogen-activated protein kinase 14** (p38-α) protein (PDB code [3HEC](https://www.rcsb.org/structure/3HEC)), a well-known **Protein Kinase enzyme**,\\n in complex with the FDA-approved **Imatinib** (PDB Ligand code [STI](https://www.rcsb.org/ligand/STI), DrugBank Ligand Code [DB00619](https://go.drugbank.com/drugs/DB00619)) and **Dasatinib** (PDB Ligand code [1N1](https://www.rcsb.org/ligand/1N1), DrugBank Ligand Code [DB01254](https://go.drugbank.com/drugs/DB01254)), small **kinase inhibitors** molecules used to treat certain types of **cancer**.\\n\\nThe tutorials will guide you through the process of identifying the **active site cavity** (pocket) without previous knowledge, and the final prediction of the **protein-ligand complex**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# AMBER Protein MD Setup tutorials using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)** wrapping the **Ambertools MD package**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# polya_liftover - sc/snRNAseq Snakemake Workflow\\n\\nA [Snakemake][sm] workflow for using PolyA_DB and UCSC Liftover with Cellranger.\\n\\nSome genes are not accurately annotated in the reference genome.\\nHere,\\nwe use information provide by the [PolyA_DB v3.2][polya] to update the coordinates,\\nthen the [USCS Liftover][liftover] tool to update to a more recent genome.\\nNext,\\nwe use [Cellranger][cr] to create the reference and count matrix.\\nFinally,\\nby taking advantage of the integrated [Conda][conda] and [Singularity][sing] support,\\nwe can run the whole thing in an isolated environment.\\n\\nPlease see our [README][readme] for the full details!\\n\\n\\n[sm]: https://snakemake.readthedocs.io/en/stable/index.html \"Snakemake\"\\n[polya]: https://exon.apps.wistar.org/polya_db/v3/index.php \"PolyA_DB\"\\n[liftover]: https://genome.ucsc.edu/cgi-bin/hgLiftOver \"Liftover\"\\n[cr]: https://github.com/alexdobin/STAR \"Cellranger\"\\n[conda]: https://docs.conda.io/en/latest/ \"Conda\"\\n[sing]: https://sylabs.io/singularity/ \"Singularity\"\\n[readme]: https://github.com/IMS-Bio2Core-Facility/polya_liftover/blob/main/README.md'\n",
      " '## Introduction\\n\\n**vibbits/rnaseq-editing** is a bioinformatics pipeline that can be used to analyse RNA sequencing data obtained from organisms with a reference genome and annotation followed by a prediction step of editing sites using RDDpred.\\n\\nThe pipeline is largely based on the [nf-core RNAseq pipeline](https://nf-co.re/rnaseq/).\\n\\nThe initial nf-core pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\n## Pipeline summary\\n\\n1. Merge re-sequenced FastQ files ([`cat`](http://www.linfo.org/cat.html))\\n2. Read QC ([`FastQC`](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\\n3. Adapter and quality trimming ([`Trimmomatics`](https://www.bioinformatics.babraham.ac.uk/projects/trim_galore/))\\n4. Use of STAR for multiple alignment and quantification: [`STAR`](https://github.com/alexdobin/STAR)\\n5. Sort and index alignments ([`SAMtools`](https://sourceforge.net/projects/samtools/files/samtools/))\\n6. Prediction of editing sites using RDDpred ([`RDDpred`](https://github.com/vibbits/RDDpred))\\n7. Extensive quality control:\\n    1. [`RSeQC`](http://rseqc.sourceforge.net/)\\n    2. [`Qualimap`](http://qualimap.bioinfo.cipf.es/)\\n    3. [`dupRadar`](https://bioconductor.org/packages/release/bioc/html/dupRadar.html)\\n8. Present QC for raw read, alignment, gene biotype, sample similarity, and strand-specificity checks ([`MultiQC`](http://multiqc.info/), [`R`](https://www.r-project.org/))\\n\\n## Quick Start\\n\\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=21.04.0`)\\n\\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) on a Linux operating system.\\n   Note: This pipeline does not currently support running with macOS.\\n\\n3. Download the pipeline via git clone, download the associated training data files for RDDpred into the assets folder, download the reference data to \\n\\n    ```console\\n    git clone https://github.com/vibbits/rnaseq-editing.git\\n    cd $(pwd)/rnaseq-editing/assets\\n    # download training data file for RDDpred\\n    wget -c \\n    # download reference data for your genome, we provide genome and indexed genome for STAR 2.7.3a\\n    \\n    ```\\n\\n    > * Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n\\n4. Start running your own analysis using Docker locally!\\n\\n    ```console\\n    nextflow run vibbits/rnaseq-editing \\\\\\n        --input samplesheet.csv \\\\\\n        --genome hg19 \\\\\\n        -profile docker\\n    ```\\n\\n    * An executable Python script called [`fastq_dir_to_samplesheet.py`](https://github.com/nf-core/rnaseq/blob/master/bin/fastq_dir_to_samplesheet.py) has been provided if you would like to auto-create an input samplesheet based on a directory containing FastQ files **before** you run the pipeline (requires Python 3 installed locally) e.g.\\n\\n        ```console\\n        wget -L https://raw.githubusercontent.com/nf-core/rnaseq/master/bin/fastq_dir_to_samplesheet.py\\n        ./fastq_dir_to_samplesheet.py <FASTQ_DIR> samplesheet.csv --strandedness reverse\\n        ```\\n\\n    * The final analysis has been executed on the Azure platform using Azure Kubernetes Services (AKS). AKS has to be set up on the Azure platform by defining a standard node pool called sys next to the scalable node pool cpumem using Standard_E8ds_v4 as node size for calculation.\\n      Furthermore, persistent volume claims (PVCs) have been setup for input and work folders of the nextflow runs. In the PVC `input` the reference data as well as the fastqc files have been stored where the PVC `work`, the temporary nextflow files for the individual runs as well as the output files have been stored.\\n    * The config file for the final execution run for [RNAseq editing for the human samples and reference genome hg19](https://github.com/vibbits/rnaseq-editing/blob/master/nextflow.config.as-executed).    \\n\\n## Documentation\\n\\nThe nf-core/rnaseq pipeline comes with documentation about the pipeline [usage](https://nf-co.re/rnaseq/usage), [parameters](https://nf-co.re/rnaseq/parameters) and [output](https://nf-co.re/rnaseq/output).\\n\\n## Credits\\nThese scripts were written to provide a reproducible data analysis pipeline until the downstream processing using dedicated R scripts for exploratory analysis and plotting. The general structure of pipeline is based on the data analysis steps of the our recent paper [ADAR1 interaction with Z-RNA promotes editing of endogenous double-stranded RNA and prevents MDA5-dependent immune activation](https://pubmed.ncbi.nlm.nih.gov/34380029/).\\n\\nNote: The nf-core scripts this pipeline is based on were originally written for use at the [National Genomics Infrastructure](https://ngisweden.scilifelab.se), part of [SciLifeLab](http://www.scilifelab.se/) in Stockholm, Sweden, by Phil Ewels ([@ewels](https://github.com/ewels)) and Rickard Hammarén ([@Hammarn](https://github.com/Hammarn)).\\n\\nThe RNAseq pipeline was re-written in Nextflow DSL2 by Harshil Patel ([@drpatelh](https://github.com/drpatelh)) from [The Bioinformatics & Biostatistics Group](https://www.crick.ac.uk/research/science-technology-platforms/bioinformatics-and-biostatistics/) at [The Francis Crick Institute](https://www.crick.ac.uk/), London.\\n\\n## Citations\\n\\nThe `nf-core` publication is cited here as follows:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n'\n",
      " '# Snakemake workflow: FAIR CRCC - send data\\n\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥6.3.0-brightgreen.svg)](https://snakemake.github.io)\\n[![GitHub actions status](https://github.com/crs4/fair-crcc-send-data/workflows/Tests/badge.svg?branch=main)](https://github.com/crs4/fair-crcc-send-data/actions?query=branch%3Amain+workflow%3ATests)\\n\\n\\nA Snakemake workflow for securely sharing Crypt4GH-encrypted sensitive data from\\nthe [CRC\\nCohort](https://www.bbmri-eric.eu/scientific-collaboration/colorectal-cancer-cohort/)\\nto a destination approved through a successful [access\\nrequest](https://www.bbmri-eric.eu/services/access-policies/).\\n\\nThe recommendation is to create a directory for the request that has been\\napproved;  it will be used as the working directory for the run.  Copy there the\\nrecipient\\'s crypt4gh key and prepare the run configuration.  The configuration\\nwill specify the repository, the destination of the data, and the list of\\nfiles/directories to transfer.\\n\\n\\n## What\\'s the CRC Cohort?\\n\\nThe CRC Cohort is a collection of clinical data and digital high-resolution\\ndigital pathology images pertaining to tumor cases.  The collection has been\\nassembled from a number of participating biobanks and other partners through the\\n[ADOPT BBMRI-ERIC](https://www.bbmri-eric.eu/scientific-collaboration/adopt-bbmri-eric/) project.\\n\\nResearchers interested in using the data for science can file an application for\\naccess.  If approved, the part of the dataset required for the planned and\\napproved work can be copied to the requester\\'s selected secure storage location\\n(using this workflow).\\n\\n\\n## Usage\\n\\n### Example\\n\\n    mkdir request_1234 && cd request_1234\\n    # Now write the configuration, specifying crypt4gh keys, destination and files to send.\\n    # Finally, execute workflow.\\n    snakemake --snakefile ../fair-crcc-send-data/workflow/Snakefile --profile ../profile/ --configfile config.yml --use-singularity --cores\\n\\n\\n#### Run configuration example\\n\\n```\\nrecipient_key: ./recipient_key\\nrepository:\\n  path: \"/mnt/rbd/data/sftp/fair-crcc/\"\\n  private_key: bbmri-key\\n  public_key: bbmri-key.pub\\nsources:\\n  glob_extension: \".tiff.c4gh\"\\n  items:\\n  - some/directory/to/glob\\n  - another/individual/file.tiff.c4gh\\ndestination:\\n  type: \"S3\"\\n  root_path: \"my-bucket/prefix/\"\\n  connection:  # all elements will be passed to the selected snakemake remote provider\\n    access_key_id: \"MYACCESSKEY\"\\n    secret_access_key: \"MYSECRET\"\\n    host: http://localhost:9000\\n    verify: false # don\\'t verify ssl certificates\\n```\\n\\n\\nTODO\\n\\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=crs4%2Ffair-crcc-send-data).\\n\\nIf you use this workflow in a paper, don\\'t forget to give credits to the authors by citing the URL of this (original) fair-crcc-send-datasitory and its DOI (see above).\\n'\n",
      " \"# Snakemake workflow: FAIR CRCC - image conversion\\n\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥6.3.0-brightgreen.svg)](https://snakemake.github.io)\\n[![GitHub actions status](https://github.com/crs4/fair-crcc-img-convert/workflows/Tests/badge.svg?branch=main)](https://github.com/crs4/fair-crcc-img-convert/actions?query=branch%3Amain+workflow%3ATests)\\n\\n\\nA Snakemake workflow for converting whole-slide images (WSI) from the [CRC\\nCohort](https://www.bbmri-eric.eu/scientific-collaboration/colorectal-cancer-cohort/)\\nfrom vendor-specific image formats to open image formats (at the moment,\\nOME-TIFF).  The workflow also encrypts the new image files with\\n[Crypt4GH](https://doi.org/10.1093/bioinformatics/btab087).\\n\\n\\n## What's the CRC Cohort?\\n\\nThe CRC Cohort is a collection of clinical data and digital high-resolution\\ndigital pathology images pertaining to tumor cases.  The collection has been\\nassembled from a number of participating biobanks and other partners through the\\n[ADOPT BBMRI-ERIC](https://www.bbmri-eric.eu/scientific-collaboration/adopt-bbmri-eric/) project.\\n\\nResearchers interested in using the data for science can [apply for\\naccess](https://www.bbmri-eric.eu/services/access-policies/).\\n\\n\\n## Usage\\n\\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=crs4%2Ffair-crcc-img-convert).\\n\\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this repository and its DOI (see above).\\n\"\n",
      " '\\n# Summary \\n\\nThis notebook demonstrates how to recreate lineages published in the paper [Live imaging of remyelination in the adult mouse corpus callosum](https://www.pnas.org/content/118/28/e2025795118) and available at [idr0113-bottes-opcclones](https://idr.openmicroscopy.org/search/?query=Name:idr0113).\\n\\nThe lineage is created from the metadata associated to the specified image.\\n\\nTo load the data from the Image Data Resource, we use:\\n\\n* the [Python API](https://docs.openmicroscopy.org/omero/latest/developers/Python.html)\\n* the [JSON API](https://docs.openmicroscopy.org/omero/latest/developers/json-api.html)\\n\\nLPC-induced focal demyelination and in vivo imaging of genetically targeted OPCs and their progeny to describe the cellular dynamics of OPC-mediated remyelination in the CC.\\n\\nLongitudinal observation of OPCs and their progeny for up to two months reveals functional inter- and intraclonal heterogeneity and provides insights into the cell division capacity and the migration/differentiation dynamics of OPCs and their daughter cells in vivo.\\n\\nThe majority of the clones remained quiescent or divided only few times. Some OPCs were highly proliferative. Large clones showed longer times between consecutive divisions compared to low proliferating clones.\\n\\nOPCs show distinct modes of cell division: from symmetric proliferative, to symmetric differentiating and also asymmetric cell division, where the OPC is self-renewed while the other daughter cell differentiates.\\n\\nOnly 16.46% of OPC-derived cells differentiated into mature, remyelinating oligodendrocytes, with OPCs born at early divisions showing a higher probability to survive and to terminally differentiate.\\n\\nCell death was associated with distinct cell division histories of different clones, with higher probability of death when generated at later divisions.\\n\\nMigratory behaviour was restricted to progenitors. Successfully differentiating progenitors moved shorter distances per day compared to dying cells.\\n\\n# Inputs\\nParameters needed to configure the workflow:\\n\\n**imageId**: Identifier of an image in IDR.\\n\\n# Ouputs\\nOutput file generated:\\n\\n**lineage_imageId.pdf**: A PDF with the generated lineage. Options to save as `png` or `svg` are also available.\\n\\n'\n",
      " \"This workflow demonstrates the usage of EODIE, a toolkit to extract object based timeseries information from Earth Observation data.\\n\\nEODIE is a toolkit to extract object based timeseries information from Earth Observation data.\\n\\nThe EODIE code can be found on [Gitlab](https://gitlab.com/fgi_nls/public/EODIE) .\\n\\nThe goal of EODIE is to ease the extraction of time series information at object level. Today, vast amounts of Earth Observation data are available to the users via for example earth explorer or scihub. Often, not the whole images are needed for exploitation, but only the timeseries of a certain feature on object level. Objects may be polygons depicting agricultural field parcels, forest plots, or areas of a certain land cover type.\\n\\nEODIE takes the objects in as polygons in a shapefile as well as the timeframe of interest and the features (eg vegetation indices) to be extracted. The output is a per polygon timeseries of the selected features over the timeframe of interest.\\n\\n**Online documentation**\\nEODIE documentation can be found [here](https://eodie.readthedocs.io/en/latest/).\\n\\n**Abstract CWL**\\nAutomatically generated from the Galaxy workflow file: Workflow constructed from history 'EODIE Sentinel'\"\n",
      " '# Protein MD Setup tutorial using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/gmx-protein-md-setup).\\n***\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Lysozyme** protein (PDB code 1AKI).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# StructuralVariants Workflow\\n'\n",
      " '# Protein MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Lysozyme** protein (PDB code 1AKI).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Protein MD Analysis tutorial using BioExcel Building Blocks (biobb)\\n\\n***\\n\\nThis workflow computes a set of Quality Control (QC) analyses on top of an uploaded MD trajectory. QC analyses include positional divergence (RMSd), change of shape (<strong>Radius of Gyration</strong>), identification of flexible regions (<strong>atomic/residue fluctuations</strong>), and identification of different molecular conformations (<strong>trajectory clustering</strong>).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Mutations Protein MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Lysozyme** protein (PDB code 1AKI).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Mutations Protein MD Setup tutorial using BioExcel Building Blocks (biobb)\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Lysozyme** protein (PDB code 1AKI).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Automatic Ligand parameterization tutorial using BioExcel Building Blocks (biobb)\\n\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/gmx-ligand-parameterization).\\n***\\n\\nThis tutorial aims to illustrate the process of **ligand parameterization** for a **small molecule**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **Sulfasalazine** protein (3-letter code SAS), used to treat rheumatoid arthritis, ulcerative colitis, and Crohn\\'s disease.\\n\\n**OpenBabel and ACPype** packages are used to **add hydrogens, energetically minimize the structure**, and **generate parameters** for the **GROMACS** package. With *Generalized Amber Force Field (GAFF) forcefield and AM1-BCC* charges.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Protein Ligand Complex MD Setup tutorial using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/gmx-protein-ligand-complex-md-setup).\\n***\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/complex/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation system** containing a **protein in complex with a ligand**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular example used is the **T4 lysozyme** L99A/M102Q protein (PDB code 3HTB), in complex with the **2-propylphenol** small molecule (3-letter Code JZ4). \\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2022 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2022 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Protein-ligand Docking tutorials using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/protein-ligand-docking).\\n***\\n\\nThis tutorials aim to illustrate the process of **protein-ligand docking**, step by step, using the **BioExcel Building Blocks library (biobb)**. The particular examples used are based on the **Mitogen-activated protein kinase 14** (p38-α) protein (PDB code [3HEC](https://www.rcsb.org/structure/3HEC)), a well-known **Protein Kinase enzyme**,\\n in complex with the FDA-approved **Imatinib** (PDB Ligand code [STI](https://www.rcsb.org/ligand/STI), DrugBank Ligand Code [DB00619](https://go.drugbank.com/drugs/DB00619)) and **Dasatinib** (PDB Ligand code [1N1](https://www.rcsb.org/ligand/1N1), DrugBank Ligand Code [DB01254](https://go.drugbank.com/drugs/DB01254)), small **kinase inhibitors** molecules used to treat certain types of **cancer**.\\n\\nThe tutorials will guide you through the process of identifying the **active site cavity** (pocket) without previous knowledge, and the final prediction of the **protein-ligand complex**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# AMBER Protein MD Setup tutorials using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/amber-protein-md-setup).\\n***\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)** wrapping the **Ambertools MD package**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# AMBER Protein MD Setup tutorials using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/amber-protein-ligand-complex-md-setup).\\n***\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)** wrapping the **Ambertools MD package**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# AMBER Protein MD Setup tutorials using BioExcel Building Blocks (biobb)\\n***\\n## This workflow must be run in **biobb.usegalaxy.es**. Please, [click here to access](https://biobb.usegalaxy.es/u/gbayarri/w/abcix-md-setup).\\n***\\n\\n**Based on the official [GROMACS tutorial](http://www.mdtutorials.com/gmx/lysozyme/index.html).**\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **setting up a simulation** system containing a **protein**, step by step, using the **BioExcel Building Blocks library (biobb)** wrapping the **Ambertools MD package**.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " \"# eQTL-Catalogue/qtlmap\\n**Portable eQTL analysis and statistical fine mapping workflow used by the eQTL Catalogue**\\n\\n### Introduction\\n\\n**eQTL-Catalogue/qtlmap** is a bioinformatics analysis pipeline used for QTL Analysis.\\n\\nThe workflow takes phenotype count matrix (normalized and quality controlled) and genotype data as input, and finds associations between them with the help of sample metadata and phenotype metadata files (See [Input formats and preparation](docs/inputs_expl.md) for required input file details). To map QTLs, pipeline uses [QTLTools's](https://qtltools.github.io/qtltools/) PCA and RUN methods. For manipulation of files [BcfTools](https://samtools.github.io/bcftools/bcftools.html), [Tabix](http://www.htslib.org/doc/tabix.html) and custom [Rscript](https://www.rdocumentation.org/packages/utils/versions/3.5.3/topics/Rscript) scripts are used.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a bioinformatics workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker / singularity containers making installation trivial and results highly reproducible.\\n\\n\\n### Documentation\\nThe eQTL-Catalogue/qtlmap pipeline comes with documentation about the pipeline, found in the `docs/` directory:\\n\\n1. [Installation](docs/installation.md)\\n2. Pipeline configuration\\n    * [Local installation](docs/configuration/local.md)\\n    * [Adding your own system](docs/configuration/adding_your_own.md)\\n3. [Input formats and preparation](docs/inputs_expl.md)\\n4. [Running the pipeline](docs/usage.md)\\n5. [Troubleshooting](docs/troubleshooting.md)\\n\\n<!-- TODO nf-core: Add a brief overview of what the pipeline does and how it works -->\\n\\n### Pipeline Description\\nMapping QTLs is a process of finding statistically significant associations between phenotypes and genetic variants located nearby (within a specific window around phenotype, a.k.a cis window)\\nThis pipeline is designed to perform QTL mapping. It is intended to add this pipeline to the nf-core framework in the future.\\nHigh level representation of the pipeline is shown below:\\n\\n### Results\\nThe output directory of the workflow contains the following subdirectories:\\n\\n1. PCA - genotype and gene expression PCA values used as covariates for QTL analysis.\\n2. sumstats - QTL summary statistics from nominal and permutation passes.\\n3. susie - SuSiE fine mapping credible sets.\\n4. susie_full - full set of susie results for all tested variants (very large files).\\n5. susie_merged - susie credible sets merged with summary statistics from univariate QTL analysis.\\n\\nColumn names of the output files are explained [here](https://github.com/eQTL-Catalogue/eQTL-Catalogue-resources/blob/master/tabix/Columns.md).\\n\\n\\n# Contributors\\n* Nurlan Kerimov\\n* Kaur Alasoo\\n* Masahiro Kanai\\n* Ralf Tambets\\n\"\n",
      " '<!-- markdownlint-disable MD013 MD041 -->\\n\\n![Logo](https://cbg-ethz.github.io/V-pipe/img/logo.svg)\\n\\n[![bio.tools](https://img.shields.io/badge/bio-tools-blue.svg)](https://bio.tools/V-Pipe)\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥7.11.0-blue.svg)](https://snakemake.github.io/snakemake-workflow-catalog/?usage=cbg-ethz/V-pipe)\\n[![Deploy Docker image](https://github.com/cbg-ethz/V-pipe/actions/workflows/deploy-docker.yaml/badge.svg)](https://github.com/cbg-ethz/V-pipe/pkgs/container/v-pipe)\\n[![Tests](https://github.com/cbg-ethz/V-pipe/actions/workflows/run_regression_tests.yaml/badge.svg)](https://github.com/cbg-ethz/V-pipe/actions/workflows/run_regression_tests.yaml)\\n[![Mega-Linter](https://github.com/cbg-ethz/V-pipe/actions/workflows/mega-linter.yml/badge.svg)](https://github.com/cbg-ethz/V-pipe/actions/workflows/mega-linter.yml)\\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\\n\\nV-pipe is a workflow designed for the analysis of next generation sequencing (NGS) data from viral pathogens. It produces a number of results in a curated format (e.g., consensus sequences, SNV calls, local/global haplotypes).\\nV-pipe is written using the Snakemake workflow management system.\\n\\n## Usage\\n\\nDifferent ways of initializing V-pipe are presented below. We strongly encourage you to deploy it [using the quick install script](#using-quick-install-script), as this is our preferred method.\\n\\nTo configure V-pipe refer to the documentation present in [config/README.md](config/README.md).\\n\\nV-pipe expects the input samples to be organized in a [two-level](config/README.md#samples) directory hierarchy,\\nand the sequencing reads must be provided in a sub-folder named `raw_data`. Further details can be found on the [website](https://cbg-ethz.github.io/V-pipe/usage/).\\nCheck the utils subdirectory for [mass-importers tools](utils/README.md#samples-mass-importers) that can assist you in generating this hierarchy.\\n\\nWe provide [virus-specific base configuration files](config/README.md#virus-base-config) which contain handy defaults for, e.g., HIV and SARS-CoV-2. Set the virus in the general section of the configuration file:\\n\\n```yaml\\ngeneral:\\n  virus_base_config: hiv\\n```\\n\\nAlso see [snakemake\\'s documentation](https://snakemake.readthedocs.io/en/stable/executing/cli.html) to learn more about the command-line options available when executing the workflow.\\n\\n\\n### Tutorials\\n\\nTutorials for your first steps with V-pipe for different scenarios are available in the [docs/](docs/README.md) subdirectory.\\n\\n\\n### Using quick install script\\n\\nTo deploy V-pipe, use the [installation script](utils/README.md#quick-installer) with the following parameters:\\n\\n```bash\\ncurl -O \\'https://raw.githubusercontent.com/cbg-ethz/V-pipe/master/utils/quick_install.sh\\'\\n./quick_install.sh -w work\\n```\\n\\nThis script will download and install miniconda, checkout the V-pipe git repository (use `-b` to specify which branch/tag) and setup a work directory (specified with `-w`) with an executable script that will execute the workflow:\\n\\n```bash\\ncd work\\n# edit config.yaml and provide samples/ directory\\n./vpipe --jobs 4 --printshellcmds --dry-run\\n```\\n\\nTest data to test your installation is available with the tutorials provided in the [docs/](docs/README.md) subdirectory.\\n\\n### Using Docker\\n\\nNote: the [docker image](https://github.com/cbg-ethz/V-pipe/pkgs/container/v-pipe) is only setup with components to run the workflow for HIV and SARS-CoV-2 virus base configurations.\\nUsing V-pipe with other viruses or configurations might require internet connectivity for additional software components.\\n\\nCreate `config.yaml` or `vpipe.config` and then populate the `samples/` directory.\\nFor example, the following config file could be used:\\n\\n```yaml\\ngeneral:\\n  virus_base_config: hiv\\n\\noutput:\\n  snv: true\\n  local: true\\n  global: false\\n  visualization: true\\n  QA: true\\n```\\n\\nThen execute:\\n\\n```bash\\ndocker run --rm -it -v $PWD:/work ghcr.io/cbg-ethz/v-pipe:master --jobs 4 --printshellcmds --dry-run\\n```\\n\\n### Using Snakedeploy\\n\\nFirst install [mamba](https://github.com/conda-forge/miniforge#mambaforge), then create and activate an environment with Snakemake and Snakedeploy:\\n\\n```bash\\nmamba create -c conda-forge -c bioconda --name snakemake snakemake snakedeploy\\nconda activate snakemake\\n```\\n\\nSnakemake\\'s [official workflow installer Snakedeploy](https://snakemake.github.io/snakemake-workflow-catalog/?usage=cbg-ethz/V-pipe) can now be used:\\n\\n```bash\\nsnakedeploy deploy-workflow https://github.com/cbg-ethz/V-pipe --tag master .\\n# edit config/config.yaml and provide samples/ directory\\nsnakemake --use-conda --jobs 4 --printshellcmds --dry-run\\n```\\n\\n## Dependencies\\n\\n- **[Conda](https://conda.io/docs/index.html)**\\n\\n  Conda is a cross-platform package management system and an environment manager application. Snakemake uses mamba as a package manager.\\n\\n- **[Snakemake](https://snakemake.readthedocs.io/)**\\n\\n  Snakemake is the central workflow and dependency manager of V-pipe. It determines the order in which individual tools are invoked and checks that programs do not exit unexpectedly.\\n\\n- **[VICUNA](https://www.broadinstitute.org/viral-genomics/vicuna)**\\n\\n  VICUNA is a _de novo_ assembly software designed for populations with high mutation rates. It is used to build an initial reference for mapping reads with ngshmmalign aligner when a `references/cohort_consensus.fasta` file is not provided. Further details can be found in the [wiki](https://github.com/cbg-ethz/V-pipe/wiki/getting-started#input-files) pages.\\n\\n### Computational tools\\n\\nOther dependencies are managed by using isolated conda environments per rule, and below we list some of the computational tools integrated in V-pipe:\\n\\n- **[FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)**\\n\\n  FastQC gives an overview of the raw sequencing data. Flowcells that have been overloaded or otherwise fail during sequencing can easily be determined with FastQC.\\n\\n- **[PRINSEQ](http://prinseq.sourceforge.net/)**\\n\\n  Trimming and clipping of reads is performed by PRINSEQ. It is currently the most versatile raw read processor with many customization options.\\n\\n- **[ngshmmalign](https://github.com/cbg-ethz/ngshmmalign)**\\n\\n  We perform the alignment of the curated NGS data using our custom ngshmmalign that takes structural variants into account. It produces multiple consensus sequences that include either majority bases or ambiguous bases.\\n\\n- **[bwa](https://github.com/lh3/bwa)**\\n\\n  In order to detect specific cross-contaminations with other probes, the Burrows-Wheeler aligner is used. It quickly yields estimates for foreign genomic material in an experiment.\\n  Additionally, It can be used as an alternative aligner to ngshmmalign.\\n\\n- **[MAFFT](http://mafft.cbrc.jp/alignment/software/)**\\n\\n  To standardise multiple samples to the same reference genome (say HXB2 for HIV-1), the multiple sequence aligner MAFFT is employed. The multiple sequence alignment helps in determining regions of low conservation and thus makes standardisation of alignments more robust.\\n\\n- **[Samtools and bcftools](https://www.htslib.org/)**\\n\\n  The Swiss Army knife of alignment postprocessing and diagnostics. bcftools is also used to generate consensus sequence with indels.\\n\\n- **[SmallGenomeUtilities](https://github.com/cbg-ethz/smallgenomeutilities)**\\n\\n  We perform genomic liftovers to standardised reference genomes using our in-house developed python library of utilities for rewriting alignments.\\n\\n- **[ShoRAH](https://github.com/cbg-ethz/shorah)**\\n\\n  ShoRAh performs SNV calling and local haplotype reconstruction by using bayesian clustering.\\n\\n- **[LoFreq](https://csb5.github.io/lofreq/)**\\n\\n  LoFreq (version 2) is SNVs and indels caller from next-generation sequencing data, and can be used as an alternative engine for SNV calling.\\n\\n- **[SAVAGE](https://bitbucket.org/jbaaijens/savage) and [Haploclique](https://github.com/cbg-ethz/haploclique)**\\n\\n  We use HaploClique or SAVAGE to perform global haplotype reconstruction for heterogeneous viral populations by using an overlap graph.\\n\\n## Citation\\n\\nIf you use this software in your research, please cite:\\n\\nFuhrmann, L., Jablonski, K. P., Topolsky, I., Batavia, A. A., Borgsmueller, N., Icer Baykal, P., Carrara, M. ... & Beerenwinkel, (2023).\\n\"V-Pipe 3.0: A Sustainable Pipeline for Within-Sample Viral Genetic Diversity Estimation.\"\\n_bioRxiv_, doi:[10.1101/2023.10.16.562462](https://doi.org/10.1101/2023.10.16.562462).\\n\\n## Contributions\\n\\n- [Ivan Topolsky\\\\* ![orcid]](https://orcid.org/0000-0002-7561-0810), [![github]](https://github.com/dryak)\\n- [Pelin Icer Baykal ![orcid]](https://orcid.org/0000-0002-9542-5292), [![github]](https://github.com/picerbaykal)\\n- [Kim Philipp Jablonski ![orcid]](https://orcid.org/0000-0002-4166-4343), [![github]](https://github.com/kpj)\\n- [Lara Fuhrmann ![orcid]](https://orcid.org/0000-0001-6405-0654), [![github]](https://github.com/LaraFuhrmann)\\n- [Uwe Schmitt ![orcid]](https://orcid.org/0000-0002-4658-0616), [![github]](https://github.com/uweschmitt)\\n- [Michal Okoniewski ![orcid]](https://orcid.org/0000-0003-4722-4506), [![github]](https://github.com/michalogit)\\n- [Monica Dragan ![orcid]](https://orcid.org/0000-0002-7719-5892), [![github]](https://github.com/monicadragan)\\n- [Susana Posada Céspedes ![orcid]](https://orcid.org/0000-0002-7459-8186), [![github]](https://github.com/sposadac)\\n- [David Seifert ![orcid]](https://orcid.org/0000-0003-4739-5110), [![github]](https://github.com/SoapZA)\\n- Tobias Marschall\\n- [Niko Beerenwinkel\\\\*\\\\* ![orcid]](https://orcid.org/0000-0002-0573-6119)\\n\\n\\\\* software maintainer ;\\n\\\\** group leader\\n\\n[github]: https://cbg-ethz.github.io/V-pipe/img/mark-github.svg\\n[orcid]: https://cbg-ethz.github.io/V-pipe/img/ORCIDiD_iconvector.svg\\n\\n## Contact\\n\\nWe encourage users to use the [issue tracker](https://github.com/cbg-ethz/V-pipe/issues). For further enquiries, you can also contact the V-pipe Dev Team <v-pipe@bsse.ethz.ch>.\\n'\n",
      " 'Create Meryl Database used for the estimation of assembly parameters and quality control with Merqury. Part of the VGP pipeline.'\n",
      " 'Performs Long Read assembly using PacBio data and Hifiasm. Part of VGP assembly pipeline. This workflow generate a phased assembly.'\n",
      " 'Purge Phased assembly of duplications and overlaps. Include purge steps for Primary and Alternate assemblies.'\n",
      " 'Performs scaffolding using Bionano Data. Part of VGP assembly pipeline.'\n",
      " 'Performs scaffolding using HiC Data. Part of VGP assembly pipeline. The scaffolding can be performed on long read assembly contigs or on scaffolds (e.g.: Bionano scaffolds).'\n",
      " '\\n# BridgeDb tutorial: Gene HGNC name to Ensembl identifier\\n\\nThis tutorial explains how to use the BridgeDb identifier mapping service to translate HGNC names to Ensembl identifiers. This step is part of the OpenRiskNet use case to link Adverse Outcome Pathways to [WikiPathways](https://wikipathways.org/).\\n\\nFirst we need to load the Python library to allow calls to the [BridgeDb REST webservice](http://bridgedb.prod.openrisknet.org/swagger/):\\n\\n\\n```python\\nimport requests\\n```\\n\\nLet\\'s assume we\\'re interested in the gene with HGNC MECP2 (FIXME: look up a gene in AOPWiki), the API call to make mappings is given below as `callUrl`. Here, the `H` indicates that the query (`MECP2`) is an HGNC symbol:\\n\\n\\n```python\\ncallUrl = \\'http://bridgedb.prod.openrisknet.org/Human/xrefs/H/MECP2\\'\\n```\\n\\nThe default call returns all identifiers, not just for Ensembl:\\n\\n\\n```python\\nresponse = requests.get(callUrl)\\nresponse.text\\n```\\n\\n\\n\\n\\n    \\'GO:0001964\\\\tGeneOntology\\\\nuc065cav.1\\\\tUCSC Genome Browser\\\\n312750\\\\tOMIM\\\\nGO:0042551\\\\tGeneOntology\\\\nuc065car.1\\\\tUCSC Genome Browser\\\\nA0A087X1U4\\\\tUniprot-TrEMBL\\\\n4204\\\\tWikiGenes\\\\nGO:0043524\\\\tGeneOntology\\\\nILMN_1702715\\\\tIllumina\\\\n34355_at\\\\tAffy\\\\nGO:0007268\\\\tGeneOntology\\\\nMECP2\\\\tHGNC\\\\nuc065caz.1\\\\tUCSC Genome Browser\\\\nA_33_P3339036\\\\tAgilent\\\\nGO:0006576\\\\tGeneOntology\\\\nuc065cbg.1\\\\tUCSC Genome Browser\\\\nGO:0006342\\\\tGeneOntology\\\\n300496\\\\tOMIM\\\\nGO:0035176\\\\tGeneOntology\\\\nuc065cbc.1\\\\tUCSC Genome Browser\\\\nGO:0033555\\\\tGeneOntology\\\\nGO:0045892\\\\tGeneOntology\\\\nA_23_P114361\\\\tAgilent\\\\nGO:0045893\\\\tGeneOntology\\\\nENSG00000169057\\\\tEnsembl\\\\nGO:0090063\\\\tGeneOntology\\\\nGO:0005515\\\\tGeneOntology\\\\nGO:0002087\\\\tGeneOntology\\\\nGO:0005634\\\\tGeneOntology\\\\nGO:0007416\\\\tGeneOntology\\\\nGO:0008104\\\\tGeneOntology\\\\nGO:0042826\\\\tGeneOntology\\\\nGO:0007420\\\\tGeneOntology\\\\nGO:0035067\\\\tGeneOntology\\\\n300005\\\\tOMIM\\\\nNP_001104262\\\\tRefSeq\\\\nA0A087WVW7\\\\tUniprot-TrEMBL\\\\nNP_004983\\\\tRefSeq\\\\nGO:0046470\\\\tGeneOntology\\\\nGO:0010385\\\\tGeneOntology\\\\n11722682_at\\\\tAffy\\\\nGO:0051965\\\\tGeneOntology\\\\nNM_001316337\\\\tRefSeq\\\\nuc065caw.1\\\\tUCSC Genome Browser\\\\nA0A0D9SFX7\\\\tUniprot-TrEMBL\\\\nA0A140VKC4\\\\tUniprot-TrEMBL\\\\nGO:0003723\\\\tGeneOntology\\\\nGO:0019233\\\\tGeneOntology\\\\nGO:0001666\\\\tGeneOntology\\\\nGO:0003729\\\\tGeneOntology\\\\nGO:0021591\\\\tGeneOntology\\\\nuc065cas.1\\\\tUCSC Genome Browser\\\\nGO:0019230\\\\tGeneOntology\\\\nGO:0003682\\\\tGeneOntology\\\\nGO:0001662\\\\tGeneOntology\\\\nuc065cbh.1\\\\tUCSC Genome Browser\\\\nX99687_at\\\\tAffy\\\\nGO:0008344\\\\tGeneOntology\\\\nGO:0009791\\\\tGeneOntology\\\\nuc065cbd.1\\\\tUCSC Genome Browser\\\\nGO:0019904\\\\tGeneOntology\\\\nGO:0030182\\\\tGeneOntology\\\\nGO:0035197\\\\tGeneOntology\\\\n8175998\\\\tAffy\\\\nGO:0016358\\\\tGeneOntology\\\\nNM_004992\\\\tRefSeq\\\\nGO:0003714\\\\tGeneOntology\\\\nGO:0005739\\\\tGeneOntology\\\\nGO:0005615\\\\tGeneOntology\\\\nGO:0005737\\\\tGeneOntology\\\\nuc004fjv.3\\\\tUCSC Genome Browser\\\\n202617_s_at\\\\tAffy\\\\nGO:0050905\\\\tGeneOntology\\\\nGO:0008327\\\\tGeneOntology\\\\nD3YJ43\\\\tUniprot-TrEMBL\\\\nGO:0003677\\\\tGeneOntology\\\\nGO:0006541\\\\tGeneOntology\\\\nGO:0040029\\\\tGeneOntology\\\\nA_33_P3317211\\\\tAgilent\\\\nNP_001303266\\\\tRefSeq\\\\n11722683_a_at\\\\tAffy\\\\nGO:0008211\\\\tGeneOntology\\\\nGO:0051151\\\\tGeneOntology\\\\nNM_001110792\\\\tRefSeq\\\\nX89430_at\\\\tAffy\\\\nGO:2000820\\\\tGeneOntology\\\\nuc065cat.1\\\\tUCSC Genome Browser\\\\nGO:0003700\\\\tGeneOntology\\\\nGO:0047485\\\\tGeneOntology\\\\n4204\\\\tEntrez Gene\\\\nGO:0009405\\\\tGeneOntology\\\\nA0A0D9SEX1\\\\tUniprot-TrEMBL\\\\nGO:0098794\\\\tGeneOntology\\\\n3C2I\\\\tPDB\\\\nHs.200716\\\\tUniGene\\\\nGO:0000792\\\\tGeneOntology\\\\nuc065cax.1\\\\tUCSC Genome Browser\\\\n300055\\\\tOMIM\\\\n5BT2\\\\tPDB\\\\nGO:0006020\\\\tGeneOntology\\\\nGO:0031175\\\\tGeneOntology\\\\nuc065cbe.1\\\\tUCSC Genome Browser\\\\nGO:0008284\\\\tGeneOntology\\\\nuc065cba.1\\\\tUCSC Genome Browser\\\\nGO:0060291\\\\tGeneOntology\\\\n202618_s_at\\\\tAffy\\\\nGO:0016573\\\\tGeneOntology\\\\n17115453\\\\tAffy\\\\nA0A1B0GTV0\\\\tUniprot-TrEMBL\\\\nuc065cbi.1\\\\tUCSC Genome Browser\\\\nGO:0048167\\\\tGeneOntology\\\\nGO:0007616\\\\tGeneOntology\\\\nGO:0016571\\\\tGeneOntology\\\\nuc004fjw.3\\\\tUCSC Genome Browser\\\\nGO:0007613\\\\tGeneOntology\\\\nGO:0007612\\\\tGeneOntology\\\\nGO:0021549\\\\tGeneOntology\\\\n11722684_a_at\\\\tAffy\\\\nGO:0001078\\\\tGeneOntology\\\\nX94628_rna1_s_at\\\\tAffy\\\\nGO:0007585\\\\tGeneOntology\\\\nGO:0010468\\\\tGeneOntology\\\\nGO:0031061\\\\tGeneOntology\\\\nA_24_P237486\\\\tAgilent\\\\nGO:0050884\\\\tGeneOntology\\\\nGO:0000930\\\\tGeneOntology\\\\nGO:0005829\\\\tGeneOntology\\\\nuc065cau.1\\\\tUCSC Genome Browser\\\\nH7BY72\\\\tUniprot-TrEMBL\\\\n202616_s_at\\\\tAffy\\\\nGO:0006355\\\\tGeneOntology\\\\nuc065cay.1\\\\tUCSC Genome Browser\\\\nGO:0010971\\\\tGeneOntology\\\\n300673\\\\tOMIM\\\\nGO:0008542\\\\tGeneOntology\\\\nGO:0060079\\\\tGeneOntology\\\\nuc065cbf.1\\\\tUCSC Genome Browser\\\\nGO:0006122\\\\tGeneOntology\\\\nuc065cbb.1\\\\tUCSC Genome Browser\\\\nGO:0007052\\\\tGeneOntology\\\\nC9JH89\\\\tUniprot-TrEMBL\\\\nB5MCB4\\\\tUniprot-TrEMBL\\\\nGO:0032048\\\\tGeneOntology\\\\nGO:0050432\\\\tGeneOntology\\\\nGO:0001976\\\\tGeneOntology\\\\nI6LM39\\\\tUniprot-TrEMBL\\\\nGO:0005813\\\\tGeneOntology\\\\nILMN_1682091\\\\tIllumina\\\\nP51608\\\\tUniprot-TrEMBL\\\\n1QK9\\\\tPDB\\\\nGO:0006349\\\\tGeneOntology\\\\nGO:1900114\\\\tGeneOntology\\\\nGO:0000122\\\\tGeneOntology\\\\nGO:0006351\\\\tGeneOntology\\\\nGO:0008134\\\\tGeneOntology\\\\nILMN_1824898\\\\tIllumina\\\\n300260\\\\tOMIM\\\\n0006510725\\\\tIllumina\\\\n\\'\\n\\n\\n\\nYou can also see the results are returned as a TSV file, consisting of two columns, the identifier and the matching database.\\n\\nWe will want to convert this reply into a Python dictionary (with the identifier as key, as one database may have multiple identifiers):\\n\\n\\n```python\\nlines = response.text.split(\"\\\\n\")\\nmappings = {}\\nfor line in lines:\\n    if (\\'\\\\t\\' in line):\\n        tuple = line.split(\\'\\\\t\\')\\n        identifier = tuple[0]\\n        database = tuple[1]\\n        if (database == \"Ensembl\"):\\n            mappings[identifier] = database\\n\\nprint(mappings)\\n```\\n\\n    {\\'ENSG00000169057\\': \\'Ensembl\\'}\\n\\n\\nAlternatively, we can restrivct the return values from the BridgeDb webservice to just return Ensembl identifiers (system code `En`). For this, we add the `?dataSource=En` call parameter:\\n\\n\\n```python\\ncallUrl = \\'http://bridgedb-swagger.prod.openrisknet.org/Human/xrefs/H/MECP2?dataSource=En\\'\\nresponse = requests.get(callUrl)\\nresponse.text\\n```\\n\\n\\n\\n\\n    \\'ENSG00000169057\\\\tEnsembl\\\\n\\'\\n'\n",
      " '# Shotgun Metagenomics Analysis\\nAnalysis of metagenomic shotgun sequences including assembly, speciation, ARG discovery and more\\n\\n## Description\\nThe input for this analysis is paired end next generation sequencing data from metagenomic samples. The workflow is designed to be modular, so that individual modules can be run depending on the nature of the metagenomics project at hand. More modules will be added as we develop them - this repo is a work in progress!\\n\\nThese scripts have been written specifically for NCI Gadi HPC, wich runs PBS Pro, however feel free to use and modify for anothre system if you are not a Gadi user. \\n\\n### Part 1. Setup and QC\\nDownload the repo. You will see directories for `Fastq`, `Inputs`, `Reference` and `Logs`. You will need to copy or symlink your fastq to `Fastq`, sample configuration file (see below) to `Inputs` and the reference genome sequence of your host species (if applicable) to `Reference` for host contamination removal.\\n \\n\\n#### Fastq inputs\\nThe scripts assume all fastq files are paired, gzipped, and all in the one directory named \\'Fastq\\'. If your fastq are within a convoluted directory structure (eg per-sample directories) or you would simply like to link them from an alternate location, please use the script `setup_fastq.sh`.\\n\\nTo use this script, parse the path name of your fastq as first argument on the command line, and run the script from the base working directory (<your_path>/Shotgun-Metagenomics-Analysis) which will from here on be referred to as `workdir`. Note that this script looks for `f*q.gz` files (ie fastq.gz or fq.gz) - if yours differ in suffix, please adjust the script accordingly.\\n\\n```\\nbash ./Scripts/setup_fastq.sh </path/to/your/parent/fastq/directory>\\n```\\n\\n#### Configuration/sample info\\nThe only required input configuration file should be named <cohort>.config, where <cohort> is the name of the current batch of samples you are processing, or some other meaningful name to your project; it will be used to name output files. The config file should be placed inside the $workdir/Inputs directory, and include the following columns, in this order:\\n\\n```\\n1. Sample ID - used to identify the sample, eg if you have 3 lanes of sequencing per sample, erach of those 6 fastq files should contain this ID that si in column 1\\n2. Lab Sample ID - can be the same as column 1, or different if you have reason to change the IDs eg if the seq centre applies an in-house ID. Please make sure IDs are unique within column 1 and unique within column 2\\n3. Group - eg different time points or treatment groups. If no specific group structure is relevant, please set this to 1 (do not leave blank!) \\n3. Platform - should be Illumina; other sequencing platforms are not tested on this workflow\\n4. Sequencing centre name\\n5. Library - eg if you have 2 sequencing libraries for the same sample. Can be left blank, or assigned to 1. Blank will be assigned libray ID of 1 during processing.\\n```\\n\\nPlease do not have spaces in any of the values for the config file. \\n\\n\\n#### General setup\\n\\nAll scripts will need to be edited to reflect your NCI project code at the `-P <project>` and `-l <storage> directive. Please run the script create_project.sh and follow the prompts to complete some of the setup for you. \\n\\nNote that you will need to manually edit the PDS resource requests for each PBS script; guidelines/example resources will be given at each step to help you do this. As the \\'sed\\' commands within this script operate on .sh and .pbs files, this setup script has been intentionally named .bash (easiest solution).\\n\\nRemember to submit all scripts from your `workdir`. \\n\\n`bash ./Scripts/create_project.sh`\\n\\nFor jobs that execute in parallel, there are 3 scripts: one to make the \\'inputs\\' file listing hte details of each parallel task, one job execution shell script that is run over each task in parallel, and one PBS launcher script. The process is to submit the make input script, check it to make sure your job details are correct, edit the resources directives depending on the number and size of your parallel tasks, then submit the PBS launcher script with `qsub`. \\n\\n#### QC\\n\\nRun fastQC over each fastq file in parallel. Adjust the resources as per your project. To run all files in parallel, set the number of NCPUS requested equal to the number of fastq files (remember that Gadi can only request <1 node or multiples of whole nodes). The make input script sorts the fastq files largest to smallest, so if you have a discrpeancy in file size, optimal efficiency can be achieved by requested less nodes than the total required to run all your fastq in parallel.\\n\\nFastQC does not multithread on a single file, so CPUs per parallel task is set to 1. Example walltimes on Gadi \\'normal\\' queue:  one 1.8 GB fastq = 4 minutes; one 52 GB fastq file = 69.5 minutes.\\n\\nMake the fastqc parallel inputs file by running (from `workdir`):\\n`bash ./Scripts/fastqc_make_inputs.sh`\\n\\nEdit the resource requests in `fastqc_run_parallel.pbs` according to your number of fastq files and their size, then submit:\\n`qsub fastqc_run_parallel.pbs`\\n\\nTo ease manual inspection of the fastQC output, running `multiqc` is recommended. This will collate the individual fastQC reports into one report. This can be done on the login node for small sample numbers, or using the below script for larger cohorts. Edit the PBS directives, then run:\\n\\n`qsub multiqc.pbs`\\n\\nSave a copy of ./MultiQC/multiqc_report.html to your local disk then open in a web browser to inspect the results. \\n\\n#### Quality filtering and trimming\\n\\nWill be added at a later date. This is highly dependent on the quality of your data and your individual project needs so will be a guide only. \\n\\n### Part 2. Removal of host contamination. \\n\\nIf you have metagenomic data extracted from a host, you will need a copy of the host reference genome sequence in order to remove any DNA sequences belonging to the host. Even if your wetlab protocol included a host removal step, it is still important to run bioinformatic host removal.\\n\\n\\n#### Prepare the reference\\nEnsure you have a copy of the reference genome (or symlink) in ./Fasta. This workflow requires BBtools(tested with version 37.98). As of writing, BBtools is not available as a global app on Gadi. Please install locally and make \"module loadable\", or else edit the scripts to point directly to your local BBtools installation.\\n\\nBBtools repeat masking will use all available threads on machine and 85% of available mem by default. For a mammalian genome, 2 hours on one Gadi \\'normal\\' node is sufficient for repeat masking. \\n\\nUpdate the name of your reference fastq in the `bbmap_prep.pbs` script (and BBtools, see note above), then run:\\n`qsub ./Scripts/bbmap_prep.pbs`\\n\\n#### Host contamination removal\\n\\nTBC 1/4/22... \\n'\n",
      " 'For integrative analysis of CAKUT multi-omics data DIABLO method of the mixOmics package (version 6.10.9. Singh et. al. 2019) was used with sPLS-DA (sparse Partial Least Squares Discriminant Analysis Discriminant Analysis) and PLS-DA classification.'\n",
      " 'In this analysis, we created an extended pathway, using the WikiPathways repository (Version 20210110) and the three -omics datasets. For this, each of the three -omics datasets was first analyzed to identify differentially expressed elements, and pathways associated with the significant miRNA-protein links were detected. A miRNA-protein link is deemed significant, and may possibly be implying causality, if both a miRNA and its target are significantly differentially expressed. \\n\\nThe peptidome and the proteome datasets were quantile normalized and log2 transformed (Pan and Zhang 2018; Zhao, Wong, and Goh 2020). Before transformation, peptide IDs were mapped to protein IDs, using the information provided by the data uploaders, and were summarized into single protein-level values using geometric mean. The miRNome dataset was already normalized and transformed; thus, the information of their targeting genes was simply added to each miRNA ID, using the information provided by miTaRBase (Huang et al. 2019). As a result, all three datasets had been mapped to their appropriate gene product-level (or, protein-level) identifiers. '\n",
      " '## CWL based workflow to assemble haploid/diploid eukaryote genomes of non-model organisms\\nThe workflow is designed to use both PacBio long-reads and Illumina short-reads. The workflow first extracts, corrects, trims and decontaminates the long reads. Decontaminated trimmed reads are then used to assemble the genome and raw reads are used to polish it. Next, Illumina reads are cleaned and used to further polish the resultant assembly. Finally, the polished assembly is masked using inferred repeats and haplotypes are eliminated. The workflow uses BioConda and DockerHub to install required software and is therefore fully automated. In addition to final assembly, the workflow produces intermediate assemblies before and after polishing steps. The workflow follows the syntax for CWL v1.0.\\n\\n### Dependencies\\n# Programs\\nThe pipeline can be run either using [Cromwell](https://cromwell.readthedocs.io/en/stable) or [cwltool reference](https://github.com/common-workflow-language/cwltool) implementation and docker containers can be run either using [Singularity](https://singularity.lbl.gov) or [udocker](https://singularity.lbl.gov).\\n\\nCromwell implementation\\n* [cromwell v44](https://github.com/broadinstitute/cromwell/releases/tag/44)\\n* [java-jdk v8.0.112](https://www.java.com/en)\\n\\nReference implementation\\n* [cwltool v1.0.20181012180214](https://github.com/common-workflow-language/cwltool)\\n* [nodejs v10.4.1 required by cwltool](https://nodejs.org/en)\\n* [Python library galaxy-lib v18.5.7](https://pypi.org/project/galaxy-lib)\\n\\nSingularity software packages have to be installed server-wide by administrator\\n* [Singularity v3.2.1](https://singularity.lbl.gov)\\n* [squashfs-tools v4.3.0](https://github.com/plougher/squashfs-tools)\\n\\nUdocker software package can be installed locally\\n* [udocker v1.1.2](https://github.com/indigo-dc/udocker)\\n\\n# Data\\n* [Illumina adapters converted to FASTA format](http://sapac.support.illumina.com/downloads/illumina-adapter-sequences-document-1000000002694.html)\\n* [NCBI nucleotide non-redundant sequences for decontamination with Centrifuge](http://www.ccb.jhu.edu/software/centrifuge)\\n* [RepBase v17.02 file RMRBSeqs.embl](https://www.girinst.org/repbase)\\n\\n### Installation\\nInstall miniconda using installation script ```installConda.sh```.\\nTo install CWL, use either installation script ```installCromwell.sh``` or ```installCwltool.sh```.\\nTo install udocker, use installation script ```installUdocker.sh```.\\nTo install singularity, ask your system administrator.\\n\\n```\\n# First confirm that you have the program \\'git\\' installed in your system\\n> cd\\n> git clone -b \\'v0.1.3-beta\\' --single-branch --depth 1 https://github.com/vetscience/Assemblosis\\n> cd Assemblosis\\n> bash installConda.sh\\n> bash installCromwell.sh # or bash installCwltool.sh\\n> bash installUdocker.sh # if singularity cannot be installed or does not run\\n\\n```\\nFor data dependencies: download and extract [RepBase database](https://www.girinst.org/repbase), download Centrifuge version of [NCBI nt database](http://www.ccb.jhu.edu/software/centrifuge) and create [Illumina adapter FASTA file](http://sapac.support.illumina.com/downloads/illumina-adapter-sequences-document-1000000002694.html) to your preferred locations. If your reads are clean from adapters, the adapter FASTA file can be empty.\\nGive the location of these data in the configuration (.yml) file (see **Usage**).\\n\\n### Usage\\nYou have to create a YAML (.yml) file for each assembly. This file defines the required parameters and the location for both PacBio and Illumina raw-reads.\\n```\\n> cd\\n> export PATH=~/miniconda3/bin:$PATH\\n> cd Assemblosis/Run\\n> cp ../Examples/assemblyCele.yml .\\n\\n\"Edit assemblyCele.yml to fit your computing environment and to define the location for the read files, databases and Illumina adapters\"\\n\\n\"Running docker images using Cromwell and singularity:\"\\n> java -Dconfig.file=cromwell.udocker.conf -jar cromwell-44.jar run -t CWL -v v1.0 assembly.cwl -i assemblyCele.yml\\n\\n\"Running docker images using Cromwell and udocker:\"\\n> java -Dconfig.file=cromwell.singularity.conf -jar cromwell-44.jar run -t CWL -v v1.0 assembly.cwl -i assemblyCele.yml\\n\\n\"Running docker images using Cwltool and singularity:\"\\n> cwltool --tmpdir-prefix /home/<username>/Tmp --beta-conda-dependencies --cachedir /home/<username>/Cache --singularity --leave-tmpdir assembly.cwl assemblyCele.yml\\n\\n\"Running docker images using Cwltool and udocker:\"\\n> cwltool --tmpdir-prefix /home/<username>/Tmp --beta-conda-dependencies --cachedir /home/<username>/Cache --user-space-docker-cmd udocker --leave-tmpdir assembly.cwl assemblyCele.yml\\n```\\n\\nAn annotated example of the YAML file for Caenorhabditis elegans assembly.\\n```\\n## Directory, which contains the PacBio raw data\\n# NOTE! The software looks for all .h5 file (or bam files if bacBioInBam below is defined true) in given directory\\npacBioDataDir:\\n  class: Directory\\n  location: /home/<username>/Dna\\n\\n## PacBio files are in bam format as returned from Sequel platform\\npacBioInBam: true\\n\\n## Prefix for the resultant assembly files\\nprefix: cele\\n\\n## Maximum number of threads used in the pipeline\\nthreads: 24\\n\\n## Minimum number of threads per job used in canu assembler\\nminThreads: 4\\n\\n## Number of concurrent jobs in canu assembler (recommended to use threads / minThreads)\\ncanuConcurrency: 6\\n\\n### Parameters for the program Canu are described in https://canu.readthedocs.io/en/latest/parameter-reference.html\\n## Expected genome size. This parameter is forwarded to Canu assembler.\\ngenomeSize: 100m\\n\\n## Minimum length for the PacBio reads used for the assembly. This parameter is forwarded to Canu assembler.\\n# The maximum resolvable repeat regions becomes 2 x minReadLength\\nminReadLen: 6000\\n\\n## Parameter for Canu assembler to adjust to GC-content. Should be 0.15 for high or low GC content.\\ncorMaxEvidenceErate: 0.20\\n\\n### Parameters for the program Trimmomatic are described in http://www.usadellab.org/cms/?page=trimmomatic\\n## Paired-end (PE) reads of Illumina raw data. These files are given to the program Trimmomatic.\\n# NOTE! Data for two paired libraries is given below.\\nreadsPe1:\\n  - class: File\\n    format: edam:format_1930  # fastq\\n    path: /home/<username>/Dna/SRR2598966_1.fastq.gz\\n  - class: File\\n    format: edam:format_1930  # fastq\\n    path: /home/<username>/Dna/SRR2598967_1.fastq.gz\\nreadsPe2:\\n  - class: File\\n    format: edam:format_1930  # fastq\\n    path: /home/<username>/Dna/SRR2598966_2.fastq.gz\\n  - class: File\\n    format: edam:format_1930  # fastq\\n    path: /home/<username>/Dna/SRR2598967_2.fastq.gz\\n\\n## Phred coding of Illumina data. This parameter is forwarded to Trimmomatic.\\n# NOTE! Each read-pair needs one phred value.\\nphredsPe: [\\'33\\',\\'33\\']\\n\\n## Sliding window and illuminaClip parameters for Trimmomatic\\nslidingWindow:\\n    windowSize: 4\\n    requiredQuality: 25\\nilluminaClip:\\n    adapters:\\n        class: File\\n        path: <path to Illumina adapter file>\\n    seedMismatches: 2\\n    palindromeClipThreshold: 30\\n    simpleClipThreshold: 10\\n    minAdapterLength: 20\\n    keepBothReads: true\\n## Further parameters for Trimmomatic\\n# Required phred-quality for leading 5 nucleotides\\nleading: 25\\n# Required phred-quality for trailing 5 nucleotides\\ntrailing: 25\\n# Minimum accepted read-length to keep the read after trimming\\nminlen: 40\\n\\n### Parameters for the program bowtie2 are described in http://bowtie-bio.sourceforge.net/bowtie2/manual.shtml\\n## Illumina PE fragment length. Program bowtie2 parameter -X.\\n# NOTE! Each read-pair needs one phred value.\\nmaxFragmentLens: [500, 600]\\n# Orientation of pair-end reads e.g. \\'fr\\', \\'rf\\', \\'ff\\': Program bowtie2 parameters --fr, --rf or --ff\\norientation: \\'fr\\'\\n\\n### Parameters for the program Pilon are described in https://github.com/broadinstitute/pilon/wiki/Requirements-&-Usage\\n# Prefix for the resultant pilon polished assembly. Pilon parameter --output\\npolishedAssembly: celePilon\\n# This is set \\'true\\' for an organism with diploid genome: Pilon parameter --diploid\\ndiploidOrganism: true\\n# Value \\'bases\\' fixes snps and indels: Pilon parameter --fix\\nfix: bases\\n\\n### Parameters for the program centrifuge are described in http://www.ccb.jhu.edu/software/centrifuge/manual.shtml\\n# Path to the directory, that contains NCBI nt database in nt.?.cf files. Centrifuge parameter -x\\ndatabase:\\n  class: Directory\\n  path:  /home/<username>/ntDatabase\\n# Lenght of the identical match in nucleotides required to infer a read as contaminant. Centrifuge parameter --min-hitlen\\npartialMatch: 100\\n# NCBI taxon root identifers for the species considered contaminants: e.g. bacteria (=2), viruses (=10239), fungi (=4751), mammals (=40674), artificial seqs (=81077). Pipeline specific parameter.\\ntaxons: [2,10239,4751,40674,81077]\\n\\n## Parameters for the RepeatModeler and RepeatMasker are described in http://www.repeatmasker.org\\nrepBaseLibrary:\\n  class: File\\n  # This is the RepBase file from https://www.girinst.org/repbase. RepeatMasker parameter -lib\\n  path: /home/<username>/RepBaseLibrary/RMRBSeqs.embl\\n# Constant true and false values for repeat masker\\ntrueValue: true\\nfalseValue: false\\n\\n```\\n### Runtimes and hardware requirements\\nThe workflow was tested in Linux environment (CentOS Linux release 7.2.1511) in a server with 24 physical CPUs (48 hyperthreaded CPUs) and 512 GB RAM.\\n\\n| Assembly | Runtime in CPU hours | RAM usage (GB) |\\n| --- | --- | --- |\\n| *Caenorhabditis elegans* | 1537 | 134.1 |\\n| *Drosophila melanogaster* | 6501 | 134.1 |\\n| *Plasmodium falciparum* | 424 | 134.1 |\\n\\nMaximum memory usage of 134.1 GB was claimed by the program Centrifuge for each assembly.\\n\\n### Software tools used in this pipeline\\n* [Dextractor v1.0](https://github.com/thegenemyers/DEXTRACTOR)\\n* [Trimmomatic v0.36](http://www.usadellab.org/cms/?page=trimmomatic)\\n* [Centrifuge v1.0.3](http://www.ccb.jhu.edu/software/centrifuge)\\n* [Canu v1.8](http://canu.readthedocs.io/en/latest/index.html)\\n* [Arrow in SmrtLink v7.0.1](https://www.pacb.com/support/software-downloads)\\n* [Bowtie 2 v2.2.8](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\\n* [SAMtools v1.6](http://samtools.sourceforge.net)\\n* [Pilon v1.22](https://github.com/broadinstitute/pilon)\\n* [RepeatMasker v4.0.6](http://www.repeatmasker.org)\\n* [RepeatModeler v1.0.11](http://www.repeatmasker.org)\\n* [RepBase v17.02](https://www.girinst.org/repbase)\\n* [HaploMerger2 build_20160512](https://github.com/mapleforest/HaploMerger2)\\n\\n### Cite\\nIf you use the pipeline, please cite:\\nKorhonen, Pasi K., Ross S. Hall, Neil D. Young, and Robin B. Gasser. \"Common Workflow Language (CWL)-based software pipeline for de novo genome assembly from long-and short-read data.\" GigaScience 8, no. 4 (2019): giz014.\\n\\n'\n",
      " '# ESCALIBUR\\n\\nEscalibur Population Genomic Analysis Pipeline is able to explore key aspects centering the population genetics of organisms, and automates three key bioinformatic components in population genomic analysis using Workflow Definition Language (WDL: https://openwdl.org/), and customised R, Perl, Python and Unix shell scripts. Associated programs are packaged into a platform independent singularity image, for which the definition file is provided.\\n\\nThe workflow for analysis using Escalibur consists of three steps - each step can be run in a separate workflow in a sequential manner; step 2 is optional.\\n\\n    1. Trimming and mapping the raw data - selection of the best reference genome;\\n    2. Removing the contamination from mapped data;\\n    3. Recalibration, variant calling and filtering;\\n\\nThis implementation runs both locally and in a distributed environment that uses SLURM job scheduler.\\n\\n## Dependencies\\nFollowing software dependencies are required:\\n\\n* Git\\n* SLURM scheduler required for distributed HPC environment (https://slurm.schedmd.com/documentation.html)\\n* Python3.7: (https://www.python.org/)\\n* Perl 5.26.2: (https://www.perl.org/)\\n* Java 1.8\\n* Singularity 3.7.3: (https://sylabs.io/singularity/)\\n\\n## Step 1: Installation\\n\\nTypically, the installation of Singularity requires root rights. You should therefore contact your administrator to get it correctly installed. Minimum Linux kernel version requirement is 3.8, thought >= 3.18 would be preferred (https://sylabs.io/guides/3.5/admin-guide/installation.html).\\n\\nClone the git repository to a directory on your cluster or stand-alone server.\\n```\\n> git clone --depth 1 -b v0.3-beta https://gitlab.unimelb.edu.au/bioscience/escalibur.git\\n> cd escalibur\\n```\\n\\n### Description of Files\\n* `workflow-main.local.config`: main configuration file for stand alone server runtime environment\\n* `workflow-main.slurm.config`: main configuration file for HPC runtime environment that support Slurm job scheduler\\n* `workflow-mapping.json`: defines location of input files, has behavioral settings and sets resource allocations\\n* `workflow-cleaning.json`:  defines location of input files and sets resource allocations\\n* `workflow-variants.json`:  defines location of input files, has behavioral settings and sets resource allocations\\n* `workflow-mapping.wdl`: main workflow file to trim and map PE reads into the genome\\n* `workflow-cleaning.wdl`: main workflow file to clean contamination from mapped PE reads against genomes representing putative contamination\\n* `workflow-variants.wdl`: main workflow file to call variants using mapped and cleaned reads\\n* `workflow-mapping.outputs.json`: defines location for resultant outputs and logs from mapping workflow\\n* `workflow-cleaning.outputs.json`: defines location for resultant outputs and logs from cleaning workflow\\n* `workflow-variants.outputs.json`: defines location for resultant outputs and logs from variants workflow\\n* `inputReads.txt`: example input file for fastq read files to mapping step\\n* `cleanup.conf`: example configuration file for putative host contamination to cleaning step\\n* `inputBams.txt`: example input file for resultant BAM files to variant calling step\\n* `references.txt`: contains list of example references genomes\\n* `perl_scripts`: contains Perl scripts used by the pipeline\\n* `scripts`: contains Python scripts used by the pipeline\\n* `R_scripts`: contains R scripts used by the pipeline\\n* `sub_workflows`: sub-workflows, one for each of the workflow steps\\n* `tasks`: workflow tasks\\n* `cromwell-50.jar`: java archive file required to run the workflow.\\n\\nTwo config files have been created. One for stand alone server (`workflow-runtime.local.config`) and another one for HPC environment that supports Slurm scheduler (`workflow-runtime.slurm.config`).\\nThese files have already been optimised. For slurm configuration you only need to define the HPC partition in line 35: \"String rt_queue\"\\nChange this to the partition you have access to on HPC environment.\\n\\nFiles `workflow-mapping.outputs.json`, `workflow-cleaning.outputs.json` and `workflow-variants.outputs.json` define the directories to copy the result files to. Modify if you want to change default output directories `outputMapping`, `outputCleaning` and `outputVariants`. These output directories are generated to the directory `escalibur`.\\n#### NOTE: delete output directories from previous runs. If you have files there already and a name matches during the copy, the workflow may fail.\\n\\n`Singularity` directory contains the definition file for the software used in Escalibur. Pre-built singularity image can be downloaded from `library://pakorhon/workflows/escalibur:0.0.1-beta`.\\n```\\n> singularity pull escalibur.sif library://pakorhon/workflows/escalibur:0.0.1-beta\\n```\\n\\n## Step 2: Test run\\n\\nTo confirm correct function of the workflows (`mapping`, `cleaning` and `variant calling`), fix the required absolute paths, marked by three dots `...` in `workflow-mapping.json`, `workflow-cleaning.json` and `workflow-variants.json` and configuration files `cleanup.conf` and `inputBams.txt`, and run the workflow with the provided test and configuration files, and parameter settings.\\n```\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-mapping.wdl -i workflow-mapping.json -o workflow-mapping.outputs.json > out.mapping 2> err.mapping\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-cleaning.wdl -i workflow-cleaning.json -o workflow-cleaning.outputs.json > out.cleaning 2> err.cleaning\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-variants.wdl -i workflow-variants.json -o workflow-variants.outputs.json > out.variants 2> err.variants\\n```\\nSlurm file templates `runMapping.slurm`, `runCleaning.slurm` and `runVariants.slurm` are available for each workflow.\\n#### NOTE: default parameter settings for run-times, memory usage and module loading may require adjustment in these files if run in HPC environment using slurm. Current settings should account for the test run.\\n\\nAfter the runs are complete, the results will be at the output directories: `outputMapping`, `outputCleaning` and `outputVariants`.\\nYou can compare the result of `outputVariants/full_genotype_output.vcf` to that or pre-run `TestResults/full_genotype_output.vcf`.\\n\\n## Step 3: Mapping\\n\\nMake a directory for your fastq files e.g. `Reads` and copy your paired end raw data in there.\\n```\\n> mkdir Reads\\n```\\n\\nIt should look something like below\\n```\\n> ls TestReads/\\n1-1_r1.fastq.gz  32-1_r1.fastq.gz  44-1_r1.fastq.gz\\n1-1_r2.fastq.gz  32-1_r2.fastq.gz  44-1_r2.fastq.gz\\n```\\nRun the python script to create a file of your input samples and edit the resulting file to match your sample identifiers and libraries.\\n```\\n> python3 scripts/inputArgMaker.py -d Reads/ -p -ps 33 -pq 20 -pl ILLUMINA -ml 50 -o inputReads.txt \\n```\\n\\nThe edited output file is shown below. The script will automatically sort the files by size.\\n```\\n> cat inputReads.txt\\n# Prefix PE/SE\\tMinLen\\tPhredS\\tSequencer\\tPhredQ\\tLibrary\\tRead Group ID\\tSample\\tPlatform Unit\\tFirst pair of PE reads\\t\\tSecond pair of PE reads\\ntest1\\t PE\\t50\\t33\\tILLUMINA\\t28\\tLIB1\\tCL100082180L1\\tSM1\\tCL100082180L1\\t./TestReads/1-1_r1.fastq.gz\\t./TestReads/1-1_r2.fastq.gz\\ntest2\\t PE\\t50\\t33\\tILLUMINA\\t20\\tLIB2\\tCL100082180L1\\tSM2\\tCL100082180L1\\t./TestReads/44-1_r1.fastq.gz\\t./TestReads/44-1_r2.fastq.gz\\ntest3\\t PE\\t50\\t33\\tILLUMINA\\t20\\tLIB3\\tCL100034574L1\\tSM2\\tCL100034574L1\\t./TestReads/32-1_r1.fastq.gz\\t./TestReads/32-1_r2.fastq.gz\\n```\\n#### NOTE: If several libraries are embedded in a single read file, library-specific reads have to be separated into own files before create the inputReads.txt file. In contrast, inputReads.txt file format can accommodate multiple library files to a single sample.\\n\\n* `Prefix`: Prefix for the resultant files from trimming.\\n* `PE/SE`: Paired-End/Single-End reads as input.\\n* `MinLen`: Minimum Length of reads after trimming.\\n* `PhredS`: Used Phred coding by the sequencer (33 or 64).\\n* `Sequencer`: Name of the sequencer.\\n* `PhredQ`: Phred cut-off score used in trimming.\\n* `Library`: Identifier for the library.\\n* `Read Group ID`: Identifier for the read groups required by GATK (inputArgMaker tries to find this from FASTQ reads). Refer to (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\\n* `Sample`: Identifier for the sample. Defined prefix for resultant sample specific files.\\n* `Platform Unit (optional)`: Information about flow cell, lane and sample. Helps GATK in recalibration (inputArgMaker copies Read Group ID here). Refer to (https://gatk.broadinstitute.org/hc/en-us/articles/360035890671-Read-groups).\\n* `First pair of PE reads`: Relative path to the forward pair of PE reads.\\n* `Second pair of PE reads`: Relative path to the reverse pair of PE reads.\\n\\nCreate a file listing reference genomes and configure `workflow-mapping.json` file.\\nAn example reference file (`references.txt`) has been created for you. Use this as an example to create your own.\\nEnsure there are no whitespaces at the end of the line or else the cromwell engine will throw an error.\\nReads are mapped to these reference files and the best matching reference will be selected for variant calling.\\n```\\n> cat references.txt\\nscf00001\\t./TestReferences/scf00001.fa\\nscf00013\\t./TestReferences/scf00013.fa\\n```\\n#### NOTE: Reference label (e.g. `scf00001`) must be a substring found in the reference fasta file (`scf00001.fa`)\\n\\nThe figure below illustrates the flow of the information, and appearance of labels (`Prefix`, `Sample`, `Label`) in file names, as defined in `inputReads.txt` and `references.txt`.\\n![](figures/labelFlow.png)\\n\\n### workflow-mapping.json config file\\nAdd the path of your fastq and reference genome input files and change parameters as appropriate, and adjust the absolute paths for singularity image. If `mapping_workflow.readQc` is set to `yes`, reads are trimmed both for quality and the adapters. Adapters to trim are given in `mapping_workflow.pe_filtering_workflow.trimmomatic_pe_task.truseq_pe_adapter`. If you want to use custom adapters, copy them to `adapters` directory and instead of default `TruSeq3-PE.fa`, refer to your custom file. If you don\\'t want to use adapters, use `empty.fa` file instead. For BGISEQ adapters, refer to (https://en.mgitech.cn/Download/download_file/id/71).\\n```\\n{\\n  \"## CONFIG FILE\": \"WDL\",\\n  \"mapping_workflow.inputSampleFile\": \"./inputReads.txt\",\\n  \"mapping_workflow.inputReferenceFile\": \"./references.txt\",\\n\\n  \"## Parameters for samtools read filtering\": \"-F 4 does filters unmapped reads from resultant files\",\\n  \"mapping_workflow.samtoolsParameters\": \"-F 4\",\\n  \\n  \"## Is read QC required\": \"yes or no\",\\n  \"mapping_workflow.readQc\": \"yes\",\\n  \"## What is the ploidy of given genome\": \"1 for haploid, 2 for diploid, etc.\",\\n  \"mapping_workflow.ploidy\": 2,\\n  \\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\\n  \"mapping_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\\n  \"mapping_workflow.singularityBindPath\": \"/home/.../escalibur/\",\\n\\n  \"## trimmomatic adapters\": \"\",\\n  \"mapping_workflow.pe_filtering_workflow.trimmomatic_pe_task.truseq_pe_adapter\":\"./adapters/TruSeq3-PE.fa\",\\n  \"mapping_workflow.pe_filtering_workflow.trimmomatic_se_task.truseq_se_adapter\":\"./adapters/TruSeq3-SE.fa\",\\n  \\n  \"## Indexing sub workflow task parameters\": \"Samtools index run time parameters\",\\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_minutes\": 300,\\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_threads\": 16,\\n  \"mapping_workflow.index_sub_workflow.indexing_sam_task.IST_mem\": 30000,\\n  .\\n  .\\n  .\\n}\\n```\\n\\nRun the mapping workflow.\\n```\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-mapping.wdl -i workflow-mapping.json -o workflow-mapping.outputs.json > out.mapping 2> err.mapping\\n```\\nThe resultant BAM files will be copied to `outputMapping` directory.\\n\\n## Step 4 (optional): Cleaning\\n\\nIf you suspect \\'host\\' contamination in your data, you can remove that using the cleaning workflow.\\nDefine the file representing the contamination. First column defines the sample identifier, second the resultant BAM file from mapping workflow and third the putative contaminant genome assembly.\\n```\\n> cat cleanup.conf\\nSM1\\t/home/.../escalibur/outputMapping/SM1.scf00001.MarkDup.bam\\t/home/.../escalibur/Hosts/host1.fa\\nSM2\\t/home/.../escalibur/outputMapping/SM2.scf00001.MarkDup.bam\\t/home/.../escalibur/Hosts/host1.fa\\n```\\n#### NOTE: you have to use absolute paths both to BAM files and the contaminant reference genome (here `host1.fa` and `host2.fa`).\\n\\n### workflow-cleaning.json config file\\nAdd the path of your cleaning config file (here `cleanup.conf`) and adjust the absolute paths for singularity image.\\n```\\n{\\n  \"## CONFIG FILE\": \"WDL\",\\n  \"cleaning_workflow.inputContaminantFile\": \"./cleanup.conf\",\\n  \\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\\n  \"cleaning_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\\n  \"cleaning_workflow.singularityBindPath\": \"/home/.../escalibur/\",\\n\\n  \"cleaning_workflow.indexing_bwa_task.IBT_minutes\": 60,\\n  \"cleaning_workflow.indexing_bwa_task.IBT_threads\": 1,\\n  \"cleaning_workflow.indexing_bwa_task.IBT_mem\": 16000,\\n\\n  \"######################################\":\"########################################\",\\n  \"CLEANING\":\"PARAMETERS\",\\n  \"######################################\":\"########################################\",\\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_minutes\": 600,\\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_threads\": 4,\\n  \"cleaning_workflow.clean_bams_workflow.cleanBams_task.CLEAN_BAMS_mem\": 32000,\\n\\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_minutes\": 300,\\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_threads\": 4,\\n  \"cleaning_workflow.create_cleaned_bams_workflow.createCleanedBams_task.CREATE_CLEAN_BAMS_mem\": 32000,\\n\\n  \"cleaning_workflow.refsBySample.RBS_minutes\": 5,\\n  \"cleaning_workflow.refsBySample.RBS_threads\": 1,\\n  \"cleaning_workflow.refsBySample.RBS_mem\": 4000\\n}\\n```\\n\\nRun the cleaning workflow.\\n```\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-cleaning.wdl -i workflow-cleaning.json -o workflow-cleaning.outputs.json > out.cleaning 2> err.cleaning\\n```\\nThe resultant cleaned BAM files will be copied to `outputCleaning` directory. You can repeat the workflow if you suspect that there may be more than one contaminant genomes per each sample. In that case you have to take care of the properly configured `cleanup.conf` file that should describe the BAM files from previous cleaning round but also define new output directory for each round in `workflow-cleaning.outputs.json` file.\\n\\n## Step 5: Variant calling\\n\\nDefine the file listing the BAM files used for variant calling. First column defines the sample identifier, and second the resultant BAM file either from mapping of cleaning workflow.\\n```\\n> cat inputBams.txt\\nSM1\\t/home/.../escalibur/outputMapping/SM1.scf00001.MarkDup.bam\\nSM2\\t/home/.../escalibur/outputCleaned/SM2.scf00001.MarkDup.cleaned.bam\\n```\\n\\n### workflow-variants.json config file\\nAdd the path of your file listing the locations of BAM files (here `inputBams.txt`), and add the location to selected reference genome (found in `outputMapping/best.ref`) and it\\'s label, as defined in `references.txt` file. Adjust the absolute paths for singularity image and adjust other parameters, especially define if you want to recalibrate the BAM files by selecting value \"independent\" to \"variants_workflow.call_type\".\\n```\\n{\\n  \"## CONFIG FILE\": \"WDL\",\\n  \"variants_workflow.inputSampleFile\": \"./inputBams.txt\",\\n  \"variants_workflow.selectedRefFile\": \"TestReferences/scf00001.fa\",\\n  \"variants_workflow.selectedRefLabel\": \"scf00001\",\\n  \\n  \"## Singularity parameters\": \"absolute paths to the container and the directory to bind visible inside singularity\",\\n  \"variants_workflow.singularityContainerPath\": \"/home/.../escalibur/escalibur.sif\",\\n  \"variants_workflow.singularityBindPath\": \"/home/.../escalibur/\",\\n\\n  \"## Which variant call workflow to use\": \"fast or independent\",\\n  \"variants_workflow.call_type\": \"fast\",\\n  \\n  \"## Variant filtering expressions\": \"For SNPs and INDELs\",\\n  \"variants_workflow.SNP_filt_exp\": \"QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0\",\\n  \"variants_workflow.INDEL_filt_exp\": \"QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0\",\\n\\n  \"## Variant Filter params\": \"Variant filter, indel, snps, report making: Safe to leave as default\",\\n  \"variants_workflow.ploidy\": 2,\\n  \"variants_workflow.maxIndelSize\": 60,\\n  \"variants_workflow.scafNumLim\": 95,\\n  \"variants_workflow.scafNumCo\": 2,\\n  \"variants_workflow.scafLenCutOff\": 0,\\n  \"variants_workflow.ldWinSize\": 10,\\n  \"variants_workflow.ldWinStep\": 5,\\n  \"variants_workflow.ldCutOff\": 0.3,\\n  \"variants_workflow.snp_indel_var_filtering_workflow.indelFilterName\": \"Indel_filter\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.indelFilterExpression\": \"QD < 2.0 || FS > 200.0 || ReadPosRankSum < -20.0\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.snpFilterName\": \"Snp_filter\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.snpFilterExpression\": \"QD < 2.0 || FS > 60.0 || MQ < 40.0 || MQRankSum < -12.5 || ReadPosRankSum < -8.0\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.vfindel_tk.selectType\": \"\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.vfsnp_tk.selectType\": \"\",\\n\\n  \"## Build chromosome map\":\"map_def_scf_lim_task\",\\n  \"variants_workflow.snp_indel_var_filtering_workflow.map_def_scf_lim_task.scafLenCutOff\": 1000000,\\n  \"variants_workflow.snp_indel_var_filtering_workflow.map_def_scf_lim_task.scafNumCo\": 3,\\n\\n  \"## Indexing sub workflow task parameters\": \"Samtools index run time parameters\",\\n  \"variants_workflow.ref_index.IST_minutes\": 300,\\n  \"variants_workflow.ref_index.IST_threads\": 2,\\n  \"variants_workflow.ref_index.IST_mem\": 8000,\\n  .\\n  .\\n  .\\n}\\n```\\n\\nRun the variant calling workflow.\\n```\\n> java -Dconfig.file=./workflow-runtime.local.config  -jar ./cromwell-50.jar run workflow-variants.wdl -i workflow-variants.json -o workflow-variants.outputs.json > out.variants 2> err.variants\\n```\\nThe resultant files will be copied to `outputVariants` directory. That includes filtered variants calls (`full_genotype_output.vcf`) and recalibrated BAM files (if independent call_type is selected).\\n\\n## Other considerations\\n\\n### Resource allocation in HPC environment\\nWall time, memory usage and thread count (`_minutes`, `_mem`, `_threads`) given in `.json` files for each workflow can vary substantially and may require adjusting in HPC environment and slurm. This may lead to frequent restarting of the workflow after each adjustment. We have automated this task by providing scripts that automatically check the failed resource allocations and double them for each round. These scripts are located in `Automation` directory and can be run as follows:\\n```\\n> cd Automation\\n> sh init.sh # Copies the content of ../tasks directory to tasksOrig directory\\n> sbatch runMapping.slurm # Runs runLoopMapping.sh in a worker node\\n> sbatch runCleaning.slurm # Runs runLoopCleaning.sh in a worker node\\n> sbatch runVariants.slurm # Runs runLoopVariants.sh in a worker node\\n```\\nScripts `runLoop*.sh` copy resource allocations from collective `runtimes.json` file to the files in `../tasks` directory, run the workflow and double the failed resource allocations in `../tasks` files, and reruns the workflow until it succeeds or until ten rounds have passed. Copying of resource allocations directly to the files in `../tasks` directory is necessary to guarantee proper function of call-caching.\\n#### NOTE: automated resource allocation adjustment is experimental, should be monitored when running and may require modifications to scripts to function properly.\\n\\n### Disk usage\\nCromwell will create duplicate copies of files while running the workflows. It is therefore recommended to remove `cromwell-executions` directory after each workflow is run, if disk space is getting sparse.\\n```\\n> rm -r cromwell-executions\\n```\\nEspecially, if there are hundreds of samples that may sum up to terabytes of data, disk space might become an issue if unused files are not removed.\\n\\n### Troubleshooting\\nIf the output text does not reveal the error, you can try to find an error message using command(s):\\n```\\n> find cromwell-executions/ -name stderr -exec cat {} \\\\; | grep -i fatal\\n> find cromwell-executions/ -name stderr -exec cat {} \\\\; | less\\n```\\n\\nMost commonly encountered error cases:\\n\\n* Singularity is not running correctly. Typically you require help from your administrator to get singularity properly installed.\\n* Singularity image `escalibur.sif` was not downloaded\\n* Check that you are using correct runtime configuration file `workflow-runtime.local.config` or `workflow-runtime.slurm.config` when calling `cromwell-50.jar`\\n* Absolute file paths for Singularity/Trimmomatic, input files or contaminant genomes are not updated or are wrong in `workflow-*.json`, `inputBams.txt` or `cleanup.conf` configuration files, respectively.\\n* Defined run-time and memory requirements for some tasks are not sufficient in `.json` configuration files to run the pipeline in HPC environment.\\n* If you are using slurm job scheduler and want to run the pipeline in HPC environment, you have to create the related configuration file yourselves.\\n* Pipeline has not been tested in other environments but Linux and we expect that users encounter challenges if trying to run the pipeline e.g. in Mac environment.\\n\\n'\n",
      " '### Workflow for Illumina Quality Control and Filtering\\n_Multiple paired datasets will be merged into single paired dataset._\\n\\n**Summary:**\\n- FastQC on raw data files<br />\\n- fastp for read quality trimming<br />\\n- BBduk for phiX and (optional) rRNA filtering<br />\\n- Kraken2 for taxonomic classification of reads (optional)<br />\\n- BBmap for (contamination) filtering using given references (optional)<br />\\n- FastQC on filtered (merged) data<br />\\n\\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\nhttps://gitlab.com/m-unlock/cwl\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\\n'\n",
      " '### Workflow for LongRead Quality Control and Filtering\\n\\n- NanoPlot  (read quality control) before and after filtering\\n- Filtlong  (read trimming)\\n- Kraken2 taxonomic read classification before and after filtering\\n- Minimap2 read filtering based on given references<br><br>\\n\\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br><br>\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\nhttps://gitlab.com/m-unlock/cwl/workflows\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\\n'\n",
      " 'Objective. Biomarkers have become important for the prognosis and diagnosis of various diseases. High-throughput methods such as RNA-sequencing facilitate the detection of differentially expressed genes (DEGs), hence potential biomarker candidates. Individual studies suggest long lists of DEGs, hampering the identification of clinically relevant ones. Concerning preeclampsia, a major obstetric burden with high risk for adverse maternal and/or neonatal outcomes, limitations in diagnosis and prediction are still important issues. Therefore, we developed a workflow to facilitate the screening for biomarkers.\\nMethods. Based on the tool DeSeq2, we established a comprehensive workflow for the identification of  DEGs, analyzing data from multiple publicly available RNA-sSequencing studies. We applied it to four RNA-sSequencing datasets (one blood, three placenta) analyzing patients with preeclampsia and normotensive controls. We compared our results with other published approaches and evaluated their performance. \\nResults. We identified 110 genes dysregulated in preeclampsia, observed in ≥3 of the analyzed studies, six even in all four studies. Among them were FLT-1, TREM-1, and FN1 which either represent established biomarkers on protein level, or promising candidates based on recent studies. In comparison, using a published meta-analysis approach we obtained 5,240  DEGs.\\nConclusions. We present a data analysis workflow for preeclampsia biomarker screening, capable of identifying significant biomarker candidates, while drastically decreasing the numbers of candidates. Moreover, we were also able to confirm its performance for heart failure. Our approach can be applied to additional diseases for biomarker identification and the set of identified DEGs in preeclampsia represents a resource for further studies.\\n'\n",
      " '# GermlineShortV_biovalidation\\n\\n - [Description](#description)\\n  - [Diagram](#diagram)\\n  - [User guide](#user-guide)\\n      - [Quick start guide](#quick-start-guide)\\n  - [Benchmarking](#benchmarking)\\n  - [Workflow summaries](#workflow-summaries)\\n      - [Metadata](#metadata)\\n      - [Component tools](#component-tools)\\n      - [Required (minimum)\\n        inputs/parameters](#required-minimum-inputsparameters)  \\n        [Preparing your own input files](#preparing-input-files)\\n  - [Additional notes](#additional-notes)\\n      - [Understanding your outputs](#understanding-your-outputs)  \\n      - [Performance metrics explained](#performance-metrics-explained)   \\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\\n\\n## Description \\nPopulation-scale WGS cohorts are essential resources for genetic analyses including heritable diseases, evolutionary genomics, conservation biology, and population genomics. Processing raw reads into analysis-ready variants remains challenging. Various mapping and variant calling pipelines have been made publicly available in recent decades. Designing a mapping and variant calling pipeline to meet your needs is dependent on the compute infrastructure you’re working on, the types of variants you’re primarily interested in, and the sequencing technology you use to generate raw sequencing data. Keep in mind that the tools you use to build your pipeline can affect variant calling accuracy. Further, optimisation and customisation of these tools’ commands can also affect their performance. Best-practice recommendations for variant calling pipelines vary dramatically between species and research questions, depending on the availability of genomic resources for the population of interest, genome structure, and clinical relevance of the resulting variant dataset. It is important to not only design a robust variant calling pipeline but also fine-tune it to achieve optimal performance for your dataset and research question. \\n\\nThere are various measurements that you can apply to evaluate the biological accuracy of your germline variant calling pipeline. Currently, no best practice methods for interrogating joint-called variant sets exist in the literature. A number of publicly available, human ‘gold standard’ truth datasets including Platinum Genomes and Genome in a Bottle (GIAB) are useful for benchmarking across high confidence regions of the genome and evaluating the recall and precision of the pipeline. We recommend individuals working with human datasets benchmark their germline variant calling pipelines using one of these datasets. Unfortunately, these resources are not typically available for non-human organisms. \\n\\nHere, we present protocols for benchmarking and validating germline short variant (SNVs and indels) datasets using a combination of methods that can capture the quality of your variant sets for human, non-human model, and non-model organisms. The process you can apply will depend on the organism you’re working with and the genomic resources available to that organism. \\n\\n## Diagram \\n\\n<p align=\"center\"> \\n<img src=\"https://github.com/Sydney-Informatics-Hub/GermlineShortV_biovalidation/blob/main/Benchmarking%20and%20validation%20protocol.png\" width=\"70%\" height=\"70%\">  \\n</p> \\n\\n## User guide \\n###  Quick start guide \\n\\nThese bash scripts were written for the University of Sydney’s high performance computer, Artemis. They can be run on the command line or submitted as PBS jobs. These scripts assume your input is a gzipped multi-sample (cohort) VCF file. Before running, edit the PBS project directive and define the variables at the top of the script. All software used in this protocol is installed on Artemis- to use alternate versions or run on a different compute infrastructure, edit the modules according to your needs.  \\n\\n#### Human datasets \\nFor human datasets, we recommend you benchmark your germline variant calling pipeline using a gold standard dataset such as Platinum Genomes. Raw sequence data in FASTQ format for these datasets can be downloaded along with their high confidence variant calls and regions from public repositories. See [Preparing input files]() for more information on how to download and prepare these files.    \\n\\n##### 1. Collect vcf summary metrics  \\nEdit the PBS -P directive and variables for your dataset in `vcfstat.sh`. Then run script with: \\n\\n```\\nqsub vcfstat.sh (or bash vcfstat.sh)\\n```\\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`. \\n\\n##### 2. Biological benchmarking using a truth set  \\n\\nEdit the PBS -P directive and variables for your files. Then run script with:  \\n\\n```\\nqsub run_happy.sh\\n```\\nThis script will subset your multi-sample VCF into individual samples, prepare them for hap.py, and output a number of files including summary metrics (including recall, precision and F1-score) and ROC count files that can be used to produce ROC curves, separately for SNVs and indels. See the [hap.py user guide](https://github.com/Illumina/hap.py/blob/master/doc/happy.md) for more information on how to interpret hap.py output. ROC curves of Hap.py runs can be plotted using the script [rocplot.Rscript](https://github.com/Illumina/hap.py/blob/master/src/R/rocplot.Rscript).   \\n\\n#### Non-human model organism datasets\\n\\n##### 1. Collect vcf summary metrics  \\nEdit the PBS -P directive and variables for your dataset in `vcfstat.sh`. We recommend you use the set of known variants used for base quality score recalibration to validate population level variants. If you used trio data, unhash the Mendelian error command within the script. Then run script with: \\n\\n```\\nqsub vcfstat.sh (or bash vcfstat.sh)\\n```\\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`.  \\n#### Non-model organism datasets \\n\\n##### 1. Collect vcf summary metrics  \\n\\nEdit the PBS -P directive and variables for your dataset in `vcfstat_nonmodel.sh`. Then run script with: \\n\\n```\\nqsub vcfstat_nonmodel.sh (or bash vcfstat_nonmodel.sh)\\n```\\n\\nThis will produce summary and quality metrics reports and plots for your cohort. It will also produce summary and detail files for known variant representation. BCFtools stats plots will be housed in a directory labelled `${cohort}_vcfplots`. \\n\\n## Benchmarking \\nComing soon!  \\n\\n## Workflow summaries \\n### Metadata \\n|metadata field     | workflow_name / workflow_version  |\\n|-------------------|:---------------------------------:|\\n|Version            | 1.0                 |\\n|Maturity           | stable                            |\\n|Creators           | Georgie Samaha, Tracy Chew, Cali Willet                 |\\n|Source             | NA                                |\\n|License            | NA                                |\\n|Workflow manager   | NA                          |\\n|Container          | None                              |\\n|Install method     | Manual                            |\\n|GitHub             | NA                                |\\n|bio.tools \\t        | NA                                |\\n|BioContainers      | NA                                | \\n|bioconda           | NA                                |\\n\\n### Component tools \\n\\nbcftools/1.14  \\nhtslib/1.14  \\npython/3.8.2  \\nR/4.1.1  \\nhap.py/0.3.14  \\n\\n### Required (minimum) inputs/parameters \\n\\n- Multi-sample or single sample VCF file (VCF.gz format)\\n- List of sample IDs that match the VCF (.txt format)\\n- Known variant dataset (VCF format. Human and non-human model organisms only)\\n- Pedigree file (format: mother,father,offspring. Trios or Platinum Genomes only)\\n- Truth set variant calls (VCF.gz format. Human, Platinum Genomes only)\\n- High confidence call regions (BED format. Human, Platinum Genomes only)\\n\\n### Preparing input files \\n\\n#### Gold standard variant truth sets  \\n\\nThe benchmarking protocol for human datasets assumes you have performed mapping and germline variant calling on a gold standard truth set. These datasets contain millions of variants that have been confirmed using orthologous technologies [Eberle et al. 2017](https://doi.org/10.1101/gr.210500.116).   \\n\\nWe recommend you use the Platinum Genomes dataset for benchmarking germline variant calling pipelines that include joint genotyping of multiple samples. Six members, comprising two trios, of the Platinum Genomes dataset can be downloaded from the Illumina BaseSpace Sequence Hub, the ENA, or dbGaP. The Platinum Genomes dataset contains multiple files including the following files you will need for running `run_happy.sh`: \\n- Paired-end FASTQ files for each sample\\n- High-confidence germline variant VCF files for each sample\\n- High-confidence genomic regions (BED format)\\n\\nCurrently, these files are available for Hg19 (GRCh37) and Hg38 (GRCh38) . Links to raw data are [here](https://github.com/Illumina/PlatinumGenomes). BaseSpace offers a command line tool for downloading files, see [here](https://developer.basespace.illumina.com/docs/content/documentation/cli/cli-examples) for instructions. \\n\\n#### Providing your own ‘truth set’ \\n*A word of caution*- testing the performance of your pipeline using a truth set is only intended to estimate the overall quality of your pipeline and detect any potential sources of error in your method. It is not intended to test the truthfulness of your variant set. See [here](https://gatk.broadinstitute.org/hc/en-us/articles/360035531572-Evaluating-the-quality-of-a-germline-short-variant-callset) for further discussion of the assumptions we make about truth sets. Most non-human organisms do not have access to gold standard truth set resources like the Platinum Genomes dataset. However there are a few alternative options you could try: \\n - Genotyping arrays: if you have genotyping data for the same samples you tested your germline variant calling pipeline with, you can reformat these to VCF using a tool like [PLINK’s recode](https://www.cog-genomics.org/plink/1.9/data#recode) and use it as a truth set. \\n - Known variant datasets: if your organism of interest has a set of known population-level variants you can use these as a truth-set. Just remember that these variants might not always be validated (i.e. dbSNP). \\n\\nUsing this method you will need to also provide your own high-confidence regions file in BED format. The location and size of these regions will depend on your dataset, organism, reference assembly and sequencing method. Typically these regions would exclude centromeres, telomeres and repetitive parts of the genome that are likely to complicate variant calling.   \\n\\n\\n## Additional notes \\n\\nTest data for Hap.py can be found [here](https://github.com/Illumina/hap.py/blob/master/doc/microbench.md)  \\n\\nInstructions on how to install Hap.py can be found [here](https://github.com/Illumina/hap.py#installation)   \\n\\nThis warning may be thrown by Hap.py and can be ignored: `WARNING  No reference file found at default locations. You can set the environment variable \\'HGREF\\' or \\'HG19\\' to point to a suitable Fasta file.`  \\n\\n\\n### Understanding your outputs \\nThe following files will be produced and stored in your designated working directory. They will all be labelled with your specified cohort name.  \\n\\n#### Variant based metrics \\nProduced by BCFtools stats command. Output file:\\n- ${cohort}.bcftools.metrics  \\n- ${cohort}_bcftools.metrics_vcfstatplots (directory and files)  \\n\\n#### Sample based metrics   \\nProduced by BCFtools smplstats and mendelian commands. Output files:\\n- ${cohort}.smplstats\\n- ${cohort}.smplstats.pdf\\n- ${cohort}.Mendelianerr\\n\\n#### Known variant concordance \\nProduced by GATK CollectVariantCallingMetrics command. Output files:\\n- ${cohort}.known.variant_calling_summary_metrics\\n- ${cohort}.known.variant_calling_detail_metrics\\n\\n#### Biological validation using a truth set \\nProduced by Hap.py. Output files:\\n- ${sample}.happy.metrics.json.gz\\n- ${sample}.happy.roc.all.csv.gz\\n- ${sample}.happy.roc.Locations.INDEL.csv.gz\\n- ${sample}.happy.roc.Locations.INDEL.PASS.csv.gz\\n- ${sample}.happy.roc.Locations.SNP.csv.gz\\n- ${sample}.happy.roc.Locations.SNP.PASS.csv.gz\\n- ${sample}.happy.roc.tsv\\n- ${sample}.happy.runinfo.json\\n- ${sample}.happy.summary.csv\\n\\n### Performance metrics explained  \\n\\n|Metric                                |Expected/ideal value                                |Tool           |Relevance                                                                                                      |\\n|--------------------------------------|----------------------------------------------------|---------------|---------------------------------------------------------------------------------------------------------------|\\n|Number of SNVs and indels (per sample)|Human WGS: ~4.4M, Human WES: ~41k, Species dependent|bcftools stats |Population, sequencing approach, and genomic region dependent. Alone, this metric cannot indicate data quality.|\\n|Indel length distribution             |Indel length range is 1-10,000bp.                   |bcftools stats |Increased length is conflated with reduced mapping quality. Distribution is dataset dependent. Recommend filtering for high quality.|\\n|Depth of coverage                     |Depends on the sequencing coverage of samples.      |bcftools stats |Dramatic deviation from expected distribution can indicate artifactual bias.                                   |\\n|Substitution type counts              |See TiTv ratio.                                     |bcftools stats |Twice as many possible transversions as transitions. See [here](https://dx.doi.org/10.1093%2Fbioinformatics%2Fbtu668)  |\\n|TiTv ratio (genome wide)              |For mammals: WGS: 2.0-2.1, WES: 3.0-3.3             |bcftools stats |Dramatic deviation from expected ratio can indicate artifactual bias. Typically elevated in coding regions where transversions are more likely to occur. |\\n|Base quality distribution             |Dataset dependent.                                  |bcftools stats |This will reflect the quality based filtering you performed. Dramatic deviation from expected ratio can indicate artifactual bias.|\\n|Indel ratio                           |Common: ~1.0, Rare: 0.2-0.5                         |GATK CollectVariantCallingMetrics|This should be evaluated after custom filtering variants for your needs. Dramatic deviation from expected ratio can indicate artifactual bias.|\\n|Het/hom(non-ref)                      |~2.0 assuming Hardy-Weinberg equilibrium.           |GATK CollectVariantCallingMetrics|Ancestry dependent, can vary dramatically. See [Wang et al. 2015](https://dx.doi.org/10.1093%2Fbioinformatics%2Fbtu668)|\\n|Mendelian error                       |0                                                   |BCFtools +mendelian|Mendelian inheritance errors are likely erroneous genotype calls. See [Pilipenko et al. 2014](https://dx.doi.org/10.1186%2F1753-6561-8-S1-S21)|\\n|True positives                        |Dataset dependent.                                  |Hap.py         |Number of query variants that are present in the truth set.                                                    |\\n|False negatives                       |Dataset dependent.                                  |Hap.py         |Number of variants in truth set, not present in query VCF.                                                     |\\n|False positives                       |Dataset dependent.                                  |Hap.py         |Number of variants in query VCF, not present in truth set.                                                     |\\n|Recall                                |1                                                   |Hap.py         |Absence of false negatives. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)               |\\n|Precision                             |1                                                   |Hap.py         |Absence of false positives. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)               |\\n|F1-score                              |1                                                   |Hap.py         |Harmonic mean of recall and precision. See [Krusche et al. 2019](https://doi.org/10.1038/s41587-019-0054-x)    |\\n|Genotype errors (FP.GT)               |Dataset dependent.                                  |Hap.py         |Number of query variants with incorrect genotype                                                               |\\n\\n### Resources and references \\n\\nEberle, M. A., Fritzilas, E., Krusche, P., Källberg, M., Moore, B. L., Bekritsky, M. A., Iqbal, Z., Chuang, H. Y., Humphray, S. J., Halpern, A. L., Kruglyak, S., Margulies, E. H., McVean, G., & Bentley, D. R. (2017). A reference data set of 5.4 million phased human variants validated by genetic inheritance from sequencing a three-generation 17-member pedigree. Genome research, 27(1), 157–164. https://doi.org/10.1101/gr.210500.116   \\n\\nKoboldt, D.C. Best practises for variant calling in clinical sequencing. Genome Med 12, 91 (2020). https://doi.org/10.1186/s13073-020-00791-w  \\n\\nKrusche, P., Trigg, L., Boutros, P.C. et al. Best practices for benchmarking germline small-variant calls in human genomes. Nat Biotechnol 37, 555–560 (2019). https://doi.org/10.1038/s41587-019-0054-x  \\n\\nMarshall, C.R., Chowdhury, S., Taft, R.J. et al. Best practices for the analytical validation of clinical whole-genome sequencing intended for the diagnosis of germline disease. npj Genom. Med. 5, 47 (2020). https://doi.org/10.1038/s41525-020-00154-9   \\n\\nPilipenko, V.V., He, H., Kurowski, B.G. et al. Using Mendelian inheritance errors as quality control criteria in whole genome sequencing data set. BMC Proc 8, S21 (2014). https://doi.org/10.1186/1753-6561-8-S1-S21   \\n\\nWang, J., Raskin, J., Samuels, D., Shyr, Y., Guo, Y., Genome measures used for quality control are dependent on gene function and ancestry, Bioinformatics 31, 318–323 (2015)  https://doi.org/10.1093/bioinformatics/btu668  \\n\\n\\n## Help/FAQ/Troubleshooting\\n\\nIf Hap.py throws an error, search the [issues at Hap.py GitHub repository](https://github.com/Illumina/hap.py/issues) and attempt to resolve it before submitting an issue here.    \\n\\n## Acknowledgements/citations/credits  \\n\\n### Authors \\n- Georgie Samaha (Sydney Informatics Hub, University of Sydney)   \\n- Tracy Chew (Sydney Informatics Hub, University of Sydney)  \\n- Cali Willet (Sydney Informatics Hub, University of Sydney)  \\n- Nandan Deshpande (Sydney Informatics Hub, University of Sydney)\\n\\nAcknowledgements (and co-authorship, where appropriate) are an important way for us to demonstrate the value we bring to your research. Your research outcomes are vital for ongoing funding of the Sydney Informatics Hub and national compute facilities. We suggest including the following acknowledgement in any publications that follow from this work:  \\n\\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia.  \\n'\n",
      " '# HiFi *de novo* genome assembly workflow\\n\\nHiFi-assembly-workflow is a bioinformatics pipeline that can be used to analyse Pacbio CCS reads for *de novo* genome assembly using PacBio Circular Consensus Sequencing (CCS)  reads. This workflow is implemented in Nextflow and has 3 major sections. \\n \\nPlease refer to the following documentation for detailed description of each workflow section:\\n \\n- [Pre-assembly quality control (QC)](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-1-pre-assembly-quality-control)\\n- [Assembly](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-2-assembly)\\n- [Post-assembly QC](https://github.com/AusARG/hifi-assembly-workflow/blob/master/recommendations.md#stage-3-post-assembly-quality-control)\\n\\n## HiFi assembly workflow flowchart\\n\\n![](https://github.com/AusARG/hifi-assembly-workflow/blob/master/workflow.png?raw=true)\\n\\n# Quick Usage:\\nThe pipeline has been tested  on NCI Gadi and AGRF balder cluster. If needed to run on AGRF cluster, please contact us at bioinformatics@agrf.org.au.\\nPlease note for running this on NCI Gadi you need access. Please refer to Gadi guidelines for account creation and usage: these can be found at https://opus.nci.org.au/display/Help/Access.\\n\\nHere is an example that can be used to run a phased assembly on Gadi:\\n\\n```\\nModule load nextflow/21.04.3\\nnextflow run Hifi_assembly.nf –bam_folder <PATH TO THE BAM FOLDER> -profile gadi \\n\\nThe workflow accepts 2 mandatory arguments:\\n--bam_folder     --    Full Path to the CCS bam files\\n-profile         --    gadi/balder/local\\n```\\n\\nPlease note that you can either run jobs interactively or submit jobs to the cluster. This is determined by the -profile flag. By passing the gadi tag to the profile argument, the jobs are submitted and run on the cluster.\\n\\n# General recommendations for using the HiFi *de novo* genome assembly workflow\\n\\n## Example local profile usage\\n\\n```\\nStart a screen, submit a job, and run the workflow \\nScreen -S ‘name’\\n\\nqsub -I -qnormal -Pwz54 -lwalltime=48:00:00,ncpus=4,mem=200GB,storage=scratch/wz54+gdata/wz54,wd\\nexport MODULEPATH=/apps/Modules/modulefiles:/g/data/wz54/groupResources/modules\\n\\nmodule load nextflow/21.04.3\\nnextflow run /g/data/wz54/groupResources/scripts/pl/hifi_assembly.nf  --bam_folder  <bam-folder_path> -profile local\\n\\n#This load the scripts directory to the environmental PATH and load nextflow module\\nmodule load hifi_assembly/1.0.0 \\n```\\n\\n# Outputs\\n\\nPipeline generates various files and folders here is a brief description: \\nThe pipeline creates a folder called `secondary_analysis` that contains two sub folders named:\\n\\n- `exeReport`     \\n- `Results`       -- Contains preQC, assembly and postQC analysis files\\n\\n## exeReport\\nThis folder contains a computation resource usage summary in various charts and a text file. \\n`report.html` provides a comprehensive summary.\\n\\n## Results\\nThe `Results` folder contains three sub-directories preQC, assembly and postqc. As the name suggests, outputs from the respective workflow sections are placed in each of these folders.\\n\\n### preQC\\nThe following table contains list of files and folder from preQC results\\n\\n| Output folder/file | File             | Description                                                                    |\\n| ------------------ | ---------------- | ------------------------------------------------------------------------------ |\\n| <sample>.fa        |                  | Bam files converted to fasta format                                            |\\n| kmer\\\\_analysis     |                  | Folder containing kmer analysis outputs                                        |\\n|                    | <sample>.jf      | k-mer counts from each sample                                                  |\\n|                    | <sample>.histo   | histogram of k-mer occurrence                                                  |\\n| genome\\\\_profiling  |                  | genomescope profiling outputs                                                  |\\n|                    | summary.txt      | Summary metrics of genome scope outputs                                        |\\n|                    | linear\\\\_plot.png | Plot showing no. of times a k-mer observed by no. of k-mers with that coverage |\\n\\n\\n### Assembly\\nThis folder contains final assembly results in <FASTA> format.\\n\\n- `<sample>_primary.fa` - Fasta file containing primary contigs\\n- `<sample>_associate.fa` - Fasta file containing associated contigs\\n\\n### postqc\\n \\nThe postqc folder contains two sub folders \\n\\n- `assembly_completeness`\\n- `assembly_evaluation`\\n\\n#### assembly_completeness\\nThis contains BUSCO evaluation results for primary and associate contig.\\n\\n#### assembly_evaluation\\nAssembly evaluation folder contains various file formats, here is a brief description for each of the outputs.\\n\\n| File        | Description                                                                               |\\n| ----------- | ----------------------------------------------------------------------------------------- |\\n| report.txt  | Assessment summary in plain text format                                                   |\\n| report.tsv  | Tab-separated version of the summary, suitable for spreadsheets (Google Docs, Excel, etc) |\\n| report.tex  | LaTeX version of the summary                                                              |\\n| icarus.html | Icarus main menu with links to interactive viewers                                        |\\n| report.html | HTML version of the report with interactive plots inside                                  |\\n\\n\\n# Infrastructure usage and recommendations\\n\\n### NCI facility access\\nOne should have a user account set with NCI to access gadi high performance computational facility. Setting up a NCI account is mentioned in detail at the following URL: https://opus.nci.org.au/display/Help/Setting+up+your+NCI+Account \\n  \\nDocumentation for a specific infrastructure should go into a infrastructure documentation template\\nhttps://github.com/AustralianBioCommons/doc_guidelines/blob/master/infrastructure_optimisation.md\\n\\n\\n## Compute resource usage across tested infrastructures\\n\\n|                                       | Computational resource for plant case study |\\n| ------------------------------------- | ------------------------------------------- |\\n|                                       | Time                                        | CPU | Memory | I/O |\\n| Process                               | duration                                    | realtime | %cpu | peak\\\\_rss | peak\\\\_vmem | rchar | wchar |\\n| Converting bam to fasta for sample    | 12m 54s                                     | 12m 48s | 99.80% | 5.2 MB | 197.7 MB | 43.3 GB | 50.1 GB |\\n| Generating k-mer counts and histogram | 26m 43s                                     | 26m 36s | 1725.30% | 19.5 GB | 21 GB | 77.2 GB | 27.1 GB |\\n| Profiling genome characteristics      | 34.7s                                       | 13.2s | 89.00% | 135 MB | 601.2 MB | 8.5 MB | 845.9 KB |\\n| Denovo assembly                       | 6h 51m 15s                                  | 6h 51m 11s | 4744.40% | 84.7 GB | 225.6 GB | 1.4 TB | 456 GB |\\n| evaluate\\\\_assemblies                  | 5m 18s                                      | 4m 54s | 98.20% | 1.6 GB | 1.9 GB | 13.6 GB | 2.8 GB |\\n| assemblies\\\\_completeness              | 25m 57s                                     | 25m 53s | 2624.20% | 22 GB | 25.2 GB | 624.9 GB | 2.9 GB |\\n\\n\\n|                                       | Computational resource for bird case study |\\n| ------------------------------------- | ------------------------------------------ |\\n|                                       | Time                                       | CPU | Memory | I/O |\\n| Process                               | duration                                   | realtime | %cpu | peak\\\\_rss | peak\\\\_vmem | rchar | wchar |\\n| Converting bam to fasta for sample    | 12m 54s                                    | 7m 9s | 86.40% | 5.2 MB | 197.8 MB | 21.5 GB | 27.4 GB |\\n| Generating k-mer counts and histogram | 26m 43s                                    | 15m 34s | 1687.70% | 10.1 GB | 11.7 GB | 44 GB | 16.6 GB |\\n| Profiling genome characteristics      | 34.7s                                      | 1m 15s | 15.30% | 181.7 MB | 562.2 MB | 8.5 MB | 819.1 KB |\\n| De novo assembly                      | 6h 51m 15s                                 | 9h 2m 47s | 1853.50% | 67.3 GB | 98.4 GB | 1 TB | 395.6 GB |\\n| evaluate assemblies                   | 5m 18s                                     | 2m 48s | 97.50% | 1.1 GB | 1.4 GB | 8.7 GB | 1.8 GB |\\n| assemblies completeness               | 25m 57s                                    | 22m 36s | 2144.00% | 22.2 GB | 25 GB | 389.7 GB | 1.4 GB |\\n\\n\\n# Workflow summaries\\n\\n## Metadata\\n\\n| Metadata field   | Pre-assembly quality control                                                      | Primary assembly   | Post-assembly quality control |\\n| ---------------- | --------------------------------------------------------------------------------- | ------------------ | ----------------------------- |\\n| Version          | 1.0                                                                               | 1.0                | 1.0                           |\\n| Maturity         | Production                                                                        | Production         | production                    |\\n| Creators         | Naga, Kenneth                                                                     | Naga, Kenneth      | Naga, Kenneth                 |\\n| Source           | [AusARG/hifi-assembly-workflow](https://github.com/AusARG/hifi-assembly-workflow) |\\n| License          |  MIT License                                                                       | MIT License         | MIT License                     |\\n| Workflow manager | NextFlow                                                                          | NextFlow           | NextFlow                      |\\n| Container        | No containers used                                                                | No containers used | No containers used            |\\n| Install method   | Manual                                                                            | Manual             | Manual                        |\\n\\n\\n## Component tools\\n\\u200b\\n| Workflow element                  | Workflow element version | Workflow title                |\\n| --------------------------------- | ------------------------ | ----------------------------- |\\n| Samtools, jellyfish, genomescope  | 1.0                      | Pre-assembly quality control  |\\n| Improved phased assembler (pbipa) | 1.0                      | Primary assembly              |\\n| Quast and busco                   | 1.0                      | Post-assembly quality control |\\n\\n\\n## Required (minimum) inputs/parameters\\n \\nPATH to HIFI bam folder is the minimum requirement for the processing the pipeline.\\n\\n## Third party tools / dependencies\\n\\nThe following packages are used by the pipeline.\\n\\n- `nextflow/21.04.3`\\n- `samtools/1.12`\\n- `jellyfish/2.3.0`\\n- `genomescope/2.0`\\n- `ipa/1.3.1`\\n- `quast/5.0.2`\\n- `busco/5.2.2`\\n\\nThe following paths contain all modules required for the pipeline.\\n\\n- `/apps/Modules/modulefiles`\\n- `/g/data/wz54/groupResources/modules`\\n\\n---\\n\\n# Help/FAQ/Troubleshooting\\n\\nDirect training and help is available if you are new to HPC and/or new to NCI/Gadi.\\n\\n- Basic information to get started with the NCI Gadi for bioinformatics can be found at https://github.com/AusARG/ABLeS/wiki/temppage.\\n- For NCI support, contact the NCI helpdesk directly at https://www.nci.org.au/users/nci-helpdesk\\n- Queue limits and structure explained at https://opus.nci.org.au/display/Help/4.+PBS+Jobs\\n\\n---\\n\\n# 3rd party Tutorials \\n\\nA tutorial by Andrew Severin on running GenomeScope 1.0 is available here:\\nhttps://github.com/AusARG/hifi-assembly-workflow.git\\n\\nImproved Phased Assembler tutorial is available at \\nhttps://github.com/PacificBiosciences/pbbioconda/wiki/Improved-Phased-Assembler\\n\\nBusco tutorial\\nhttps://wurmlab.com/genomicscourse/2016-SIB/practicals/busco/busco_tutorial\\n\\n---\\n\\n# Licence(s)\\n\\nMIT License\\n\\nCopyright (c) 2022 AusARG\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n---\\n\\n# Acknowledgements/citations/credits\\n\\n> Jung, H. et al. Twelve quick steps for genome assembly and annotation in the classroom. PLoS Comput. Biol. 16, 1–25 (2020).\\n\\n> 2020, G. A. W. No Title. https://ucdavis-bioinformatics-training.github.io/2020-Genome_Assembly_Workshop/kmers/kmers.\\n\\n> Sović, I. et al. Improved Phased Assembly using HiFi Data. (2020).\\n\\n> Gurevich, A., Saveliev, V., Vyahhi, N. & Tesler, G. QUAST: Quality assessment tool for genome assemblies. Bioinformatics 29, 1072–1075 (2013).\\n\\n> Waterhouse, R. M. et al. BUSCO applications from quality assessments to gene prediction and phylogenomics. Mol. Biol. Evol. 35, 543–548 (2018).\\n\\n---\\n'\n",
      " 'A workflow for the quality assessment of mass spectrometry (MS) based proteomics analyses'\n",
      " '### Workflow Kallisto RNAseq \\n**(pseudoalignment on transcripts)**\\n  - Workflow Illumina Quality: https://workflowhub.eu/workflows/336?version=1\\t\\n  - kallisto\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\n  Tools: https://git.wur.nl/unlock/cwl/-/tree/master/cwl<br>\\n  Workflows: https://git.wur.nl/unlock/cwl/-/tree/master/cwl/workflows\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html\\n\\n'\n",
      " '# workflow-qc-of-radseq-reads\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\n## Inputs\\n* demultiplexed reads in fastq format, in a collection\\n* two adapter sequences in fasta format, for input into cutadapt\\n\\n## Steps and outputs\\n\\nThe workflow can be modified to suit your own parameters. \\n\\nThe workflow steps are:\\n* Run FastQC to get statistics on the raw reads, send to MultiQC to create a nice output. This is tagged as \"Report 1\" in the Galaxy history. \\n* Run Cutadapt on the reads to cut adapters - enter two files with adapter sequence at the workflow option for \"Choose file containing 3\\' adapters\". The default settings are on except that the \"Maximum error rate\" for the adapters is set to 0.2 instead of 0.1. Send output statistics to MulitQC, this is \"Report 2\" in the Galaxy history. Note that you may have different requirements here in terms of how many adapter sequences you want to enter. We recommend copying the workflow and modifying as needed. \\n* Send these reads to fastp for additional filtering or trimming. Default settings are on but can be modified as needed. Send output statistics to MultiQC, this is \"Report 3\" in the Galaxy history. \\n* The filtered and trimmed reads are then ready for the stacks workflows. \\n\\n![qc-wf](wf-image-qc.png)\\n'\n",
      " '# workflow-ref-guided-stacks\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\n## Inputs\\n* demultiplexed reads in fastq format, may be output from the QC workflow. Files are in a collection. \\n* population map in text format\\n* reference genome in fasta format\\n\\n## Steps and outputs\\n\\nBWA MEM 2:\\n* The reads are mapped to the reference genome; output in BAM format\\n* The collection of bam files is named something like Map with BWA-MEM on collection 5 (mapped reads in BAM format)\\n* Each of the bam files in the collection is named something like sample_CAAC\\n\\nSamtools stats before filtering:\\n* These bam files are sent to Samtools stats to get statistics; these are then sent to MultiQC to provide a nice output. This is tagged as \"bam stats before filtering\" in the Galaxy history. \\n* The \"General Statistics\" show how many reads were mapped - if there is a low mapping rate, it may be worth re-checking or repeating QC on the raw reads, or considering a different reference genome, or using a de novo approach. To see if many reads have been soft-clipped by Bwa mem (which may affect how well gstacks can work), look at the \"Alignment Metrics\" section, and the row with \"Mapped bases (Cigar)\". Hover over the dots to see sample names especially towards the left of the row - these have the least mapped reads.\\n\\nSamtools view:\\n* This step filters out certain reads from the bam files. The default settings are to exclude reads if they are unmapped, if the alignment is not primary or is supplementary, if the read fails platform/vendor quality checks, and if the read is a PCR or optical duplicate. \\n* The output bams are tagged with \"filtered bams\" in the Galaxy history.\\n\\nSamtools stats after filtering:\\n* Filtered bams are sent again to samtools stats, and statistics to MultiQC, with the report tagged as \"bam stats after filtering\" in the Galaxy history. \\n\\ngstacks:\\n* Filtered bams and a population map are sent to gstacks. The outputs are:\\n* Catalog of loci in fasta format\\n* Variant calls in VCF format\\n* Note: some bam files cause errors here with gstacks. For example, the log file may say \"Error, all records discard with file SampleXYZ.FASTQ.bam, Aborted\". If this occurs, check the bam stats (as described above). Some of the options are to re-do QC on the raw reads, change settings for mapping reads in BWA MEM, and/or delete this sample/s from the population map and proceed to gstacks. \\nThe sample can still remain in the list of bam files but gstacks will only consider what is listed in the pop map. \\n\\npopulations:\\n* gstacks outputs and a population map are snet to the \"populations\" module. The outputs are:\\n* Locus consensus sequences in fasta format\\n* Snp calls, in VCF format\\n* Haplotypes, in VCF format\\n* Summary statistics\\n\\n![qc-wf](wf-ref-guided.png)\\n'\n",
      " '# workflow-denovo-stacks\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\n## Inputs\\n* demultiplexed reads in fastq format, may be output from the QC workflow. Files are in a collection. \\n* population map in text format\\n\\n\\n## Steps and outputs\\n\\nustacks:\\n* input reads go to ustacks. \\n* ustacks assembles the reads into matching stacks (hypothetical alleles). \\n* The outputs are in a collection called something like: Stacks2: ustacks  on data 21, data 20, and others Loci and polymorphism. Click on this to see the files:\\n* for each sample, assembled loci (tsv format), named e.g. sample_CAAC.tags\\n* for each sample, model calls from each locus (tsv format), named e.g. sample_CAAC.snps\\n* for each sample, haplotypes/alleles recorded from each locus (tsv format), named e.g. sample_CAAC.alleles\\n* Please see sections 6.1 to 6.4 in https://catchenlab.life.illinois.edu/stacks/manual/#ufiles for a full description. \\n\\ncstacks:\\n* cstacks will merge stacks into a catalog of consensus loci. \\n* The outputs are in a collection called something like Stacks2: cstacks  on data 3, data 71, and others Catalog of loci. Click on this to see the three files, each in tsv format:\\ncatalog.tags\\ncatalog.snps\\ncatalog.alleles\\n\\n\\nsstacks:\\n* sstacks will compare each sample to the loci in the catalog. \\n* The outputs are in a collection called something like Stacks2: sstacks  on data 3, data 76, and others Matches to the catalog.Click on this to see the files:\\nThere is one file for each sample, named e.g. sample_CAAC.matches, in tsv format. \\n\\ntsv2bam:\\n* Conversion to BAM format\\n* Reads from each sample are now aligned to each locus, and the tsv2bam tool will convert this into a bam file for each sample. \\n* The outputs are in a collection called something like Stacks2: tsv2bam  on data 3, data 94, and others Matches to the catalog.Click on this to see the files:\\nThere is one file for each sample, named e.g sample_CAAC.matches, in BAM format. \\n\\ngstacks:\\n* Catalog of loci in fasta format\\n* Variant calls in VCF format\\n\\npopulations:\\n* Locus consensus sequences in fasta format\\n* Snp calls, in VCF format\\n* Haplotypes, in VCF format\\n* Summary statistics\\n\\n![denovo](wf-denovo.png)\\n'\n",
      " '# workflow-partial-ustacks-only\\n\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\n\\nFor the full de novo workflow see https://workflowhub.eu/workflows/348\\n\\nYou may want to run ustacks with different batches of samples. \\n* To be able to combine these later, there are some necessary steps - we need to keep track of how many samples have already run in ustacks, so that new samples can be labelled with different identifying numbers.  \\n* In ustacks, under \"Processing options\" there is an option called \"Start identifier at\". \\n* The default for this is 1, which can be used for the first batch of samples. These will then be labelled as sample 1, sample 2 and so on. \\n* For any new batches of samples to process in ustacks, we will want to start numbering these at the next available number. e.g. if there were 10 samples in batch 1, this should then be set to start at 11. \\n\\nTo combine multiple outputs from ustacks, providing these have been given appropriate starting identifiers:\\n* Find the ustacks output in the Galaxy history. This will be a list of samples. \\n* Click on the cross button next to the filename to delete, but select \"Collection only\". This releases the items from the list, but they will now be hidden in the Galaxy history.\\n* In the history panel, click on \"hidden\" to reveal any hidden files. Unhide the samples. \\n* Do this for all the batches of ustacks outputs that are needed. \\n* Click on the tick button, tick all the samples needed, then \"For all selected\" choose \"Build dataset list\"\\n* This is now a combined set of samples for input into cstacks. \\n'\n",
      " '# workflow-partial-cstacks-sstacks-gstacks\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\nThis workflow takes in ustacks output, and runs cstacks, sstacks and gstacks. \\n\\nTo generate ustacks output see https://workflowhub.eu/workflows/349\\n\\nFor the full de novo workflow see https://workflowhub.eu/workflows/348\\n'\n",
      " '# workflow-partial-bwa-mem\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\nThis workflow is part of the reference-guided stacks workflow, https://workflowhub.eu/workflows/347\\n\\nInputs\\n* demultiplexed reads in fastq format, may be output from the QC workflow. Files are in a collection. \\n* reference genome in fasta format\\n\\nOutputs\\n* A set of filtered bam files, ready for the next part of the stacks workflow (e.g. gstacks). \\n* Statistics on the bam files. \\n\\n'\n",
      " '# workflow-partial-gstacks-populations\\n\\nThese workflows are part of a set designed to work for RAD-seq data on the Galaxy platform, using the tools from the Stacks program. \\n\\nGalaxy Australia: https://usegalaxy.org.au/\\n\\nStacks: http://catchenlab.life.illinois.edu/stacks/\\n\\nThis workflow is part of the reference-guided stacks workflow, https://workflowhub.eu/workflows/347\\n\\n This workflow takes in bam files and a population map. \\n\\nTo generate bam files see: https://workflowhub.eu/workflows/351\\n'\n",
      " '# Generic variant calling\\n\\n\\nA generic workflow for identification of variants in a haploid genome such as genomes of bacteria or viruses. It can be readily used on MonkeyPox. The workflow accepts two inputs:\\n\\n- A genbank file with the reference genomes\\n- A collection of paired fastqsanger files\\n\\nThe workflow outputs a collection of VCF files for each sample (each fastq pair). These VCF files serve as input to the [Reporting workflow](https://workflowhub.eu/workflows/354). \\n\\nWorkflow can be accessed directly on [usegalaxy.org](https://usegalaxy.org/u/aun1/w/generic-variation-analysis-on-wgs-pe-data)\\n\\nThe general idea of the workflow is:\\n\\n![](https://i.imgur.com/rk40Y4t.png)'\n",
      " '# Generic variation analysis reporting\\n\\nThis workflow generates reports from a list of variants generated by [Variant Calling Workflow](https://workflowhub.eu/workflows/353). \\n\\nThe workflow accepts a single input:\\n\\n- A collection of VCF files\\n\\nThe workflow produces two outputs (format description below):\\n\\n1. A list of variants grouped by Sample\\n2. A list of variants grouped by Variant\\n\\nHere is example of output **by sample**. In this table all varinats in all samples are epxlicitrly listed:\\n\\n| Sample\\t| POS\\t| FILTER\\t| REF\\t| ALT\\t| DP\\t| AF\\t| AFcaller\\t| SB\\t| DP4\\t| IMPACT | FUNCLASS\\t| EFFECT\\t| GENE\\t| CODON\\t| AA\\t| TRID\\t| min(AF)\\t| max(AF)\\t| countunique(change)\\t| countunique(FUNCLASS)\\t| change |\\n|----------|------|----------|---------|-----|-----|------|-----------|-----|-------|----------|---------------|-------------|--------|-------------| ---|--------|----------|-----------|-------------------------|------------------------------|------------|\\n| ERR3485786\\t| 11644\\t| PASS\\t| A\\t| G\\t| 97\\t| 0.979381\\t| 0.907216\\t| 0\\t| 1,1,49,46\\t| LOW\\t| SILENT\\t| SYNONYMOUS_CODING\\t| D7L\\t| tgT/tgC\\t| C512\\t| AKG51361.1\\t| 0.979381\\t| 1\\t| 1\\t| 1\\t| A>G |\\n| ERR3485786\\t| 11904\\t| PASS\\t| T\\t| C\\t| 102\\t| 0.990196\\t| 0.95098\\t| 0\\t| 0,0,51,50\\t| MODERATE\\t| MISSENSE\\t| NON_SYNONYMOUS_CODING\\t| D7L\\t| Act/Gct\\t| T426A\\t| AKG51361.1\\t| 0.990196\\t| 1\\t| 1\\t| 1\\t| T>C |\\n\\n> **Note** the two alernative allele frequency fields: \"AFcaller\" ans \"AF\". LoFreq reports AF values listed in \"AFcaller\". They incorrect due to the known LoFreq [bug](https://github.com/CSB5/lofreq/issues/80). To correct for this we are recomputing AF values from DP4 and DP fields as follows: `AF == (DP4[2] + DP4[3]) / DP.`\\n\\nHere is an example of output **by variant**. In this table data is aggregated by variant across all samples in which this variant is present:\\n\\n| POS\\t| REF\\t| ALT\\t| IMPACT\\t| FUNCLASS\\t| EFFECT\\t| GENE\\t| CODON\\t| AA\\t| TRID\\t| countunique(Sample)\\t| min(AF)\\t| max(AF)\\t| SAMPLES(above-thresholds)\\t| SAMPLES(all)\\t| AFs(all)\\t| change |\\n|-----|-------|-----|-----------|----------------|------------|----------|-----------|------|--------|------------------------|----------|-----------|------------------------------------|------------------|----------|---------|\\n| 11644\\t| A\\t| G\\t| LOW\\t| SILENT\\t| SYNONYMOUS_CODING\\t| D7L\\t| tgT/tgC\\t| C512\\t| AKG51361.1\\t| 11\\t| 0.979381\\t| 1\\t| ERR3485786,ERR3485787... | \\tERR3485786,ERR3485787,ERR3485789 ... \\t| 0.979381,1.0...\\t| A>G |\\n| 11904\\t| T\\t| C\\t| MODERATE\\t| MISSENSE\\t| NON_SYNONYMOUS_CODING\\t| D7L\\t| Act/Gct\\t| T426A\\t| AKG51361.1\\t| 12\\t| 0.990196\\t| 1\\t| ERR3485786,ERR3485787... | \\tERR3485786,ERR3485787,ERR3485789... | \\t0.990196,1.0,1.0... | \\tT>C | \\n\\nThe workflow can be accessed at [usegalaxy.org](https://usegalaxy.org/u/aun1/w/genetic-variation-analysis-reporting)\\n\\nThe general idea of the workflow is:\\n\\n![](https://i.imgur.com/k2cIZK5.png)\\n\\n'\n",
      " '# Generic consensus building\\n\\nThis workflow generates consensus sequences using a list of variants generated by [Variant Calling Workflow](https://workflowhub.eu/workflows/353). \\n\\nThe workflow accepts a single input:\\n\\n- A collection of VCF files\\n\\nThe workflow produces a single output:\\n\\n- Consensus sequence for each input VCF file\\n\\nThe workflow can be accessed at [usegalaxy.org](https://usegalaxy.org/u/aun1/w/consensus-construction)'\n",
      " 'Generic variation analysis on WGS PE data\\n-------------------------------------------\\n\\nThis workflows performs paired end read mapping with bwa-mem followed by\\nsensitive variant calling across a wide range of AFs with lofreq and variant\\nannotation with snpEff. The reference genome can be provided as a GenBank file.\\n'\n",
      " \"This workflow begins from a set of genome assemblies of different samples, strains, species. The genome is first annotated with Funnanotate. Predicted proteins are furtner annotated with Busco. Next, 'ProteinOrtho' finds orthologs across the samples and makes orthogroups. Orthogroups where all samples are represented are extracted. Orthologs in each orthogroup are aligned with ClustalW. Test dataset: https://zenodo.org/record/6610704#.Ypn3FzlBw5k\"\n",
      " \"Phylogenetic reconstruction using genome-wide and single-gene alignment data. Here we use maximum likelihood reconstruction program IQTree. \\nData can be prepared using the [phylogenetic data preparation workflow](http://workflowhub.eu/workflows/358) prior to phylogenetic reconstruction.\\nResulting trees can be viewed interactively using Galaxy's 'Phyloviz' or 'Phylogenetic Tree Visualization'\"\n",
      " \"MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\\n\\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#assembly-analysis-pipeline\\n\"\n",
      " \"MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\\n\\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#amplicon-analysis-pipeline\\n\"\n",
      " \"MGnify (http://www.ebi.ac.uk/metagenomics) provides a free to use platform for the assembly, analysis and archiving of microbiome data derived from sequencing microbial populations that are present in particular environments. Over the past 2 years, MGnify (formerly EBI Metagenomics) has more than doubled the number of publicly available analysed datasets held within the resource. Recently, an updated approach to data analysis has been unveiled (version 5.0), replacing the previous single pipeline with multiple analysis pipelines that are tailored according to the input data, and that are formally described using the Common Workflow Language, enabling greater provenance, reusability, and reproducibility. MGnify's new analysis pipelines offer additional approaches for taxonomic assertions based on ribosomal internal transcribed spacer regions (ITS1/2) and expanded protein functional annotations. Biochemical pathways and systems predictions have also been added for assembled contigs. MGnify's growing focus on the assembly of metagenomic data has also seen the number of datasets it has assembled and analysed increase six-fold. The non-redundant protein database constructed from the proteins encoded by these assemblies now exceeds 1 billion sequences. Meanwhile, a newly developed contig viewer provides fine-grained visualisation of the assembled contigs and their enriched annotations.\\n\\nDocumentation: https://docs.mgnify.org/en/latest/analysis.html#raw-reads-analysis-pipeline\"\n",
      " 'To discover causal mutations of inherited diseases it’s common practice to do a trio analysis. In a trio analysis DNA is sequenced of both the patient and parents. Using this method, it’s possible to identify multiple inheritance patterns. Some examples of these patterns are autosomal recessive, autosomal dominant, and de-novo variants, which are represented in the figure below. To elaborate, the most left tree shows an autosomal dominant inhertitance pattern where the offspring inherits a faulty copy of the gene from one of the parents.\\n\\nTo discover these mutations either whole exome sequencing (WES) or whole genome sequencing (WGS) can be used. With these technologies it is possible to uncover the DNA of the parents and offspring to find (shared) mutations in the DNA. These mutations can include insertions/deletions (indels), loss of heterozygosity (LOH), single nucleotide variants (SNVs), copy number variations (CNVs), and fusion genes.\\n\\nIn this workflow  we will also make use of the HTSGET protocol, which is a program to download our data securely and savely. This protocol has been implemented in the EGA Download Client Tool: toolshed.g2.bx.psu.edu/repos/iuc/ega_download_client/pyega3/4.0.0+galaxy0 tool, so we don’t have to leave Galaxy to retrieve our data.\\n\\nWe will not start our analysis from scratch, since the main goal of this tutorial is to use the HTSGET protocol to download variant information from an online archive and to find the causative variant from those variants. If you want to learn how to do the analysis from scratch, using the raw reads, you can have a look at the Exome sequencing data analysis for diagnosing a genetic disease tutorial.'\n",
      " 'This workflow demonstrates the usage of the [Community Earth System Model](https://www.cesm.ucar.edu/) on Galaxy Europe. \\n\\nA fully coupled B1850 compset with resolution f19_g17 is run for 1 month.\\n\\n![](https://nordicesmhub.github.io/GEO4962/fig/newcase.png)'\n",
      " '# VGP Workflow #1\\n\\nThis workflow collects the metrics on the properties of the genome under consideration by analyzing the k-mer frequencies. It provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It uses reads from two parental genomes to partition long reads from the offspring into haplotype-specific k-mer databases.\\n\\n### Inputs\\n\\n-   Collection of Hifi long reads in FASTQ format\\n-   Paternal short-read Illumina sequencing reads in FASTQ format\\n-   Maternal short-read Illumina sequencing reads in FASTQ format\\n\\n### Outputs\\n\\n-   Meryl databases of k-mer counts\\n    - Child\\n    - Paternal haplotype\\n    - Maternal haplotype\\n-   GenomeScope metrics of child and parental genomes\\n    -   Linear plot\\n    -   Log plot\\n    -   Transformed linear plot\\n    -   Transformed log plot\\n    -   Summary\\n    -   Model\\n    -   Model parameteres'\n",
      " '# VGP Workflow #1\\n\\nThis workflow produces a Meryl database and Genomescope outputs that will be used to determine parameters for following workflows, and assess the quality of genome assemblies. Specifically, it provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality.\\n\\n### Inputs\\n\\n-   Collection of Hifi long reads in FASTQ format\\n\\n### Outputs\\n\\n-   Meryl Database of kmer counts\\n-   GenomeScope\\n    -   Linear plot\\n    -   Log plot\\n    -   Transformed linear plot\\n    -   Transformed log plot\\n    -   Summary\\n    -   Model\\n    -   Model parameteres'\n",
      " '### Workflow (hybrid) metagenomic assembly and binning + GEMs\\n_Accepts both Illumina and Long reads (ONT/PacBio)_\\n\\n- **Workflow Illumina Quality:** https://workflowhub.eu/workflows/336?version=1\\t\\n- **Workflow LongRead Quality:** https://workflowhub.eu/workflows/337\\n    \\n- Kraken2 taxonomic classification of FASTQ reads\\n- SPAdes/Flye (Assembly)\\n- QUAST (Assembly quality report)\\n\\n**Workflow binnning** https://workflowhub.eu/workflows/64?version=11  (optional)\\n- Metabat2/MaxBin2/SemiBin\\n- DAS Tool\\n- CheckM\\n- BUSCO\\n- GTDB-Tk\\n  \\n**Workflow Genome-scale metabolic models** https://workflowhub.eu/workflows/372 (optional)\\n- CarveMe (GEM generation)\\n- MEMOTE (GEM test suite)\\n- SMETANA (Species METabolic interaction ANAlysis)\\n\\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\nhttps://gitlab.com/m-unlock/cwl<br>\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\\n'\n",
      " '### Workflow for Metagenomics from bins to metabolic models (GEMs)\\n\\n**Summary**\\n  - Prodigal gene prediction\\n  - CarveMe genome scale metabolic model reconstruction\\n  - MEMOTE for metabolic model testing\\n  - SMETANA Species METabolic interaction ANAlysis\\n\\nOther UNLOCK workflows on WorkflowHub: https://workflowhub.eu/projects/16/workflows?view=default<br>\\n\\n**All tool CWL files and other workflows can be found here:**<br>\\nTools: https://gitlab.com/m-unlock/cwl<br>\\nWorkflows: https://gitlab.com/m-unlock/cwl/workflows\\n\\n**How to setup and use an UNLOCK workflow:**<br>\\nhttps://m-unlock.gitlab.io/docs/setup/setup.html<br>\\n'\n",
      " 'An example workflow to allow users to run the Specimen Data Refinery tools on data provided in an input CSV file.'\n",
      " 'An example workflow for the Specimen Data Refinery tool, allowing an individual tool to be used'\n",
      " \"# ABR\\\\_Threshold_Detection\\n\\n## What is this?\\n\\nThis code can be used to automatically determine hearing thresholds from ABR hearing curves. \\n\\nOne of the following methods can be used for this purpose:\\n \\n+ neural network (NN) training, \\n+ calibration of a self-supervised sound level regression (SLR) method \\n\\non given data sets with manually determined hearing thresholds.\\n\\n## Installation:\\n\\nRun inside the [src](./src) directory:\\n\\n### Installation as python package\\n\\n```\\npip install -e ./src        (Installation as python package)\\n```\\n\\n### Installation as conda virtual environment\\n```\\nconda create -n abr_threshold_detection python=3.7\\nconda activate abr_threshold_detection\\nconda install pip\\npip install -e ./src\\n```\\n\\n## Usage:\\nData files can be downloaded here: [https://zenodo.org/deposit/5779876](https://zenodo.org/deposit/5779876).\\n\\nFor the Jupyter Notebooks (see the [`notebooks`](./notebooks) directory) to run, the path to the data has to be defined. For this, see the corresponding documentation of the respective notebooks.\\n\\n### Using NNs (`./src/ABR_ThresholdFinder_NN`)\\n\\nThe neural network models were trained in `./src/notebooks/GMCtrained_NN*_training.ipynb` with GMC data and in `./src/notebooks/INGtrained_NN*_training.ipynb` with ING data.\\n\\n```\\nimport ABR_ThresholdFinder_NN.data_preparation as dataprep\\nfrom ABR_ThresholdFinder_NN.models import create_model_1, compile_model_1\\n```\\nFor automatic threshold detection based on NNs, `GMCtrained_NN_threshold_detection.ipynb` and `INGtrained_NN_threshold_detection.ipynb` in `./src/notebooks` can be used.\\n\\n```\\nimport ABR_ThresholdFinder_NN.data_preparation as dataprep\\nimport ABR_ThresholdFinder_NN.thresholder as abrthr\\n```\\n\\n### Using the SLR method (`./src/ABR_ThresholdFinder_SLR`)\\n\\nIn `./src/notebooks/GMCcalibrated_SLR_threshold_detection.ipynb` and `./src/notebooks/INGcalibrated_SLR_threshold_detection.ipynb` it is shown how to use the module to:\\n\\n+ train a threshold detector on a data set and estimate the thresholds\\n+ save a trained model\\n+ load a model\\n+ apply a trained threshold estimator to a data set\\n+ evaluate thresholds by comparing it to a ground truth\\n+ evaluate thresholds by analysing signal averages\\n\\n```\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom ABR_ThresholdFinder_SLR import ABR_Threshold_Detector_multi_stimulus\\nfrom ABR_ThresholdFinder_SLR.evaluations import evaluate_classification_against_ground_truth, plot_evaluation_curve_for_specific_stimulus\\n```\\n\\n##### Evaluate thresholds by comparing it with a 'ground truth' (a human set threshold in this case)\\n\\nFor example:\\n\\n```\\n# 5dB buffer\\nevaluation = evaluate_classification_against_ground_truth(GMC_data2, 5, \\n                                 frequency = 'frequency',\\n                                 mouse_id = 'mouse_id',\\n                                 sound_level = 'sound_level',\\n                                 threshold_estimated = 'slr_estimated_thr',\\n                                 threshold_ground_truth = 'threshold')\\n```     \\n### Compute and plot evaluation curves that allow to judge the quality of a thresholding\\n\\nFour threshold types are evaluated and compared:\\n\\n+ the threshols predicted with neural networks ('threshold NN')\\n+ the thresholds estimated by a sound level regression method ('threshold SLR')\\n+ the human ground truth ('threshold manual')\\n+ a constant threshold ('50')\\n\\nFor more details, please see `Evaluation_of_ML_detected_thresholds.ipynb` in `./src/notebooks`.\\n\\n## Folder structure:\\n\\n### [`data`](./data)\\nContains the preprocessed ABR and mouse phenotyping datasets from GMC and Ingham et al. in csv format, as well as the mouse ID distributions stored as numpy arrays for neural networks training, validation and testing.\\n\\n### [`models`](./models)\\nContains the trained models of the two neural networks and the SLR method, but also the predictions of the first neural network with which the second neural network was fed.\\n\\n### [`models_cross-validation`](./models_cross-validation)\\nContains the models that resulted from the cross-validation of the neural networks.\\n\\n### [`notebooks`](./notebooks)\\nContains the Jupyter notebooks used for training, testing and evaluation of the neural networks and the SLR method, as well as those used for the hearing curve analysis.\\n\\n### [`notebooks_reports`](./notebooks_reports)\\nContains the contents of Jupyter notebooks in html format.\\n\\n### [`results`](./results)\\nContains the predictions or estimates made by the neural networks or the SLR method for the two data sets from GMC and Ingham et al. but also all the plots made to analyse the results.\\n\\n### [`src`](./src)\\nContains the Python scripts used in the Jupyter notebooks.\"\n",
      " 'Lysozyme in Water simplest version, from COMPSs Tutorial. The original idea of this worklfow comes from http://www.mdtutorials.com/gmx/lysozyme/index.html'\n",
      " 'A prototype implementation of the Air Quality Prediction pipeline in Galaxy, using CWL tools.'\n",
      " \"# Snakemake workflow: dna-seq-varlociraptor\\n\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥6.3.0-brightgreen.svg)](https://snakemake.github.io)\\n[![GitHub actions status](https://github.com/snakemake-workflows/dna-seq-varlociraptor/workflows/Tests/badge.svg?branch=master)](https://github.com/snakemake-workflows/dna-seq-varlociraptor/actions?query=branch%3Amaster+workflow%3ATests)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.4675661.svg)](https://doi.org/10.5281/zenodo.4675661)\\n\\n\\nA Snakemake workflow for calling small and structural variants under any kind of scenario (tumor/normal, tumor/normal/relapse, germline, pedigree, populations) via the unified statistical model of [Varlociraptor](https://varlociraptor.github.io).\\n\\n\\n## Usage\\n\\nThe usage of this workflow is described in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?usage=snakemake-workflows%2Fdna-seq-varlociraptor).\\n\\nIf you use this workflow in a paper, don't forget to give credits to the authors by citing the URL of this (original) repository and its DOI (see above).\\n\"\n",
      " \"# metaGOflow: A workflow for marine Genomic Observatories' data analysis\\n\\n![logo](https://raw.githubusercontent.com/hariszaf/metaGOflow-use-case/gh-pages/assets/img/metaGOflow_logo_italics.png)\\n\\n\\n## An EOSC-Life project\\n\\nThe workflows developed in the framework of this project are based on `pipeline-v5` of the MGnify resource.\\n\\n> This branch is a child of the [`pipeline_5.1`](https://github.com/hariszaf/pipeline-v5/tree/pipeline_5.1) branch\\n> that contains all CWL descriptions of the MGnify pipeline version 5.1.\\n\\n## Dependencies\\n\\nTo run metaGOflow you need to make sure you have the following set on your computing environmnet first:\\n\\n- python3 [v 3.8+]\\n- [Docker](https://www.docker.com) [v 19.+] or [Singularity](https://apptainer.org) [v 3.7.+]/[Apptainer](https://apptainer.org) [v 1.+]\\n- [cwltool](https://github.com/common-workflow-language/cwltool) [v 3.+]\\n- [rdflib](https://rdflib.readthedocs.io/en/stable/) [v 6.+]\\n- [rdflib-jsonld](https://pypi.org/project/rdflib-jsonld/) [v 0.6.2]\\n- [ro-crate-py](https://github.com/ResearchObject/ro-crate-py) [v 0.7.0]\\n- [pyyaml](https://pypi.org/project/PyYAML/) [v 6.0]\\n- [Node.js](https://nodejs.org/) [v 10.24.0+]\\n- Available storage ~235GB for databases\\n\\n### Storage while running\\n\\nDepending on the analysis you are about to run, disk requirements vary.\\nIndicatively, you may have a look at the metaGOflow publication for computing resources used in various cases.\\n\\n## Installation\\n\\n### Get the EOSC-Life marine GOs workflow\\n\\n```bash\\ngit clone https://github.com/emo-bon/MetaGOflow\\ncd MetaGOflow\\n```\\n\\n### Download necessary databases (~235GB)\\n\\nYou can download databases for the EOSC-Life GOs workflow by running the\\n`download_dbs.sh` script under the `Installation` folder.\\n\\n```bash\\nbash Installation/download_dbs.sh -f [Output Directory e.g. ref-dbs] \\n```\\nIf you have one or more already in your system, then create a symbolic link pointing\\nat the `ref-dbs` folder or at one of its subfolders/files.\\n\\nThe final structure of the DB directory should be like the following:\\n\\n````bash\\nuser@server:~/MetaGOflow: ls ref-dbs/\\ndb_kofam/  diamond/  eggnog/  GO-slim/  interproscan-5.57-90.0/  kegg_pathways/  kofam_ko_desc.tsv  Rfam/  silva_lsu/  silva_ssu/\\n````\\n\\n## How to run\\n\\n### Ensure that `Node.js` is installed on your system before running metaGOflow\\n\\nIf you have root access on your system, you can run the commands below to install it:\\n\\n##### DEBIAN/UBUNTU\\n```bash\\nsudo apt-get update -y\\nsudo apt-get install -y nodejs\\n```\\n\\n##### RH/CentOS\\n```bash\\nsudo yum install rh-nodejs<stream version> (e.g. rh-nodejs10)\\n```\\n\\n### Set up the environment\\n\\n#### Run once - Setup environment\\n\\n- ```bash\\n  conda create -n EOSC-CWL python=3.8\\n  ```\\n\\n- ```bash\\n  conda activate EOSC-CWL\\n  ```\\n\\n- ```bash\\n  pip install cwlref-runner cwltool[all] rdflib-jsonld rocrate pyyaml\\n\\n  ```\\n\\n#### Run every time\\n\\n```bash\\nconda activate EOSC-CWL\\n``` \\n\\n### Run the workflow\\n\\n- Edit the `config.yml` file to set the parameter values of your choice. For selecting all the steps, then set to `true` the variables in lines [2-6].\\n\\n#### Using Singularity\\n\\n##### Standalone\\n- run:\\n   ```bash\\n   ./run_wf.sh -s -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\\n   ``\\n\\n##### Using a cluster with a queueing system (e.g. SLURM)\\n\\n- Create a job file (e.g., SBATCH file)\\n\\n- Enable Singularity, e.g. module load Singularity & all other dependencies \\n\\n- Add the run line to the job file\\n\\n\\n#### Using Docker\\n\\n##### Standalone\\n- run:\\n    ``` bash\\n    ./run_wf.sh -n osd-short -d short-test-case -f test_input/wgs-paired-SRR1620013_1.fastq.gz -r test_input/wgs-paired-SRR1620013_2.fastq.gz\\n  ```\\n  HINT: If you are using Docker, you may need to run the above command without the `-s' flag.\\n\\n## Testing samples\\nThe samples are available in the `test_input` folder.\\n\\nWe provide metaGOflow with partial samples from the Human Metagenome Project ([SRR1620013](https://www.ebi.ac.uk/ena/browser/view/SRR1620013) and [SRR1620014](https://www.ebi.ac.uk/ena/browser/view/SRR1620014))\\nThey are partial as only a small part of their sequences have been kept, in terms for the pipeline to test in a fast way. \\n\\n\\n## Hints and tips\\n\\n1. In case you are using Docker, it is strongly recommended to **avoid** installing it through `snap`.\\n\\n2. `RuntimeError`: slurm currently does not support shared caching, because it does not support cleaning up a worker\\n   after the last job finishes.\\n   Set the `--disableCaching` flag if you want to use this batch system.\\n\\n3. In case you are having errors like:\\n\\n```\\ncwltool.errors.WorkflowException: Singularity is not available for this tool\\n```\\n\\nYou may run the following command:\\n\\n```\\nsingularity pull --force --name debian:stable-slim.sif docker://debian:stable-sli\\n```\\n\\n## Contribution\\n\\nTo make contribution to the project a bit easier, all the MGnify `conditionals` and `subworkflows` under\\nthe `workflows/` directory that are not used in the metaGOflow framework, have been removed.   \\nHowever, all the MGnify `tools/` and `utils/` are available in this repo, even if they are not invoked in the current\\nversion of metaGOflow.\\nThis way, we hope we encourage people to implement their own `conditionals` and/or `subworkflows` by exploiting the\\ncurrently supported `tools` and `utils` as well as by developing new `tools` and/or `utils`.\\n\\n\\n<!-- cwltool --print-dot my-wf.cwl | dot -Tsvg > my-wf.svg -->\\n\"\n",
      " 'BackTrackBB is a program for detection and space-time location of seismic sources based on multi-scale, frequency-selective statistical coherence of the wave field recorded by dense large-scale seismic networks and local antennas. The method is designed to enhance coherence of the signal statistical features across the array of sensors and consists of three steps. They are signal processing, space-time imaging and detection and location.\\n\\nSource with inputs and outputs included (too big for WorkflowHub): [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7788030.svg)](https://doi.org/10.5281/zenodo.7788030)\\n\\nMore information: https://backtrackbb.github.io/'\n",
      " 'Generates Dose-response curve fits on cell-based toxicity data. Outliers of replicate data-sets can be removed by setting a threshold for standard deviation (here set to 25). Curve fits for compounds showing low response can be removed by setting a threshold for minimum activity (here set to 75% confluence).\\nThis workflow needs R-Server to run in the back-end. Start R and run the following command: library(Rserve); Rserve(args = \"--vanilla\")'\n",
      " 'This workflow can be used to fit dose-response curves from normalised cell-based assay data (%confluence) using the KNIME HCS extension. The workflow expects triplicates for each of eight test concentrations. This workflow needs R-Server to run in the back-end. Start R and run the following command: library(Rserve); Rserve(args = \"--vanilla\"). \\nThree types of outliers can be removed: 1 - Outliers from triplicate measurement (standard deviation cut-off can be selected), 2 - inactive and weekly active compounds (% confluence cut-offs can be selected), 3 - toxic concentrations (cut-off for reduction in confluence with stepwise increasing concentration can be selected)\\nOutput are two dose-response curve fits per compound for pre and post outlier removal with graphical representation and numerical fit parameters. \\n'\n",
      " 'This workflow can be used to fit dose-response curves from normalised biochemical assay data (%Inhibition) using the HCS extension. This workflow needs R-Server to run in the back-end. Start R and run the following command: library(Rserve); Rserve(args = \"--vanilla\")\\nIC50 values will not be extrapolated outside the tested concentration range\\nFor activity classification the following criteria are applied:\\n- maximum (average % inhibion) >25 % and slope is >0 and IC50 > 5 µM or\\n- minimum (average % inhibion) >75 %\\nResults are formatted for upload to the European Chemical Biology Database (ECBD)'\n",
      " '# CroMaSt: A workflow for assessing protein domain classification by cross-mapping of structural instances between domain databases and structural alignment\\n\\nCroMaSt (<span style=\"color:red\">**Cro**</span>ss <span style=\"color:red\">**Ma**</span>pper of domain <span style=\"color:red\">**St**</span>ructural instances) is an automated iterative workflow to clarify  the assignment of protein domains to a given domain type of interest, based on their 3D structure and by cross-mapping of domain structural instances between domain databases. CroMaSt (for Cross-Mapper of domain Structural instances) will classify all structural instances of a given domain type into 4 different categories (**Core**, **True**, **Domain-like**, and **Failed**). \\n\\n\\n## Requirements\\n1. [Conda](https://docs.conda.io/projects/conda/en/latest/) or [Miniconda](https://docs.conda.io/en/latest/miniconda.html)\\n2. [Kpax](http://kpax.loria.fr/download.php)  \\nDownload and install conda (or Miniconda) and Kpax by following the instructions from their official site.\\n\\n\\n## Get it running \\n(Considering the requirements are already met)\\n\\n1. Clone the repository and change the directory\\n\\n```\\ngit clone https://gitlab.inria.fr/capsid.public_codes/CroMaSt.git\\ncd CroMaSt\\n```\\n\\n2. Create the conda environment for the workflow\\n```\\nconda env create --file yml/environment.yml\\nconda activate CroMaSt\\n```\\n\\n3. Change the path of variables in paramter file\\n```\\nsed -i \\'s/\\\\/home\\\\/hdhondge\\\\/CroMaSt\\\\//\\\\/YOUR\\\\/PATH\\\\/TO_CroMaSt\\\\//g\\' yml/CroMaSt_input.yml \\n```\\n\\n4. Create the directory to store files from PDB and SIFTS (if not already)\\n```\\nmkdir PDB_files SIFTS\\n```\\n\\n5. Download the source input data\\n```\\ncwl-runner Tools/download_data.cwl yml/download_data.yml\\n```\\n\\n## Basic example\\n\\n### 1. First, we will run the workflow for the KH domain with family identifiers `RRM_1` and `RRM` in Pfam and CATH, respectively.\\nRun the workflow -\\n\\n```\\ncwl-runner --parallel  --outdir=Results/  CroMaSt.cwl yml/CroMaSt_input.yml\\n```\\n\\n### 2.  Once the iteration is complete, check the `new_param.yml` file from the `outputdir` (Results), if there is any family identifier in either `pfam` or `cath`; run the next iteration using following command (Until there is no new families explored by workflow) -\\n\\n```\\ncwl-runner --parallel  --outdir=Results/  CroMaSt.cwl Results/new_param.yml\\n```\\n  \\n### **Extra:** Start the workflow with multiple families from one or both databases  \\nIf you would like to start the workflow with multiple families from one or both databases, then simply add a comma in between two family identifiers. \\n```\\npfam: [\\'PF00076\\', \\'PF08777\\']\\ncath: [\\'3.30.70.330\\']\\n```\\n\\n- **Pro Tip**: Don\\'t forget to give different path to `--outdir` option while running the workflow multiple times or at least move the results to some other location after first run.\\n\\n## Run the workflow for protein domain of your choice  \\n### 1. You can run the workflow for the domain of your choice by simply changing the family identifers in `yml/CroMaSt_input.yml` file.\\n\\nSimply replace the following values of family identifiers (for pfam and cath) with the family identifiers of your choice in `yml/CroMaSt_input.yml` file. \\n```\\npfam: [\\'PF00076\\']\\ncath: [\\'3.30.70.330\\']\\n```\\n\\n\\n\\n## Data files used in current version are as follows:\\n**Files in Data directory can be downloaded as follows**:\\n\\n1. File used from Pfam database: [pdbmap.gz](http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam35.0/pdbmap.gz)\\n\\n2. File used from CATH database: [cath-domain-description-file.txt](ftp://orengoftp.biochem.ucl.ac.uk:21/cath/releases/latest-release/cath-classification-data/cath-domain-description-file.txt)  \\n\\n3. Obsolete entries from RCSB PDB\\n[obsolete_PDB_entry_ids.txt](https://data.rcsb.org/rest/v1/holdings/removed/entry_ids)  \\n\\n\\nCATH Version - 4.3.0 (Ver_Date - 11-Sep-2019) [FTP site](ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/cath-classification-data/)\\nPfam Version - 35.0 (Ver_Date - November-2021) [FTP site](http://ftp.ebi.ac.uk/pub/databases/Pfam/releases/Pfam35.0/)\\n\\n## Reference\\n```\\nPoster - \\n1. Hrishikesh Dhondge, Isaure Chauvot de Beauchêne, Marie-Dominique Devignes. CroMaSt: A workflow for domain family curation through cross-mapping of structural instances between protein domain databases. 21st European Conference on Computational Biology, Sep 2022, Sitges, Spain. ⟨hal-03789541⟩\\n\\n```\\n\\n## Acknowledgements\\nThis  project  has  received  funding  from  the  Marie  Skłodowska-Curie Innovative Training Network (MSCA-ITN) RNAct supported by European Union’s Horizon 2020 research and innovation programme under granta greement No 813239.\\n'\n",
      " '# IndexReferenceFasta-nf\\n===========\\n\\n  - [Description](#description)\\n  - [Diagram](#diagram)\\n  - [User guide](#user-guide)\\n  - [Benchmarking](#benchmarking)\\n  - [Workflow summaries](#workflow-summaries)\\n      - [Metadata](#metadata)\\n      - [Component tools](#component-tools)\\n      - [Required (minimum)\\n        inputs/parameters](#required-minimum-inputsparameters)\\n  - [Additional notes](#additional-notes)\\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\\n\\n---\\n\\n## Description\\nThis is a flexible pipeline for generating common reference genome index files for WGS data analysis. IndexReferenceFasta-nf is a Nextflow (DSL2) pipeline that runs the following tools using Singularity containers:\\n* Samtools faidx\\n* BWA index\\n* GATK CreateSequenceDictionary \\n\\n## Diagram\\n<p align=\"center\"> \\n<img src=\"https://user-images.githubusercontent.com/73086054/189310509-375fea4f-11fb-41ca-ba52-90760e9a5aa3.png\" width=\"80%\">\\n</p> \\n\\n## User guide\\n**1. Set up**\\n\\nClone this repository by running:\\n```\\ngit clone https://github.com/Sydney-Informatics-Hub/IndexReferenceFasta-nf.git\\ncd IndexReferenceFasta-nf\\n``` \\n\\n**2. Generate indexes**  \\n\\nUsers can specify which index files to create by using the `--samtools`, `--bwa`, and/or `--gatk` flags. All are optional. Run the pipeline with:\\n\\n```\\nnextflow run main.nf /path/to/ref.fasta --bwa --samtools --gatk \\n```\\n\\n## Benchmarking\\n\\n### Human hg38 reference assembly @ Pawsey\\'s Nimbus (NCPU/task = 1)\\n|task_id|hash     |native_id|name          |status   |exit|submit |duration  |realtime  |%cpu   |peak_rss|peak_vmem|rchar  |wchar  |\\n|-------|---------|---------|--------------|---------|----|-------|----------|----------|-------|--------|---------|-------|-------|\\n|3      |27/33fffc|131621   |samtools_index|COMPLETED|0   |55:44.9|12.2s     |12s       |99.20% |6.3 MB  |11.8 MB  |3 GB   |19.1 KB|\\n|1      |80/f03e46|131999   |gatk_index    |COMPLETED|0   |55:46.7|22.6s     |22.3s     |231.90%|3.8 GB  |37.1 GB  |3.1 GB |726 KB |\\n|2      |ea/e29535|131594   |bwa_index     |COMPLETED|0   |55:44.9|1h 50m 16s|1h 50m 15s|99.50% |4.5 GB  |4.5 GB   |12.1 GB|8.2 GB |\\n\\n## Workflow summaries\\n\\n### Metadata\\n|metadata field     | workflow_name / workflow_version  |\\n|-------------------|:---------------------------------:|\\n|Version            | workflow_version                  |\\n|Maturity           | under development                 |\\n|Creators           | Georgie Samaha                    |\\n|Source             | NA                                |\\n|License            | GPL-3.0 license                   |\\n|Workflow manager   | NextFlow                          |\\n|Container          | None                              |\\n|Install method     | Manual                            |\\n|GitHub             | Sydney-Informatics-Hub/IndexReferenceFasta-nf                                |\\n|bio.tools          | NA                                |\\n|BioContainers      | NA                                | \\n|bioconda           | NA                                |\\n\\n### Component tools\\n\\n* samtools/1.15.1\\n* gatk/4.2.6.1 \\n* bwa/0.7.17\\n\\n### Required (minimum) inputs/parameters\\n\\n* A reference genome file in fasta format.\\n\\n## Additional notes\\n\\n### Help/FAQ/Troubleshooting\\n\\n## Acknowledgements/citations/credits\\n### Authors \\n- Georgie Samaha (Sydney Informatics Hub, University of Sydney)   \\n\\n### Acknowledgements \\n\\n- This pipeline was built using the [Nextflow DSL2 template](https://github.com/Sydney-Informatics-Hub/Nextflow_DSL2_template).  \\n- Documentation was created following the [Australian BioCommons documentation guidelines](https://github.com/AustralianBioCommons/doc_guidelines).  \\n\\n### Cite us to support us! \\nAcknowledgements (and co-authorship, where appropriate) are an important way for us to demonstrate the value we bring to your research. Your research outcomes are vital for ongoing funding of the Sydney Informatics Hub and national compute facilities. We suggest including the following acknowledgement in any publications that follow from this work:  \\n\\nThe authors acknowledge the technical assistance provided by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia. \\n'\n",
      " \"# ChIP-seq paired-end Workflow\\n\\n## Inputs dataset\\n\\n- The workflow needs a single input which is a list of dataset pairs of fastqsanger.\\n\\n## Inputs values\\n\\n- adapters sequences: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera.\\n- reference_genome: this field will be adapted to the genomes available for bowtie2.\\n- effective_genome_size: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes).\\n\\n## Processing\\n\\n- The workflow will remove illumina adapters and low quality bases and filter out any pair with mate smaller than 15bp.\\n- The filtered reads are mapped with bowtie2 with default parameters.\\n- The BAM is filtered to keep only MAPQ30 and concordant pairs.\\n- The peaks are called with MACS2 which at the same time generates a coverage file.\\n- The coverage is converted to bigwig.\\n- A MultiQC is run to have an overview of the QC.\\n\\n### Warning\\n\\n- The coverage output is not normalized.\\n- The filtered bam still has PCR duplicates which are removed by MACS2.\\n\\n## Contribution\\n\\n@lldelisle wrote the workflow.\\n\\n@nagoue updated the tools, made it work in usegalaxy.org, fixed the best practices and wrote the tests.\\n\"\n",
      " 'This workflow take as input a collection of paired fastq. Remove adapters with cutadapt, map pairs with bowtie2 allowing dovetail. Keep MAPQ30 and concordant pairs. BAM to BED. MACS2 with \"ATAC\" parameters.'\n",
      " 'This workflow takes as input a collection of fastqs (single reads). Remove adapters with cutadapt, map with bowtie2. Keep MAPQ30. MACS2 for bam with fixed extension or model.'\n",
      " 'This workflow takes as input a collection of paired fastqs. Remove adapters with cutadapt, map pairs with bowtie2. Keep MAPQ30 and concordant pairs. MACS2 for paired bam.'\n",
      " \"This workflow take as input a collection of paired fastq. It will remove bad quality and adapters with cutadapt. Map with Bowtie2 end-to-end. Will remove reads on MT and unconcordant pairs and pairs with mapping quality below 30 and PCR duplicates. Will compute the pile-up on 5' +- 100bp. Will call peaks and count the number of reads falling in the 1kb region centered on the summit. Will plot the number of reads for each fragment length.\"\n",
      " 'This workflow takes as input a list of single-read fastqs. Adapters and bad quality bases are removed with cutadapt. Reads are mapped with STAR with ENCODE parameters and genes are counted simultaneously. The counts are reprocess to be similar to HTSeq-count output. FPKM are computed with cufflinks. Coverage (per million mapped reads) are computed with bedtools on uniquely mapped reads.'\n",
      " 'This workflow takes as input a list of paired-end fastqs. Adapters and bad quality bases are removed with cutadapt. Reads are mapped with STAR with ENCODE parameters and genes are counted simultaneously. The counts are reprocess to be similar to HTSeq-count output. FPKM are computed with cufflinks. Coverage (per million mapped reads) are computed with bedtools on uniquely mapped reads (with R2 orientation inverted).'\n",
      " 'This workflow is designed to analyze to a multi-omics data set that comprises genome-wide DNA methylation profiles, targeted metabolomics, and behavioral data of two cohorts that participated in the ACTION Biomarker Study (ACTION, Aggression in Children: Unraveling gene-environment interplay to inform Treatment and InterventiON strategies. (Boomsma 2015, Bartels 2018, Hagenbeek 2020, van Dongen 2021, Hagenbeek 2022). The ACTION-NTR cohort consists of twins that are either longitudinally concordant or discordant for childhood aggression. The ACTION-Curium-LUMC cohort consists of children referred to the Dutch LUMC Curium academic center for child and youth psychiatry. With the joint analysis of multi-omics data and behavioral data, we aim to identify substructures in the ACTION-NTR cohort and link them to aggressive behavior. First, the individuals are clustered using Similarity Network Fusion (SNF, Wang 2014), and latent feature dimensions are uncovered using different unsupervised methods including Multi-Omics Factor Analysis (MOFA) (Argelaguet 2018) and Multiple Correspondence Analysis (MCA, Lê 2008, Husson 2017). In a second step, we determine correlations between -omics and phenotype dimensions, and use them to explain the subgroups of individuals from the ACTION-NTR cohort. In order to validate the results, we project data of the ACTION-Curium-LUMC cohort onto the latent dimensions and determine if correlations between omics and phenotype data can be reproduced.'\n",
      " 'Post-genome assembly quality control workflow using Quast, BUSCO, Meryl, Merqury and Fasta Statistics. Updates November 2023.  \\n\\n* Inputs: reads as fastqsanger.gz (not fastq.gz), and assembly.fasta. (To change format: click on the pencil icon next to the file in the Galaxy history, then \"Datatypes\", then set \"New type\" as fastqsanger.gz).\\n* New default settings for BUSCO: lineage = eukaryota; for Quast: lineage = eukaryotes, genome = large.\\n* Reports assembly stats into a table called metrics.tsv, including selected metrics from Fasta Stats, and read coverage; reports BUSCO versions and dependencies; and displays these tables in the workflow report.\\n* Note: a known bug is that sometimes the workflow report text resets to default text.\\n* To restore: open the workflow in Galaxy for editing.\\n* Click on the \"Edit Report\" icon\\n* Copy and paste the following text into the workflow report, then exit and save.\\n\\n---\\n\\n\\\\# Workflow Execution Report\\n\\nWorkflow name: Genome assessment post assembly\\n\\n\\\\## Genome assembly metrics\\n\\nSelected statistics from the workflow outputs. Additional metrics are available in other outputs in the history.\\n\\n````\\n```galaxy\\nhistory_dataset_display(output=\"Genome assembly metrics\")\\n```\\n````\\n\\n\\\\## Software\\n\\nBusco version and dependencies:\\n\\n````\\n```galaxy\\nhistory_dataset_display(output=\"Busco and dependencies version\")\\n```\\n````\\n\\n\\\\## Galaxy Australia \\n\\nThanks for using Galaxy! When you use Galaxy Australia to support your publication or project, please acknowledge its use with the following statement: \"This work is supported by Galaxy Australia, a service provided by the Australian Biocommons and its partners. The service receives NCRIS funding through Bioplatforms Australia and the Australian Research Data Commons (https://doi.org/10.47486/PL105), as well as The University of Melbourne and Queensland Government RICF funding.\"\\n'\n",
      " 'This Galaxy-E workflow was made from the [\"Cleaning GBIF data for the use in biogeography\" tutorial](https://ropensci.github.io/CoordinateCleaner/articles/Cleaning_GBIF_data_with_CoordinateCleaner.html) and allows to:\\n- Use CoordinateCleaner to automatically flag problematic records\\n- Use GBIF provided meta-data to improve coordinate quality, tailored to your downstream analyses\\n- Use automated cleaning algorithms of CoordinateCleaner to identify problematic contributing datasets\\n- Visualize data on a map'\n",
      " 'Workflow for the GTN training \"Antibiotic resistance detection\"'\n",
      " 'With this galaxy pipeline you can use Salmonella sp. next generation sequencing results to predict bacterial AMR phenotypes and compare the results against gold standard Salmonella sp. phenotypes obtained from food.\\n\\nThis pipeline is based on the work of the National Food Agency of Canada.  \\nDoi: [10.3389/fmicb.2020.00549](https://doi.org/10.3389/fmicb.2020.00549)'\n",
      " '[![Development](https://img.shields.io/badge/development-active-blue.svg)](https://img.shields.io/badge/development-active-blue.svg)\\n[![Reads2Map](https://circleci.com/gh/Cristianetaniguti/Reads2Map.svg?style=svg)](https://app.circleci.com/pipelines/github/Cristianetaniguti/Reads2Map)\\n\\n## Reads2Map \\n\\nReads2Map presents a collection of [WDL workflows](https://openwdl.org/)  to build linkage maps from sequencing reads. Each workflow release is described in the [Read2Map releases page](https://github.com/Cristianetaniguti/Reads2Map/releases). \\n\\nThe main workflows are the `EmpiricalReads2Map.wdl` and the `SimulatedReads2Map.wdl`. The `EmpiricalReads2Map.wdl` is composed by the `EmpiricalSNPCalling.wdl` that performs the SNP calling, and the `EmpiricalMaps.wdl` that performs the genotype calling and map building in empirical reads. The `SimulatedReads2Map.wdl` simulates Illumina reads for RADseq, exome, or WGS data and performs the SNP and genotype calling and genetic map building.\\n\\nBy now, [GATK](https://github.com/broadinstitute/gatk), [Freebayes](https://github.com/ekg/freebayes) are included for SNP calling; [updog](https://github.com/dcgerard/updog), [polyRAD](https://github.com/lvclark/polyRAD), [SuperMASSA](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030906) for dosage calling; and [OneMap](https://github.com/augusto-garcia/onemap), and [GUSMap](https://github.com/tpbilton/GUSMap) for linkage map build.\\n\\n![math_meth2](https://user-images.githubusercontent.com/7572527/203172239-e4d2d857-84e2-48c5-bb88-01052a287004.png)\\n\\n## How to use\\n\\nMultiple systems are available to run WDL workflows such as Cromwell, miniWDL, and dxWDL. See further information in the [openwdl documentation](https://github.com/openwdl/wdl#execution-engines).\\n\\nTo run a pipeline, first navigate to [Reads2Map releases page](https://github.com/Cristianetaniguti/Reads2Map/releases), search for the pipeline tag you which to run, and download the pipeline’s assets (the WDL workflow, the JSON, and the ZIP with accompanying dependencies).\\n\\n## Documentation\\n\\nCheck the description of the inputs for the pipelines:\\n\\n* [EmpiricalReads2Map (EmpiricalSNPCalling and EmpiricalMaps)](https://cristianetaniguti.github.io/Tutorials/Reads2Map/EmpiricalReads.html)\\n\\n* [SimulatedReads2Map](https://cristianetaniguti.github.io/Tutorials/Reads2Map/simulatedreads.html)\\n\\nCheck how to evaluate the workflows results in Reads2MapApp Shiny:\\n\\n* [Reads2MapApp](https://github.com/Cristianetaniguti/Reads2MapApp)\\n\\nOnce you selected the best pipeline using a subset of your data, you can build a complete high-density linkage map:\\n\\n* [A Guide to Build High-Density Linkage Maps](https://cristianetaniguti.github.io/Tutorials/onemap/Quick_HighDens/High_density_maps.html)\\n\\nCheck more information and examples of usage in:\\n\\n* [Taniguti, C. H., Taniguti, L. M., Amadeu, R. R., Mollinari, M., Da, G., Pereira, S., Riera-Lizarazu, O., Lau, J., Byrne, D., de Siqueira Gesteira, G., De, T., Oliveira, P., Ferreira, G. C., &#38; Franco Garcia, A. A.  Developing best practices for genotyping-by-sequencing analysis using linkage maps as benchmarks. BioRxiv. https://doi.org/10.1101/2022.11.24.517847](https://www.biorxiv.org/content/10.1101/2022.11.24.517847v2)\\n\\n## Third-party software and images\\n\\n- [BWA](https://github.com/lh3/bwa) in [us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z](https://console.cloud.google.com/gcr/images/broad-gotc-prod/US/genomes-in-the-cloud): Used to align simulated reads to reference;\\n- [cutadapt](https://github.com/marcelm/cutadapt) in [cristaniguti/ pirs-ddrad-cutadapt:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/pirs-ddrad-cutadapt): Trim simulated reads;\\n- [ddRADseqTools](https://github.com/GGFHF/ddRADseqTools) in [cristaniguti/ pirs-ddrad-cutadapt:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/pirs-ddrad-cutadapt): Set of applications useful to in silico design and testing of double digest RADseq (ddRADseq) experiments;\\n- [Freebayes](https://github.com/ekg/freebayes) in [Cristaniguti/freebayes:0.0.1](): Variant call step;\\n- [GATK](https://github.com/broadinstitute/gatk) in [us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z](https://console.cloud.google.com/gcr/images/broad-gotc-prod/US/genomes-in-the-cloud): Variant call step using Haplotype Caller, GenomicsDBImport and GenotypeGVCFs;\\n- [PedigreeSim](https://github.com/PBR/pedigreeSim?files=1) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Simulates progeny genotypes from parents genotypes for different types of populations;\\n- [picard](https://github.com/broadinstitute/picard) in [us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z](https://console.cloud.google.com/gcr/images/broad-gotc-prod/US/genomes-in-the-cloud): Process alignment files;\\n- [pirs](https://github.com/galaxy001/pirs) in [cristaniguti/ pirs-ddrad-cutadapt:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/pirs-ddrad-cutadapt): To generate simulates paired-end reads from a reference genome;\\n- [samtools](https://github.com/samtools/samtools) in [us.gcr.io/broad-gotc-prod/genomes-in-the-cloud:2.5.7-2021-06-09_16-47-48Z](https://console.cloud.google.com/gcr/images/broad-gotc-prod/US/genomes-in-the-cloud): Process alignment files;\\n- [SimuSCoP](https://github.com/qasimyu/simuscop) in [cristaniguti/simuscopr:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/simuscopr): Exome and WGS Illumina reads simulations;\\n- [RADinitio](http://catchenlab.life.illinois.edu/radinitio/) in [\\tcristaniguti/radinitio:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/radinitio): RADseq Illumina reads simulation;\\n- [SuperMASSA](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0030906) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Efficient Exact Maximum a Posteriori Computation for Bayesian SNP Genotyping in Polyploids;\\n- [bcftools](https://github.com/samtools/bcftools) in [lifebitai/bcftools:1.10.2](https://hub.docker.com/r/lifebitai/bcftools): utilities for variant calling and manipulating VCFs and BCFs;\\n- [vcftools](http://vcftools.sourceforge.net/) in [cristaniguti/split_markers:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/split_markers): program package designed for working with VCF files.\\n- [MCHap](https://github.com/PlantandFoodResearch/MCHap) in [cristaniguti/mchap:0.7.0](https://hub.docker.com/repository/docker/cristaniguti/mchap): Polyploid micro-haplotype assembly using Markov chain Monte Carlo simulation.\\n\\n### R packages\\n\\n- [OneMap](https://github.com/augusto-garcia/onemap) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Is a software for constructing genetic maps in experimental crosses: full-sib, RILs, F2 and backcrosses;\\n- [Reads2MapTools](https://github.com/Cristianetaniguti/Reads2MapTools) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Support package to perform mapping populations simulations and genotyping for OneMap genetic map building\\n- [GUSMap](https://github.com/tpbilton/GUSMap): Genotyping Uncertainty with Sequencing data and linkage MAPping\\n- [updog](https://github.com/dcgerard/updog) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Flexible Genotyping of Polyploids using Next Generation Sequencing Data\\n- [polyRAD](https://github.com/lvclark/polyRAD) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Genotype Calling with Uncertainty from Sequencing Data in Polyploids\\n- [Reads2MapApp](https://github.com/Cristianetaniguti/Reads2MapApp) in [cristaniguti/reads2mapApp:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Shiny app to evaluate Reads2Map workflows results\\n- [simuscopR](https://github.com/Cristianetaniguti/simuscopR) in [cristaniguti/reads2map:0.0.1](https://hub.docker.com/repository/docker/cristaniguti/reads2map): Wrap-up R package for SimusCop simulations.'\n",
      " '# RNA-seq Scientific Workflow\\nWorkflow for RNA sequencing using the Parallel Scripting Library - Parsl.\\n\\n**Reference:** Cruz, L., Coelho, M., Terra, R., Carvalho, D., Gadelha, L., Osthoff, C., & Ocaña, K. (2021). *Workflows* Científicos de RNA-Seq em Ambientes Distribuídos de Alto Desempenho: Otimização de Desempenho e Análises de Dados de Expressão Diferencial de Genes. In *Anais do XV Brazilian e-Science Workshop*, p. 57-64. Porto Alegre: SBC. DOI: https://doi.org/10.5753/bresci.2021.15789\\n\\n## Requirements\\n\\nIn order to use RNA-seq Workflow the following tools must be available:\\n\\n- [Bowtie2](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\\n\\nYou can install Bowtie2 by running:\\n\\n> bowtie2-2.3.5.1-linux-x86_64.zip\\n\\nOr\\n\\n> sudo yum install bowtie2-2.3.5-linux-x86_64\\n\\n- [Samtools](http://www.htslib.org/)\\n\\nSamtools is a suite of programs for interacting with high-throughput sequencing data.\\n\\n- [Picard](https://github.com/broadinstitute/picard)\\n\\nPicard is a set of Java command line tools for manipulating high-throughput sequencing (HTS) data and formats.\\n\\n- [HTSeq](https://htseq.readthedocs.io/en/master/)\\n\\nHTSeq is a native Python library that folows conventions of many Python packages. You can install it by running:\\n\\n> pip install HTSeq\\n\\nHTSeq uses [NumPy](https://numpy.org/), [Pysam](https://github.com/pysam-developers/pysam) and [matplotlib](https://matplotlib.org/). Be sure this tools are installed.\\n\\n- [R](https://www.r-project.org/)\\n\\nTo use [DESEq2](https://bioconductor.org/packages/release/bioc/html/DESeq2.html) script make sure R language is also installed. You can install it by running:\\n\\n\\n> sudo apt install r-base\\n\\n- [Parsl - Parallel Scripting Library](https://parsl.readthedocs.io/en/stable/index.html)\\n\\nThe recommended way to install Parsl is the suggest approach from Parsl\\'s documentation:\\n\\n\\n> python3 -m pip install parsl\\n\\n- [Python (version >= 3.5)](https://www.python.org/)\\n\\nTo use Parsl, you need Python 3.5 or above. You also need Python to use HTSeq, so you should load only one Python version.\\n\\n## Workflow invocation\\n\\nFirst of all, make a Comma Separated Values (CSV) file. So, onto the first line type: ``sampleName,fileName,condition``. **Remember, there must be no spaces between items**. You can use the file *\"table.csv\"* in this repository as an example. Your CSV file will be like this:\\n\\n   |    sampleName    |     fileName     |condition|\\n   |------------------|------------------|---------|\\n   | tissue control 1 | SRR5445794.merge.count | control |\\n   | tissue control 2 | SRR5445795.merge.count | control |\\n   | tissue control 3 | SRR5445796.merge.count | control |\\n   | tissue wntup 1   | SRR5445797.merge.count | wntup   |\\n   | tissue wntup 2   | SRR5445798.merge.count | wntup   |\\n   | tissue wntup 3   | SRR5445799.merge.count | wntup   |\\n\\nThe list of command line arguments passed to Python script, beyond the script\\'s name, must be: \\n\\n 1. The indexed genome; \\n 2. The number of threads for bowtie task, sort task, number of splitted files for split_picard task and number of CPU running in htseq task; \\n 3. Path to read fastaq file, which is the path of the input files; \\n 4. Directory\\'s name where the output files must be placed;  \\n 5. GTF file;\\n 7. and, lastly the DESeq script. \\n \\nMake sure all the files necessary to run the workflow are in the same directory and the fastaq files in a dedicated folder, as a input directory. The command line will be like this:\\n\\n> python3 rna-seq.py ../mm9/mm9 24 ../inputs/ ../outputs ../Mus_musculus.NCBIM37.67.gtf ../DESeq.R\\n\\n**Remember to adjust the parameter multithreaded and multicore according with your computational environment.** \\nExample: If your machine has 8 cores, you should set the parameter on 8.\\n'\n",
      " 'RNAseq workflow UMG: Here we introduce a scientific workflow implementing several open-source software executed by Galaxy parallel scripting language in an high-performance computing environment. We have applied the workflow to a single-cardiomyocyte RNA-seq data retrieved from Gene Expression Omnibus database. The workflow allows for the analysis (alignment, QC, sort and count reads, statistics generation) of raw RNA-seq data and seamless integration of differential expression results into a configurable script code.\\n'\n",
      " 'Example workflow which allows the use of Mothra\\n\\nAccepts (e.g.) [these](https://github.com/machine-shop/mothra-data/tree/main/test_images) input files, bundled as a collection.'\n",
      " \"The Regulatory Mendelian Mutation (ReMM) score was created for relevance prediction of non-coding variations (SNVs and small InDels) in the human genome (GRCh37) in terms of Mendelian diseases. This project updates the ReMM score for the genome build GRCh38 and combines GRCh37 and GRCh38 into one workflow.\\n\\n## Pre-requirements\\n\\n### Conda\\nWe use Conda as software and dependency management tool. Conda installation guidelines can be found here:\\n\\nhttps://conda.io/projects/conda/en/latest/user-guide/install/index.html\\n\\n### Additional programs\\nThese programs are used during the workflow. They usually need to be compiled, however, the repository already contains the executables or generated files.\\n\\n- [AttributeDB](https://github.com/visze/attributedb)\\n- [Jannovar](https://github.com/charite/jannovar) \\n- [parSMURF](https://github.com/AnacletoLAB/parSMURF)\\n\\n### Snakemake\\n\\nThe workflow is managed by Snakemake - a workflow management system used to create reproducible and scalable data analyses. To install Snakemake as well as all other required packages, you need to create a working environment according to the description in the file env/ReMM.yaml. For that, first\\n\\nClone the repository\\n```\\ngit clone https://github.com/kircherlab/ReMM\\ncd ReMM\\n```\\n\\nCreate a working environment and activate it\\n\\n```\\nconda env create -n ReMM --file workflow/envs/ReMM.yaml\\nconda activate ReMM\\n```\\n\\nAll paths are relative to the Snakemake file so you do not need to change any path variables. Additionally, Snakemake creates all missing directories, so no need to create any aditional folders either.\\n\\n## Workflow\\n\\nThe workflow consists of four main parts:\\n\\n- Download of feature data\\n- Data processing and cleaning\\n- Model training and validation\\n- Calculation of ReMM for the whole genome\\n\\nThe `workflow` folder contains a graph of the workflow and more detailed information on the most important steps.\\n\\nTo launch a snakemake workflow, you need to tell snakemake which file you want to generate. We defined all rules for multiple steps. They can be found here: `workflow/Snakefile`. For example, you want to generate all feature sets defined in a config file you can run:\\n\\n```\\nsnakemake -c1 all_feature_sets\\n```\\n\\nTo execute any step separately (see `README.md` in the `workflow` folder for details on workflow steps), you need to look up the name of the desired output file in the scripts and call Snakemake with the exact name. Using a flag `-n`, you can initiate a 'dry run': Snakemake will check the consistency of all rules and files and show the number of steps. However, a clean dry run does not necessarily mean that no errors will occur during a normal run. ReMM score is not allele-specific so that you get only one score independent of the variant itself. The workflow from the download of data up to computing the scores may take several days or weeks depending on the computing power and internet connection.\\n\\n\\n### The config files\\n\\nThe main config file can be found in `config/config.yaml`. This config file was used to generate the ReMM score. Here most of the configuration magic happens. There is a second config file `config/features.yaml` where all features are listed (with additional description). Config files are controled via [json-schema](http://json-schema.org). \\n\\nWe also provide a slurm config file for runtimes, memory and number of threads per rule: `config/slurm.yaml`.\\n\"\n",
      " '\\n# Github: https://github.com/Lcornet/GENERA\\n\\n# BCCM GEN-ERA tools repository\\n\\nPlease visit the wiki for tutorials and access to the tools:\\nhttps://github.com/Lcornet/GENERA/wiki  \\n\\n# NEWS\\nMantis is now installed in a singularity container for the Metabolic workflow (install is no longer necessary).  \\n\\n# Information about the GEN-ERA project\\nPlease visit  \\nhttps://bccm.belspo.be/content/bccm-collections-genomic-era  \\n\\n# Publications\\n1. ToRQuEMaDA: tool for retrieving queried Eubacteria, metadata and dereplicating assemblies.  \\n   Léonard, R. R., Leleu, M., Vlierberghe, M. V., Cornet, L., Kerff, F., and Baurain, D. (2021).  \\n   PeerJ 9, e11348. doi:10.7717/peerj.11348.  \\n   https://peerj.com/articles/11348/  \\n2. The taxonomy of the Trichophyton rubrum complex: a phylogenomic approach.  \\n   Cornet, L., D’hooge, E., Magain, N., Stubbe, D., Packeu, A., Baurain, D., and Becker P. (2021).  \\n   Microbial Genomics 7, 000707. doi:10.1099/mgen.0.000707.  \\n   https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000707  \\n3. ORPER: A Workflow for Constrained SSU rRNA Phylogenies.  \\n   Cornet, L., Ahn, A.-C., Wilmotte, A., and Baurain, D. (2021).  \\n   Genes 12, 1741. doi:10.3390/genes12111741.  \\n   https://www.mdpi.com/2073-4425/12/11/1741/html  \\n4. AMAW: automated gene annotation for non-model eukaryotic genomes.  \\n   Meunier, L., Baurain, D., Cornet, L. (2021)  \\n   https://www.biorxiv.org/content/10.1101/2021.12.07.471566v1  \\n5. Phylogenomic analyses of Snodgrassella isolates from honeybees and bumblebees reveals taxonomic and functional diversity.  \\n   Cornet, L.,  Cleenwerck, I., Praet, J., Leonard, R., Vereecken, N.J., Michez, D., Smagghe, G., Baurain, D., Vandamme, P. (2021)  \\n   https://www.biorxiv.org/content/10.1101/2021.12.10.472130v1  \\n6. Contamination detection in genomic data: more is not enough.   \\n   Cornet, L & Baurain, D (2022)   \\n   Genome Biology. 2022;23:60.  \\n   https://genomebiology.biomedcentral.com/articles/10.1186/s13059-022-02619-9  \\n7. The GEN-ERA toolbox: unified and reproducible workflows for research in microbial genomics  \\n   Cornet, L., Durieu, B., Baert, F., D’hooge, E., Colignon, D., Meunier, L., Lupo, V., Cleenwerck I.,\\n   Daniel, HM., Rigouts, L., Sirjacobs, D., Declerck, D., Vandamme, P., Wilmotte, A., Baurain, D., Becker P (2022).  \\n   https://www.biorxiv.org/content/10.1101/2022.10.20.513017v1  \\n8. CRitical Assessment of genomic COntamination detection at several Taxonomic ranks (CRACOT)    \\n   Cornet, L., Lupo, V., Declerck, S., Baurain, D. (2022).   \\n   https://www.biorxiv.org/content/10.1101/2022.11.14.516442v1  \\n\\n# Copyright and License\\n\\nThis softwares is copyright (c) 2017-2021 by University of Liege / Sciensano / BCCM collection by Luc CORNET\\nThis is free softwares; you can redistribute it and/or modify.\\n\\n![BCCM](https://github.com/Lcornet/GENERA/blob/main/images/GENERA-logo.png)  \\n'\n",
      " '![CoVigator logo](images/CoVigator_logo_txt_nobg.png \"CoVigator logo\")\\n\\n# CoVigator pipeline: variant detection pipeline for Sars-CoV-2\\n\\n[![DOI](https://zenodo.org/badge/374669617.svg)](https://zenodo.org/badge/latestdoi/374669617)\\n[![Run tests](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline/actions/workflows/automated_tests.yml)\\n[![Powered by NumFOCUS](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\\n\\n\\n\\nThe Covigator pipeline processes SARS-CoV-2 FASTQ or FASTA files into annotated and normalized analysis ready VCF files.\\nIt also classifies samples into lineages using pangolin.\\nThe pipeline is implemented in the Nextflow framework (Di Tommaso, 2017), it is a stand-alone pipeline that can be\\nused independently of the CoVigator dashboard and knowledge base.\\n\\nAlthough it is configured by default for SARS-CoV-2 it can be employed for the analysis of other microbial organisms \\nif the required references are provided.\\n\\nThe result of the pipeline is one or more annotated VCFs with the list of SNVs and indels ready for analysis.\\n\\nThe results from the CoVigator pipeline populate our CoVigator dashboard [https://covigator.tron-mainz.de](https://covigator.tron-mainz.de) \\n\\n**Table of Contents**\\n\\n1. [Two pipelines in one](#id1)\\n2. [Implementation](#id2)\\n3. [How to run](#id3)\\n4. [Understanding the output](#id4)\\n6. [Annotation resources](#id5)\\n7. [Future work](#id6)\\n8. [Bibliography](#id7)\\n\\n\\n## Two pipelines in one\\n\\nIn CoVigator we analyse samples from two different formats, FASTQ files (e.g.: as provided by the European Nucleotide \\nArchive) and FASTA files containing a consensus assembly. While from the first we get the raw reads, \\nfrom the second we obtain already assembled genomes. Each of these formats has to be \\nanalysed differently. Also, the output data that we can obtain from each of these is different.\\n\\n![CoVigator pipeline](images/pipeline.drawio.png)\\n\\n### Pipeline for FASTQ files\\n\\nWhen FASTQ files are provided the pipeline includes the following steps:\\n- **Trimming**. `fastp` is used to trim reads with default values. This step also includes QC filtering.\\n- **Alignment**. `BWA mem 2` is used for the alignment of single or paired end samples.\\n- **BAM preprocessing**. BAM files are prepared and duplicate reads are marked using GATK and Sambamba tools.\\n- **Primer trimming**. When a BED with primers is provided, these are trimmed from the reads using iVar. This is applicable to the results from all variant callers.\\n- **Coverage analysis**. `samtools coverage` and `samtools depth` are used to compute the horizontal and vertical \\n  coverage respectively.\\n- **Variant calling**. Four different variant callers are employed: BCFtools, LoFreq, iVar and GATK. \\n  Subsequent processing of resulting VCF files is independent for each caller.\\n- **Variant normalization**. `bcftools norm` is employed to left align indels, trim variant calls and remove variant duplicates.\\n- **Technical annotation**. `VAFator` is employed to add VAF and coverage annotations from the reads pileup.\\n- **Phasing**. Clonal mutations (ie: VAF >= 0.8) occurring in the same amino acid are merged for its correct functional annotation.\\n- **Biological annotation**. `SnpEff` is employed to annotate the variant consequences of variants and\\n  `bcftools annotate` is employed to add additional SARS-CoV-2 annotations.\\n- **Lineage determination**. `pangolin` is used for this purpose, this runs over the results from each of the variant callers separately.\\n\\nBoth single end and paired end FASTQ files are supported.\\n\\n### Pipeline for FASTA files\\n\\nWhen a FASTA file is provided with a single assembly sequence the pipeline includes the following steps:\\n- **Variant calling**. A Smith-Waterman global alignment is performed against the reference sequence to call SNVs and \\n  indels. Indels longer than 50 bp and at the beginning or end of the assembly sequence are excluded. Any mutation where\\n  either reference or assembly contain an N is excluded.\\n- **Variant normalization**. Same as described above.\\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\\n- **Biological annotation**. Same as described above.\\n- **Lineage determination**. `pangolin` is used for this purpose.\\n\\nThe FASTA file is expected to contain a single assembly sequence. \\nBear in mind that only clonal variants can be called on the assembly.\\n\\n### Pipeline for VCF files\\n\\nWhen a VCF file is provided the pipeline includes the following steps:\\n- **Variant normalization**. Same as described above.\\n- **Technical annotation**. Same as described above (optional if BAM is provided)\\n- **Phasing**. mutations occurring in the same amino acid are merged for its correct annotation.\\n- **Biological annotation**. Same as described above\\n- **Lineage determination**. `pangolin` is used for this purpose.\\n\\n## Implementation\\n\\nThe pipeline is implemented as a Nextflow workflow with its DSL2 syntax.\\nThe dependencies are managed through a conda environment to ensure version traceability and reproducibility.\\nThe references for SARS-CoV-2 are embedded in the pipeline.\\nThe pipeline is based on a number of third-party tools, plus a custom implementation based on biopython (Cock, 2009) \\nfor the alignment and subsequent variant calling over a FASTA file.\\n\\nAll code is open sourced in GitHub [https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline](https://github.com/TRON-Bioinformatics/covigator-ngs-pipeline)\\nand made available under the MIT license. We welcome any contribution. \\nIf you have troubles using the CoVigator pipeline or you find an issue, we will be thankful if you would report a ticket \\nin GitHub.\\n\\nThe alignment, BAM preprocessing and variant normalization pipelines are based on the implementations in additional \\nNextflow pipelines within the TronFlow initiative [https://tronflow-docs.readthedocs.io/](https://tronflow-docs.readthedocs.io/). \\n\\n\\n### Variant annotations\\n\\nThe variants derived from a FASTQ file are annotated on the `FILTER` column using the VAFator \\n(https://github.com/TRON-Bioinformatics/vafator) variant allele frequency \\n(VAF) into `LOW_FREQUENCY`, `SUBCLONAL`, `LOW_QUALITY_CLONAL` and finally `PASS` variants correspond to clonal variants. \\nBy default, variants with a VAF < 2 % are considered `LOW_FREQUENCY`, variants with a VAF >= 2 % and < 50 % are \\nconsidered `SUBCLONAL` and variants with a VAF >= 50 % and < 80 % are considered `LOW_QUALITY_CLONAL`. \\nThis thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold` respectively.\\n\\nVAFator technical annotations:\\n\\n- `INFO/vafator_af`: variant allele frequency of the mutation \\n- `INFO/vafator_ac`: number of reads supporting the mutation \\n- `INFO/vafator_dp`: total number of reads at the position, in the case of indels this represents the number of reads in the previous position\\n\\nSnpEff provides the functional annotations. And all mutations are additionally annotated with the following SARS-CoV-2 specific annotations:\\n- ConsHMM conservation scores as reported in (Kwon, 2021)\\n- Pfam domains as reported in Ensemble annotations.\\n\\nBiological annotations: \\n\\n- `INFO/ANN` are the SnpEff consequence annotations (eg: overlapping gene, effect of the mutation). \\nThis are described in detail here [http://pcingola.github.io/SnpEff/se_inputoutput/](http://pcingola.github.io/SnpEff/se_inputoutput/) \\n- `INFO/CONS_HMM_SARS_COV_2` is the ConsHMM conservation score in SARS-CoV-2\\n- `INFO/CONS_HMM_SARBECOVIRUS` is the ConsHMM conservation score among Sarbecovirus\\n- `INFO/CONS_HMM_VERTEBRATE_COV` is the ConsHMM conservation score among vertebrate Corona virus\\n- `INFO/PFAM_NAME` is the Interpro name for the overlapping Pfam domains\\n- `INFO/PFAM_DESCRIPTION` is the Interpro description for the overlapping Pfam domains\\n- `INFO/problematic` contains the filter provided in DeMaio et al. (2020) for problematic mutations\\n\\nAccording to DeMaio et al. (2020), mutations at the beginning (ie: POS <= 50) and end (ie: POS >= 29,804) of the \\ngenome are filtered out\\n\\nThis is an example of biological annotations of a missense mutation in the spike protein on the N-terminal subunit 1 domain.\\n```\\nANN=A|missense_variant|MODERATE|S|gene-GU280_gp02|transcript|TRANSCRIPT_gene-GU280_gp02|protein_coding|1/1|c.118G>A|\\np.D40N|118/3822|118/3822|40/1273||;CONS_HMM_SARS_COV_2=0.57215;CONS_HMM_SARBECOVIRUS=0.57215;CONS_HMM_VERTEBRATE_COV=0;\\nPFAM_NAME=bCoV_S1_N;PFAM_DESCRIPTION=Betacoronavirus-like spike glycoprotein S1, N-terminal\\n```\\n\\n\\n### Phasing limitations\\n\\nThe phasing implementation is applicable only to clonal mutations. It assumes all clonal mutations are in phase and \\nhence it merges those occurring in the same amino acid.\\nIn order to phase intrahost mutations we would need to implement a read-backed phasing approach such as in WhatsHap \\nor GATK\\'s ReadBackedPhasing. Unfortunately these tools do not support the scenario of a haploid organism with an\\nundefined number of subclones.\\nFor this reason, phasing is implemented with custom Python code at `bin/phasing.py`.\\n\\n### Primers trimming\\n\\nWith some library preparation protocols such as ARTIC it is recommended to trim the primers from the reads.\\nWe have observed that if primers are not trimmed spurious mutations are being called specially SNVs with lower frequencies and long deletions.\\nAlso the variant allele frequencies of clonal mutations are underestimated.\\n\\nThe BED files containing the primers for each ARTIC version can be found at https://github.com/artic-network/artic-ncov2019/tree/master/primer_schemes/nCoV-2019.\\n\\nIf the adequate BED file is provided to the CoVigator pipeline with `--primers` the primers will be trimmed with iVar. \\nThis affects the output of every variant caller, not only iVar.\\n\\n### Reference data\\n\\nThe default SARS-CoV-2 reference files correspond to Sars_cov_2.ASM985889v3 and were downloaded from Ensembl servers.\\nNo additional parameter needs to be provided to use the default SARS-CoV-2 reference genome.\\n\\n#### Using a custom reference genome\\n\\nThese references can be customised to use a different SARS-CoV-2 reference or to analyse a different virus.\\nTwo files need to be provided:\\n- Use a custom reference genome by providing the parameter `--reference your.fasta`.\\n- Gene annotation file in GFFv3 format `--gff your.gff`. This is only required to run iVar\\n\\nAdditionally, the FASTA needs bwa indexes, .fai index and a .dict index.\\nThese indexes can be generated with the following two commands:\\n```\\nbwa index reference.fasta\\nsamtools faidx reference.fasta\\ngatk CreateSequenceDictionary --REFERENCE your.fasta\\n```\\n\\n**NOTE**: beware that for Nextflow to find these indices the reference needs to be passed as an absolute path.\\n\\nThe SARS-CoV-2 specific annotations will be skipped when using a custom genome.\\n\\nIn order to have SnpEff functional annotations available you will also need to provide three parameters:\\n- `--snpeff_organism`: organism to annotate with SnpEff (ie: as registered in SnpEff)\\n- `--snpeff_data`: path to the SnpEff data folder\\n- `--snpeff_config`: path to the SnpEff config file\\n\\n### Intrahost mutations\\n\\nSome mutations may be observed in a subset of the virus sample, this may arise through intrahost virus evolution or\\nco-infection. Intrahost mutations can only be detected when analysing the raw reads (ie: the FASTQs) \\nas in the assembly (ie: the FASTA file) a single virus consensus sequence is represented. \\nBCFtools and GATK do not normally capture intrahost mutations; on the other hand LoFreq and iVar both capture\\nmutations that deviate from a clonal-like VAF. \\nNevertheless, mutations with lower variant allele frequency (VAF) are challenging to distinguish from sequencing and\\nanalytical errors.  \\n\\nMutations are annotated on the `FILTER` column using the VAF into three categories: \\n- `LOW_FREQUENCY`: subset of intrahost mutations with lowest frequencies, potentially enriched with false positive calls (VAF < 2 %).\\n- `SUBCLONAL`: subset of intrahost mutations with higher frequencies (2 % <= VAF < 50 %).\\n- `LOW_QUALITY_CLONAL`: subset of clonal mutations with lower frequencies (50 % <= VAF < 80 %).\\n- `PASS` clonal mutations (VAF >= 80 %)\\n\\nOther low quality mutations are removed from the output.\\n\\nThe VAF thresholds can be changed with the parameters `--low_frequency_variant_threshold`,\\n`--subclonal_variant_threshold` and `--low_quality_clonal_variant_threshold`.\\n\\n## How to run\\n\\n### Requirements\\n\\n- Nextflow >= 19.10.0\\n- Java >= 8\\n- Conda >=4.9\\n\\n### Testing\\n\\nTo run the workflow on a test assembly dataset run:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fasta\\n```\\n\\nFind the output in the folder `covigator_test_fasta`.\\n\\nTo run the workflow on a test raw reads dataset run:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda,test_fastq\\n```\\n\\nFind the output in the folder `covigator_test_fastq`.\\n\\nThe above commands are useful to create the conda environments beforehand.\\n\\n**NOTE**: pangolin is the most time-consuming step of the whole pipeline. To make it faster, locate the conda \\nenvironment that Nextflow created with pangolin (eg: `find $YOUR_NEXTFOW_CONDA_ENVS_FOLDER -name pangolin`) and run\\n`pangolin --decompress-model`.\\n\\n### Running\\n\\nFor paired end reads:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\\\\n[-r v0.10.0] \\\\\\n[-profile conda] \\\\\\n--fastq1 <FASTQ_FILE> \\\\\\n--fastq2 <FASTQ_FILE> \\\\\\n--name example_run \\\\\\n--output <OUTPUT_FOLDER> \\\\\\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\n\\nFor single end reads:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\\\\n[-r v0.10.0] \\\\\\n[-profile conda] \\\\\\n--fastq1 <FASTQ_FILE> \\\\\\n--name example_run \\\\\\n--output <OUTPUT_FOLDER> \\\\\\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\n\\nFor assembly:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\\\\n[-r v0.10.0] \\\\\\n[-profile conda] \\\\\\n--fasta <FASTA_FILE> \\\\\\n--name example_run \\\\\\n--output <OUTPUT_FOLDER> \\\\\\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\n\\nFor VCF:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\\\\n[-r v0.10.0] \\\\\\n[-profile conda] \\\\\\n--vcf <VCF_FILE> \\\\\\n--name example_run \\\\\\n--output <OUTPUT_FOLDER> \\\\\\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\n\\nAs an optional input when processing directly VCF files you can provide BAM files to annotate VAFs:\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline \\\\\\n[-r v0.10.0] \\\\\\n[-profile conda] \\\\\\n--vcf <VCF_FILE> \\\\\\n--bam <BAM_FILE> \\\\\\n--bai <BAI_FILE> \\\\\\n--name example_run \\\\\\n--output <OUTPUT_FOLDER> \\\\\\n[--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n[--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\n\\nFor batch processing of reads use `--input_fastqs_list` and `--name`.\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastqs_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\nwhere the TSV file contains two or three columns tab-separated columns **without header**. Columns: sample name, path to FASTQ 1 and optionally path to FASTQ 2. \\n\\n| Sample    | FASTQ 1                       | FASTQ 2 (optional column)     |\\n|-----------|-------------------------------|-------------------------------|\\n| sample1   | /path/to/sample1_fastq1.fastq | /path/to/sample1_fastq2.fastq |\\n| sample2   | /path/to/sample2_fastq1.fastq | /path/to/sample2_fastq2.fastq |\\n| ...       | ...                           | ...                           |\\n\\n\\nFor batch processing of assemblies use `--input_fastas_list`.\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_fastas_list <TSV_FILE> --library <paired|single> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to FASTA.\\n\\n| Sample    | FASTA                  | \\n|-----------|------------------------|\\n| sample1   | /path/to/sample1.fasta |\\n| sample2   | /path/to/sample2.fasta |\\n| ...       | ...                    |\\n\\nFor batch processing of VCFs use `--input_vcfs_list`.\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] --input_vcfs_list <TSV_FILE> --output <OUTPUT_FOLDER> [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\nwhere the TSV file contains two columns tab-separated columns **without header**. Columns: sample name and path to VCF.\\n\\n| Sample    | FASTA                  |\\n|-----------|------------------------|\\n| sample1   | /path/to/sample1.vcf |\\n| sample2   | /path/to/sample2.vcf |\\n| ...       | ...                    |\\n\\nOptionally, provide BAM files for batch processing of VCFs using `--input_bams_list`.\\n```\\nnextflow run tron-bioinformatics/covigator-ngs-pipeline [-profile conda] \\\\\\n  --input_vcfs_list <TSV_FILE> \\\\\\n  --input_bams_list <TSV_FILE> \\\\\\n  --output <OUTPUT_FOLDER> \\\\\\n  [--reference <path_to_reference>/Sars_cov_2.ASM985889v3.fa] \\\\\\n  [--gff <path_to_reference>/Sars_cov_2.ASM985889v3.gff3]\\n```\\nwhere the BAMs TSV file contains three columns tab-separated columns **without header**. Columns: sample name, \\npath to BAM and path to BAI.\\n\\n| Sample    | BAM                  | BAI                  |\\n|-----------|----------------------|----------------------|\\n| sample1   | /path/to/sample1.bam | /path/to/sample1.bai |\\n| sample2   | /path/to/sample2.bam | /path/to/sample2.bai |\\n| ...       | ...                  | ...                  |\\n\\n\\n\\n### Getting help\\n\\nYou can always contact us directly or create a GitHub issue, otherwise see all available options using `--help`:\\n```\\n$ nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\\n\\nUsage:\\n    nextflow run tron-bioinformatics/covigator-ngs-pipeline -profile conda --help\\n\\nInput:\\n    * --fastq1: the first input FASTQ file (not compatible with --fasta, nor --vcf)\\n    * --fasta: the FASTA file containing the assembly sequence (not compatible with --fastq1, nor --vcf)\\n    * --vcf: the VCF file containing mutations to analyze (not compatible with --fastq1, nor --fasta)\\n    * --bam: the BAM file containing reads to annotate VAFs on a VCF (not compatible with --fastq1, nor --fasta)\\n    * --bai: the BAI index for a BAM file (not compatible with --fastq1, nor --fasta)\\n    * --name: the sample name, output files will be named after this name\\n    * --output: the folder where to publish output\\n    * --input_fastqs_list: alternative to --name and --fastq1 for batch processing\\n    * --library: required only when using --input_fastqs\\n    * --input_fastas_list: alternative to --name and --fasta for batch processing\\n    * --input_vcfs_list: alternative to --name and --vcf for batch processing\\n    * --input_bams_list: alternative to --name, --vcf, --bam and --bai for batch processing\\n\\nOptional input only required to use a custom reference:\\n    * --reference: the reference genome FASTA file, *.fai, *.dict and bwa indexes are required.\\n    * --gff: the GFFv3 gene annotations file (required to run iVar and to phase mutations from all variant callers)    \\n    * --snpeff_data: path to the SnpEff data folder, it will be useful to use the pipeline on other virus than SARS-CoV-2\\n    * --snpeff_config: path to the SnpEff config file, it will be useful to use the pipeline on other virus than SARS-CoV-2\\n    * --snpeff_organism: organism to annotate with SnpEff, it will be useful to use the pipeline on other virus than SARS-CoV-2\\n\\nOptional input:\\n    * --fastq2: the second input FASTQ file\\n    * --primers: a BED file containing the primers used during library preparation. If provided primers are trimmed from the reads.\\n    * --min_base_quality: minimum base call quality to take a base into account for variant calling (default: 20)\\n    * --min_mapping_quality: minimum mapping quality to take a read into account for variant calling (default: 20)\\n    * --vafator_min_base_quality: minimum base call quality to take a base into account for VAF annotation (default: 0)\\n    * --vafator_min_mapping_quality: minimum mapping quality to take a read into account for VAF annotation (default: 0)\\n    * --low_frequency_variant_threshold: VAF threshold to mark a variant as low frequency (default: 0.02)\\n    * --subclonal_variant_threshold: VAF superior threshold to mark a variant as subclonal  (default: 0.5)\\n    * --lq_clonal_variant_threshold: VAF superior threshold to mark a variant as loq quality clonal (default: 0.8)\\n    * --memory: the ammount of memory used by each job (default: 3g)\\n    * --cpus: the number of CPUs used by each job (default: 1)\\n    * --skip_lofreq: skips calling variants with LoFreq\\n    * --skip_gatk: skips calling variants with GATK\\n    * --skip_bcftools: skips calling variants with BCFTools\\n    * --skip_ivar: skips calling variants with iVar\\n    * --skip_pangolin: skips lineage determination with pangolin\\n    * --match_score: global alignment match score, only applicable for assemblies (default: 2)\\n    * --mismatch_score: global alignment mismatch score, only applicable for assemblies (default: -1)\\n    * --open_gap_score: global alignment open gap score, only applicable for assemblies (default: -3)\\n    * --extend_gap_score: global alignment extend gap score, only applicable for assemblies (default: -0.1)\\n    * --skip_sarscov2_annotations: skip some of the SARS-CoV-2 specific annotations (default: false)\\n    * --keep_intermediate: keep intermediate files (ie: BAM files and intermediate VCF files)\\n    * --args_bcftools_mpileup: additional arguments for bcftools mpileup command (eg: --args_bcftools_mpileup=\\'--ignore-overlaps\\')\\n    * --args_bcftools_call: additional arguments for bcftools call command (eg: --args_bcftools_call=\\'--something\\')\\n    * --args_lofreq: additional arguments for lofreq command (eg: --args_lofreq=\\'--something\\')\\n    * --args_gatk: additional arguments for gatk command (eg: --args_gatk=\\'--something\\')\\n    * --args_ivar_samtools: additional arguments for ivar samtools mpileup command (eg: --args_ivar_samtools=\\'--ignore-overlaps\\')\\n    * --args_ivar: additional arguments for ivar command (eg: --args_ivar=\\'--something\\')\\n\\nOutput:\\n    * Output a VCF file for each of BCFtools, GATK, LoFreq and iVar when FASTQ files are\\n    provided or a single VCF obtained from a global alignment when a FASTA file is provided.\\n    * A pangolin results file for each of the VCF files.\\n    * Only when FASTQs are provided:\\n      * FASTP statistics\\n      * Depth and breadth of coverage analysis results\\n      \\n```\\n\\n## Understanding the output\\n\\nAlthough the VCFs are normalized for both pipelines, the FASTQ pipeline runs four variant callers, while the FASTA\\npipeline runs a single variant caller. Also, there are several metrics in the FASTQ pipeline that are not present\\nin the output of the FASTA pipeline. Here we will describe these outputs.\\n\\n### FASTQ pipeline output\\n\\nFind in the table below a description of each of the expected files and a link to a sample file for the FASTQ pipeline.\\nThe VCF files will be described in more detail later.\\n\\n| Name                            | Description                                                    | Sample file                                                                                                                                       |\\n|---------------------------------|----------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------|\\n| $NAME.fastp_stats.json          | Output metrics of the fastp trimming process in JSON format    | [ERR4145453.fastp_stats.json](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.json)                                         |\\n| $NAME.fastp_stats.html          | Output metrics of the fastp trimming process in HTML format    | [ERR4145453.fastp_stats.html](_static/covigator_pipeline_sample_output_reads/ERR4145453.fastp_stats.html)                                         |\\n| $NAME.deduplication_metrics.txt | Deduplication metrics                                          | [ERR4145453.deduplication_metrics.txt](_static/covigator_pipeline_sample_output_reads/ERR4145453.deduplication_metrics.txt)                       |\\n| $NAME.coverage.tsv              | Coverage metrics (eg: mean depth, % horizontal coverage)       | [ERR4145453.coverage.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.coverage.tsv)                                                 |\\n| $NAME.depth.tsv                 | Depth of coverage per position                                 | [ERR4145453.depth.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.depth.tsv)                                                       |\\n| $NAME.bcftools.vcf.gz           | Bgzipped, tabix-indexed and annotated output VCF from BCFtools | [ERR4145453.bcftools.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.bcftools.normalized.annotated.vcf.gz) |\\n| $NAME.gatk.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from GATK     | [ERR4145453.gatk.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.gatk.normalized.annotated.vcf.gz)         |\\n| $NAME.lofreq.vcf.gz             | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.lofreq.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.normalized.annotated.vcf.gz)     |\\n| $NAME.ivar.vcf.gz               | Bgzipped, tabix-indexed and annotated output VCF from LoFreq   | [ERR4145453.ivar.tsv](_static/covigator_pipeline_sample_output_reads/ERR4145453.ivar.tsv)                                                         |\\n| $NAME.lofreq.pangolin.csv       | Pangolin CSV output file derived from LoFreq mutations         | [ERR4145453.lofreq.pangolin.csv](_static/covigator_pipeline_sample_output_reads/ERR4145453.lofreq.pangolin.csv)                                              |\\n\\n\\n### FASTA pipeline output\\n\\nThe FASTA pipeline returns a single VCF file. The VCF files will be described in more detail later.\\n\\n| Name                        | Description                                                  | Sample file                                                                                          |\\n|-----------------------------|--------------------------------------------------------------|------------------------------------------------------------------------------------------------------|\\n| $NAME.assembly.vcf.gz | Bgzipped, tabix-indexed and annotated output VCF | [ERR4145453.assembly.normalized.annotated.vcf.gz](_static/covigator_pipeline_sample_output_assembly/hCoV-19_NTXX.assembly.normalized.annotated.vcf.gz) |\\n\\n\\n## Annotations resources\\n\\nSARS-CoV-2 ASM985889v3 references were downloaded from Ensembl on 6th of October 2020:\\n- ftp://ftp.ensemblgenomes.org/pub/viruses/fasta/sars_cov_2/dna/Sars_cov_2.ASM985889v3.dna.toplevel.fa.gz\\n- ftp://ftp.ensemblgenomes.org/pub/viruses/gff3/sars_cov_2/Sars_cov_2.ASM985889v3.101.gff3.gz\\n\\nConsHMM mutation depletion scores downloaded on 1st of July 2021:\\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionConsHMM.bed\\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionSarbecovirusConsHMM.bed\\n- https://github.com/ernstlab/ConsHMM_CoV/blob/master/wuhCor1.mutDepletionVertebrateCoVConsHMM.bed\\n\\nGene annotations including Pfam domains downloaded from Ensembl on 25th of February 2021 from:\\n- ftp://ftp.ensemblgenomes.org/pub/viruses/json/sars_cov_2/sars_cov_2.json\\n\\n\\n## Future work\\n\\n- Primer trimming on an arbitrary sequencing library.\\n- Pipeline for Oxford Nanopore technology.\\n- Variant calls from assemblies contain an abnormally high number of deletions of size greater than 3 bp. This\\nis a technical artifact that would need to be avoided.\\n\\n## Bibliography\\n\\n- Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316–319. https://doi.org/10.1038/nbt.3820\\n- Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\\n- Adrian Tan, Gonçalo R. Abecasis and Hyun Min Kang. Unified Representation of Genetic Variants. Bioinformatics (2015) 31(13): 2202-2204](http://bioinformatics.oxfordjournals.org/content/31/13/2202) and uses bcftools [Li, H. (2011). A statistical framework for SNP calling, mutation discovery, association mapping and population genetical parameter estimation from sequencing data. Bioinformatics (Oxford, England), 27(21), 2987–2993. 10.1093/bioinformatics/btr509\\n- Danecek P, Bonfield JK, Liddle J, Marshall J, Ohan V, Pollard MO, Whitwham A, Keane T, McCarthy SA, Davies RM, Li H. Twelve years of SAMtools and BCFtools. Gigascience. 2021 Feb 16;10(2):giab008. doi: 10.1093/gigascience/giab008. PMID: 33590861; PMCID: PMC7931819.\\n- Van der Auwera GA, Carneiro M, Hartl C, Poplin R, del Angel G, Levy-Moonshine A, Jordan T, Shakir K, Roazen D, Thibault J, Banks E, Garimella K, Altshuler D, Gabriel S, DePristo M. (2013). From FastQ Data to High-Confidence Variant Calls: The Genome Analysis Toolkit Best Practices Pipeline. Curr Protoc Bioinformatics, 43:11.10.1-11.10.33. DOI: 10.1002/0471250953.bi1110s43.\\n- Martin, M., Patterson, M., Garg, S., O Fischer, S., Pisanti, N., Klau, G., Schöenhuth, A., & Marschall, T. (2016). WhatsHap: fast and accurate read-based phasing. BioRxiv, 085050. https://doi.org/10.1101/085050\\n- Danecek, P., & McCarthy, S. A. (2017). BCFtools/csq: haplotype-aware variant consequences. Bioinformatics, 33(13), 2037–2039. https://doi.org/10.1093/bioinformatics/btx100\\n- Wilm, A., Aw, P. P. K., Bertrand, D., Yeo, G. H. T., Ong, S. H., Wong, C. H., Khor, C. C., Petric, R., Hibberd, M. L., & Nagarajan, N. (2012). LoFreq: A sequence-quality aware, ultra-sensitive variant caller for uncovering cell-population heterogeneity from high-throughput sequencing datasets. Nucleic Acids Research, 40(22), 11189–11201. https://doi.org/10.1093/nar/gks918\\n- Grubaugh, N. D., Gangavarapu, K., Quick, J., Matteson, N. L., De Jesus, J. G., Main, B. J., Tan, A. L., Paul, L. M., Brackney, D. E., Grewal, S., Gurfield, N., Van Rompay, K. K. A., Isern, S., Michael, S. F., Coffey, L. L., Loman, N. J., & Andersen, K. G. (2019). An amplicon-based sequencing framework for accurately measuring intrahost virus diversity using PrimalSeq and iVar. Genome Biology, 20(1), 8. https://doi.org/10.1186/s13059-018-1618-7\\n- Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884–i890, https://doi.org/10.1093/bioinformatics/bty560\\n- Kwon, S. Bin, & Ernst, J. (2021). Single-nucleotide conservation state annotation of the SARS-CoV-2 genome. Communications Biology, 4(1), 1–11. https://doi.org/10.1038/s42003-021-02231-w\\n- Cock, P. J., Antao, T., Chang, J. T., Chapman, B. A., Cox, C. J., Dalke, A., et al. (2009). Biopython: freely available Python tools for computational molecular biology and bioinformatics. Bioinformatics, 25(11), 1422–1423.\\n- Artem Tarasov, Albert J. Vilella, Edwin Cuppen, Isaac J. Nijman, Pjotr Prins, Sambamba: fast processing of NGS alignment formats, Bioinformatics, Volume 31, Issue 12, 15 June 2015, Pages 2032–2034, https://doi.org/10.1093/bioinformatics/btv098\\n'\n",
      " '# TronFlow alignment pipeline\\n\\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/release/tron-bioinformatics/tronflow-bwa?sort=semver)\\n[![Run tests](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml/badge.svg?branch=master)](https://github.com/TRON-Bioinformatics/tronflow-bwa/actions/workflows/automated_tests.yml)\\n[![DOI](https://zenodo.org/badge/327943420.svg)](https://zenodo.org/badge/latestdoi/327943420)\\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\\n[![Powered by Nextflow](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\\n\\nThe TronFlow alignment pipeline is part of a collection of computational workflows for tumor-normal pair \\nsomatic variant calling.\\n\\nFind the documentation here [![Documentation Status](https://readthedocs.org/projects/tronflow-docs/badge/?version=latest)](https://tronflow-docs.readthedocs.io/en/latest/?badge=latest)\\n\\nThis pipeline aligns paired and single end FASTQ files with BWA aln and mem algorithms and with BWA mem 2.\\nFor RNA-seq STAR is also supported. To increase sensitivity of novel junctions use `--star_two_pass_mode` (recommended for RNAseq variant calling).\\nIt also includes an initial step of read trimming using FASTP.\\n\\n\\n## How to run it\\n\\nRun it from GitHub as follows:\\n```\\nnextflow run tron-bioinformatics/tronflow-alignment -profile conda --input_files $input --output $output --algorithm aln --library paired\\n```\\n\\nOtherwise download the project and run as follows:\\n```\\nnextflow main.nf -profile conda --input_files $input --output $output --algorithm aln --library paired\\n```\\n\\nFind the help as follows:\\n```\\n$ nextflow run tron-bioinformatics/tronflow-alignment  --help\\nN E X T F L O W  ~  version 19.07.0\\nLaunching `main.nf` [intergalactic_shannon] - revision: e707c77d7b\\n\\nUsage:\\n    nextflow main.nf --input_files input_files [--reference reference.fasta]\\n\\nInput:\\n    * input_fastq1: the path to a FASTQ file (incompatible with --input_files)\\n    * input_files: the path to a tab-separated values file containing in each row the sample name and two paired FASTQs (incompatible with --fastq1 and --fastq2)\\n    when `--library paired`, or a single FASTQ file when `--library single`\\n    Example input file:\\n    name1\\tfastq1.1\\tfastq1.2\\n    name2\\tfastq2.1\\tfastq2.2\\n    * reference: path to the indexed FASTA genome reference or the star reference folder in case of using star\\n\\nOptional input:\\n    * input_fastq2: the path to a second FASTQ file (incompatible with --input_files, incompatible with --library paired)\\n    * output: the folder where to publish output (default: output)\\n    * algorithm: determines the BWA algorithm, either `aln`, `mem`, `mem2` or `star` (default `aln`)\\n    * library: determines whether the sequencing library is paired or single end, either `paired` or `single` (default `paired`)\\n    * cpus: determines the number of CPUs for each job, with the exception of bwa sampe and samse steps which are not parallelized (default: 8)\\n    * memory: determines the memory required by each job (default: 32g)\\n    * inception: if enabled it uses an inception, only valid for BWA aln, it requires a fast file system such as flash (default: false)\\n    * skip_trimming: skips the read trimming step\\n    * star_two_pass_mode: activates STAR two-pass mode, increasing sensitivity of novel junction discovery, recommended for RNA variant calling (default: false)\\n    * additional_args: additional alignment arguments, only effective in BWA mem, BWA mem 2 and STAR (default: none) \\n\\nOutput:\\n    * A BAM file \\\\${name}.bam and its index\\n    * FASTP read trimming stats report in HTML format \\\\${name.fastp_stats.html}\\n    * FASTP read trimming stats report in JSON format \\\\${name.fastp_stats.json}\\n```\\n\\n### Input tables\\n\\nThe table with FASTQ files expects two tab-separated columns without a header\\n\\n| Sample name          | FASTQ 1                      | FASTQ 2                  |\\n|----------------------|---------------------------------|------------------------------|\\n| sample_1             | /path/to/sample_1.1.fastq      |    /path/to/sample_1.2.fastq   |\\n| sample_2             | /path/to/sample_2.1.fastq      |    /path/to/sample_2.2.fastq   |\\n\\n\\n### Reference genome\\n\\nThe reference genome has to be provided in FASTA format and it requires two set of indexes:\\n* FAI index. Create with `samtools faidx your.fasta`\\n* BWA indexes. Create with `bwa index your.fasta`\\n\\nFor bwa-mem2 a specific index is needed:\\n```\\nbwa-mem2 index your.fasta\\n```\\n\\nFor star a reference folder prepared with star has to be provided. In order to prepare it will need the reference\\ngenome in FASTA format and the gene annotations in GTF format. Run a command as follows:\\n```\\nSTAR --runMode genomeGenerate --genomeDir $YOUR_FOLDER --genomeFastaFiles $YOUR_FASTA --sjdbGTFfile $YOUR_GTF\\n```\\n\\n## References\\n\\n* Li H. and Durbin R. (2010) Fast and accurate long-read alignment with Burrows-Wheeler Transform. Bioinformatics, Epub. https://doi.org/10.1093/bioinformatics/btp698 \\n* Shifu Chen, Yanqing Zhou, Yaru Chen, Jia Gu; fastp: an ultra-fast all-in-one FASTQ preprocessor, Bioinformatics, Volume 34, Issue 17, 1 September 2018, Pages i884–i890, https://doi.org/10.1093/bioinformatics/bty560\\n* Vasimuddin Md, Sanchit Misra, Heng Li, Srinivas Aluru. Efficient Architecture-Aware Acceleration of BWA-MEM for Multicore Systems. IEEE Parallel and Distributed Processing Symposium (IPDPS), 2019.\\n* Dobin A, Davis CA, Schlesinger F, Drenkow J, Zaleski C, Jha S, Batut P, Chaisson M, Gingeras TR. STAR: ultrafast universal RNA-seq aligner. Bioinformatics. 2013 Jan 1;29(1):15-21. doi: 10.1093/bioinformatics/bts635. Epub 2012 Oct 25. PMID: 23104886; PMCID: PMC3530905.\\n'\n",
      " \"# TronFlow BAM preprocessing pipeline\\n\\n![GitHub tag (latest SemVer)](https://img.shields.io/github/v/release/tron-bioinformatics/tronflow-bam-preprocessing?sort=semver)\\n[![Automated tests](https://github.com/TRON-Bioinformatics/tronflow-bam-preprocessing/actions/workflows/automated_tests.yml/badge.svg)](https://github.com/TRON-Bioinformatics/tronflow-bam-preprocessing/actions/workflows/automated_tests.yml)\\n[![DOI](https://zenodo.org/badge/358400957.svg)](https://zenodo.org/badge/latestdoi/358400957)\\n[![License](https://img.shields.io/badge/license-MIT-green)](https://opensource.org/licenses/MIT)\\n[![Powered by Nextflow](https://img.shields.io/badge/powered%20by-Nextflow-orange.svg?style=flat&colorA=E1523D&colorB=007D8A)](https://www.nextflow.io/)\\n\\nThe TronFlow BAM preprocessing pipeline is part of a collection of computational workflows for tumor-normal pair \\nsomatic variant calling. These workflows are implemented in the Nextflow (Di Tommaso, 2017) framework.\\n\\nFind the documentation here [![Documentation Status](https://readthedocs.org/projects/tronflow-docs/badge/?version=latest)](https://tronflow-docs.readthedocs.io/en/latest/?badge=latest)\\n\\n\\nThe aim of this workflow is to preprocess BAM files based on Picard and GATK (DePristo, 2011) best practices.\\n\\n\\n## Background\\n\\nIn order to have a variant calling ready BAM file there are a number of operations that need to be applied on the BAM. \\nThis pipeline depends on the particular variant caller, but there are some common operations.\\n\\nGATK has been providing a well known best practices document on BAM preprocessing, the latest best practices for \\nGATK4 (https://software.broadinstitute.org/gatk/best-practices/workflow?id=11165) does not perform anymore realignment around indels as opposed to best practices for GATK3 (https://software.broadinstitute.org/gatk/documentation/article?id=3238). This pipeline is based on both Picard and GATK. These best practices have been implemented a number of times, see for instance this implementation in Workflow Definition Language https://github.com/gatk-workflows/gatk4-data-processing/blob/master/processing-for-variant-discovery-gatk4.wdl.\\n\\n\\n## Objectives\\n\\nWe aim at providing a single implementation of the BAM preprocessing pipeline that can be used across different \\nuse cases. \\nFor this purpose there are some required steps and some optional steps.  \\n\\nThe input can be either a tab-separated values file (`--input_files`) where each line corresponds to one input BAM or a single BAM (`--input_bam` and `--input_name`).\\n\\n## Implementation\\n\\nSteps:\\n\\n* **Clean BAM**. Sets the mapping quality to 0 for all unmapped reads and avoids soft clipping going beyond the reference genome boundaries. Implemented in Picard\\n* **Reorder chromosomes**. Makes the chromosomes in the BAM follow the same order as the reference genome. Implemented in Picard\\n* **Add read groups**. GATK requires that some headers are adde to the BAM, also we want to flag somehow the normal and tumor BAMs in the header as some callers, such as Mutect2 require it. Implemented in Picard.\\n* **Mark duplicates** (optional). Identify the PCR and the optical duplications and marks those reads. This uses the parallelized version on Spark, it is reported to scale linearly up to 16 CPUs.\\n* **Realignment around indels** (optional). This procedure is important for locus based variant callers, but for any variant caller doing haplotype assembly it is not needed. This is computing intensive as it first finds regions for realignment where there are indication of indels  and then it performs a local realignment over those regions. Implemented in GATK3, deprecated in GATK4\\n* **Base Quality Score Recalibration (BQSR)** (optional). It aims at correcting systematic errors in the sequencer when assigning the base call quality errors, as these scores are used by variant callers it improves variant calling in some situations. Implemented in GATK4\\n* **Metrics** (optional). A number of metrics are obtained from the BAM file with Picard's CollectMetrics, CollectHsMetrics and samtools' coverage and depth.\\n\\n![Pipeline](figures/bam_preprocessing2.png)\\n\\n\\n## How to run it\\n\\n```\\n$ nextflow run tron-bioinformatics/tronflow-bam-preprocessing --help\\n\\nN E X T F L O W  ~  version 19.07.0\\nLaunching `main.nf` [intergalactic_shannon] - revision: e707c77d7b\\n\\nUsage:\\n    main.nf --input_files input_files\\n\\nInput:\\n    * --input_bam: the path to a single BAM (this option is not compatible with --input_files)\\n    * --input_files: the path to a tab-separated values file containing in each row the sample name, sample type (eg: tumor or normal) and path to the BAM file (this option is not compatible with --input_bam)\\n    Sample type will be added to the BAM header @SN sample name\\n    The input file does not have header!\\n    Example input file:\\n    name1       tumor   tumor.1.bam\\n    name1       normal  normal.1.bam\\n    name2       tumor   tumor.2.bam\\n    * --reference: path to the FASTA genome reference (indexes expected *.fai, *.dict)\\n\\nOptional input:\\n    * --input_name: the name of the sample. Only used when --input_bam is provided (default: normal)\\n    * --dbsnp: path to the dbSNP VCF (required to perform BQSR)\\n    * --known_indels1: path to a VCF of known indels (optional to perform realignment around indels)\\n    * --known_indels2: path to a second VCF of known indels (optional to perform realignment around indels)\\n    * --intervals: path to a BED file to collect coverage and HS metrics from (default: None)\\n    * --collect_hs_minimum_base_quality: minimum base quality for a base to contribute coverage (default: 20).\\n    * --collect_hs_minimum_mapping_quality: minimum mapping quality for a read to contribute coverage (default: 20).\\n    * --skip_bqsr: optionally skip BQSR (default: false)\\n    * --skip_realignment: optionally skip realignment (default: false)\\n    * --skip_deduplication: optionally skip deduplication (default: false)\\n    * --remove_duplicates: removes duplicate reads from output BAM instead of flagging them (default: true)\\n    * --skip_metrics: optionally skip metrics (default: false)\\n    * --output: the folder where to publish output (default: ./output)\\n    * --platform: the platform to be added to the BAM header. Valid values: [ILLUMINA, SOLID, LS454, HELICOS and PACBIO] (default: ILLUMINA)\\n\\nComputational resources:\\n    * --prepare_bam_cpus: (default: 3)\\n    * --prepare_bam_memory: (default: 8g)\\n    * --mark_duplicates_cpus: (default: 16)\\n    * --mark_duplicates_memory: (default: 64g)\\n    * --realignment_around_indels_cpus: (default: 2)\\n    * --realignment_around_indels_memory: (default: 31g)\\n    * --bqsr_cpus: (default: 3)\\n    * --bqsr_memory: (default: 4g)\\n    * --metrics_cpus: (default: 1)\\n    * --metrics_memory: (default: 8g)\\n\\n Output:\\n    * Preprocessed and indexed BAMs\\n    * Tab-separated values file with the absolute paths to the preprocessed BAMs, preprocessed_bams.txt\\n\\nOptional output:\\n    * Recalibration report\\n    * Deduplication metrics\\n    * Realignment intervals\\n    * GATK multiple metrics\\n    * HS metrics\\n    * Horizontal and vertical coverage metrics\\n```\\n\\n### Input table\\n\\nThe table with FASTQ files expects two tab-separated columns **without a header**\\n\\n| Sample name          | Sample type                      | BAM                  |\\n|----------------------|---------------------------------|------------------------------|\\n| sample_1             | normal      |    /path/to/sample_1.normal.bam   |\\n| sample_1             | tumor      |    /path/to/sample_1.tumor.bam   |\\n| sample_2             | normal      |    /path/to/sample_2.normal.bam   |\\n| sample_2             | tumor      |    /path/to/sample_2.tumor.bam   |\\n\\nThe values used in `sample type` are arbitrary. These will be set in the BAM header tag @RG:SM for sample. There may be some downstream constraints, eg: Mutect2 pipeline requires that the sample type between normal and tumor samples of the same pair are not the same.\\n\\n### References\\n\\nThe BAM preprocessing workflow requires the human reference genome (`--reference`)\\nBase Quality Score Recalibration (BQSR) requires dbSNP to avoid extracting error metrics from polymorphic sites (`--dbsnp`)\\nRealignment around indels requires a set of known indels (`--known_indels1` and `--known_indels2`).\\nThese resources can be fetched from the GATK bundle https://gatk.broadinstitute.org/hc/en-us/articles/360035890811-Resource-bundle.\\n\\nOptionally, in order to run Picard's CollectHsMetrics a BED file will need to be provided (`--intervals`).\\nThis BED file will also be used for `samtools coverage`.\\n\\n## Troubleshooting\\n\\n### Too new Java version for MarkDuplicatesSpark\\n\\nWhen using Java 11 the cryptic error messsage `java.lang.IllegalArgumentException: Unsupported class file major version 55` has been observed.\\nThis issue is described here and the solution is to use Java 8 https://gatk.broadinstitute.org/hc/en-us/community/posts/360056174592-MarkDuplicatesSpark-crash.\\n\\n\\n\\n## Bibliography\\n\\n* DePristo M, Banks E, Poplin R, Garimella K, Maguire J, Hartl C, Philippakis A, del Angel G, Rivas MA, Hanna M, McKenna A, Fennell T, Kernytsky A, Sivachenko A, Cibulskis K, Gabriel S, Altshuler D, Daly M. (2011). A framework for variation discovery and genotyping using next-generation DNA sequencing data. Nat Genet, 43:491-498. DOI: 10.1038/ng.806.\\n* Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P. P., Palumbo, E., & Notredame, C. (2017). Nextflow enables reproducible computational workflows. Nature Biotechnology, 35(4), 316–319. 10.1038/nbt.3820\\n\"\n",
      " 'This workflow take as input a collection of paired fastq. It uses HiCUP to go from fastq to validPair file. The pairs are filtered for MAPQ and sorted by cooler to generate a tabix dataset. Cooler is used to generate a balanced cool file to the desired resolution.'\n",
      " 'We present an R script that describes the workflow for analysing honey bee (Apis mellifera) wing shape. It is based on a large dataset of wing images and landmark coordinates available at Zenodo: https://doi.org/10.5281/zenodo.7244070. The dataset can be used as a reference for the identification of unknown samples. As unknown samples, we used data from Nawrocka et al. (2018), available at Zenodo: https://doi.org/10.5281/zenodo.7567336. Among others, the script can be used to identify the geographic origin of unknown samples and therefore assist in the monitoring and conservation of honey bee biodiversity in Europe.'\n",
      " '\\n\\n\\nGermlineStructuralV-nf is a pipeline for identifying structural variant events in human Illumina short read whole genome sequence data. GermlineStructuralV-nf identifies structural variant and copy number events from BAM files using [Manta](https://github.com/Illumina/manta/blob/master/docs/userGuide/README.md#de-novo-calling), [Smoove](https://github.com/brentp/smoove), and [TIDDIT](https://github.com/SciLifeLab/TIDDIT). Variants are then merged using [SURVIVOR](https://github.com/fritzsedlazeck/SURVIVOR), and annotated by [AnnotSV](https://pubmed.ncbi.nlm.nih.gov/29669011/). The pipeline is written in Nextflow and uses Singularity/Docker to run containerised tools.'\n",
      " \"# Snakemake workflow: Reconstructing raw tomography data\\n\\nA Snakemake worfklow for tomographically reconstructing raw data using [tomopy](https://tomopy.readthedocs.io/en/stable/).\\n\\n## Installation\\n\\nFirst download this repo and navigate to it\\n```bash\\ngit clone https://codebase.helmholtz.cloud/gernha62/reconstructing-raw-tomography-data.git\\n```\\n```bash\\ncd /path/to/repo\\n```\\n(Optional) Download the example folder with:\\n```bash\\nwget -m -np https://doi2.psi.ch/datasets/das/work/p15/p15869/compression/MI04_02/tif\\n```\\nCreate a virtual environment and install all necessary packages (requires conda): \\n```bash\\nconda env create --name reconstr_env --file workflow/envs/reconstr.yml\\n```\\nActivate the new virtual environment: \\n```bash\\nconda activate reconstr_env\\n```\\n\\n## Configuration\\n\\nTo configure the workflow, adapt the config file found at `config/config.yaml` . The config looks as follows:\\n```yaml\\nnumber_of_darks: 50\\nnumber_of_flats: 100\\nnumber_of_projections: 501\\nrotation_center: 508.77\\nraw_data:\\n  MI04_02: doi2.psi.ch/datasets/das/work/p15/p15869/compression/MI04_02/tif\\n```\\n In the config, adjust `number_of_darks`, `number_of_flats`, `number_of_projections` and `rotation_center` to the number of darks, flats, projections and the rotation center of your dataset. The necessary information can usually be found in the .log file of the folder that contains the raw data. \\n\\n`MI04_02: doi2.psi.ch/datasets/das/work/p15/p15869/compression/MI04_02/tif` denotes the path to the example folder used for reconstruction and the keyword `MI04_02` will be used to name the output (e.g. in this case the output folder will be named `recon_dir_MI04_02`). Replace the examle path with the path to the dataset you want to reconstruct. Additionally, if you want the name of the output folder to have a different suffix, replace the keyword `MI04_02` with a name you prefer.\\n\\n## Run the workflow\\n\\nIf the .tif files contain a numerical prefix that is not separated from the actual image index, it is best to first rename the files. The files will be renamed to `00001.tif`, `00002.tif` and so on. If the renaming is needed, run:\\n\\n```bash\\nsnakemake --cores 1 'logs/renamefile_MI04_02.log'\\n```\\nIf you replaced the keyword `MI04_02` in the config file then adjust the command accordingly (e.g. if you replaced the keyword with `Tomo_dataset` then the command should be `snakemake --cores 1 'logs/renamefile_Tomo_dataset.log'`).\\n\\nBefore trying to compute the reconstructions, make sure you have enough memory available (ideally more than 60 GB).\\nTo compute the reconstructions using one core, use the command:\\n```bash\\nsnakemake --cores 1\\n```\\nIf you want to use all available cores instead, use:\\n```bash\\nsnakemake --cores all\\n```\\nThis creates a folder in `results` with the reconstructed data.\\n\\n## Credit\\nThe example dataset used in this project (MI04_02 evolving magma, Mattia Pistone, University of Georgia) was taken from: https://doi.psi.ch/detail/10.16907/05a50450-767f-421d-9832-342b57c201af\\n\\nThe script used for reconstruction (`scripts/reconstructs_tomo_datasets.py`) was provided by Alain Studer, PSI.\"\n",
      " '# 1. About TF-Prioritizer\\n\\nThis pipeline gives you a full analysis of nfcore chromatine accessibility peak data (ChIP-Seq, ATAC-Seq or DNAse-Seq)\\nand nfcore RNA-seq count data. It performs\\nDESeq2, TEPIC and DYNAMITE including all preprocessing and postprocessing steps necessary to transform the data. It also\\ngives you plots for deep analysis of the data. The general workflow is sketched in the images below:\\n\\n## Graphical abstract:\\n\\n![Graphical abstrat](https://raw.githubusercontent.com/biomedbigdata/TF-Prioritizer/master/media/graphicalAbstract.png)\\n\\n## Technical workflow:\\n\\n![Technical workflow](https://github.com/biomedbigdata/TF-Prioritizer/raw/master/media/technicalWorkflow.png)\\n\\n# 2. License and Citing\\n\\nTF-Prioritizer is distributed under the [GNU General Public License](https://www.gnu.org/licenses/gpl-3.0.en.html). The\\nGraphical Abstract and the Technical Workflow\\nwas created using [biorender.com](https://biorender.com/).\\n\\n# 3. Usage\\n\\nThe software can be executed using docker. For the following command, only [python3](https://www.python.org/downloads/),\\n[curl](https://curl.se/download.html) and [docker](https://docs.docker.com/get-docker/) are required.\\nExplanations about the configs can be found in\\nthe [config readme](https://github.com/biomedbigdata/TF-Prioritizer/blob/master/configTemplates/README.md).\\n\\n```bash\\ncurl -s https://raw.githubusercontent.com/biomedbigdata/TF-Prioritizer/master/docker.py | python3 - -c [config_file] -o [output_dir] -t [threads]\\n```\\n\\nNote, that for this approach an internet connection is required. The docker image will be downloaded\\nfrom [DockerHub](https://hub.docker.com/r/nicotru/tf-prioritizer) on the first execution as well as with every update we\\nrelease. Furthermore, the wrapper script\\nwill be fetched from GitHub with every execution.\\n\\nIf curl is not available (for example if you are using windows), or you want to be able to execute the software without\\nan internet connection, you can download the wrapper script\\nfrom [here](https://raw.githubusercontent.com/biomedbigdata/TF-Prioritizer/pipeJar/docker.py).\\n\\nYou can then execute the script using\\n\\n```bash\\npython3 [script_path] -c [config_file] -o [output_dir] -t [threads]\\n```\\n\\n## If you want to use the pipeline without docker\\n\\nWe do not recommend using the pipeline without docker, because the dependencies are very complex, and it is very hard to\\ninstall them correctly. However, if you want to use the pipeline without docker, you can do so by installing the\\ndependencies manually. The dependencies and their correct installation process can be derived from\\nthe [Dockerfile](https://github.com/biomedbigdata/TF-Prioritizer/blob/master/Dockerfile) and the environment scripts\\nwhich can be found in\\nthe [environment directory](https://github.com/biomedbigdata/TF-Prioritizer/tree/master/environment).'\n",
      " 'extract 1 Id from SRA and assume it is PE as input to viralRNASpades.'\n",
      " '# sqtlseeker2-nf\\n\\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A50.27.0-blue.svg)](http://nextflow.io)\\n[![CI-checks](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml/badge.svg)](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml)\\n\\nA pipeline for splicing quantitative trait loci (sQTL) mapping.\\n\\nThe pipeline performs the following analysis steps:\\n\\n* Index the genotype file\\n* Preprocess the transcript expression data\\n* Test for association between splicing ratios and genetic variants in *cis* (nominal pass)\\n* Obtain an empirical P-value for each phenotype (permutation pass, optional)\\n* Control for multiple testing \\n\\nFor details on each step, please read [sQTLseekeR2](https://github.com/guigolab/sQTLseekeR2) documentation.\\n\\nThe pipeline uses [Nextflow](http://www.nextflow.io) as the execution backend. Please check [Nextflow documentation](http://www.nextflow.io/docs/latest/index.html) for more information.\\n\\n## Requirements\\n\\n- Unix-like operating system (Linux, MacOS, etc.)\\n- Java 8 or later \\n- [Docker](https://www.docker.com/) (v1.10.0 or later) or [Singularity](http://singularity.lbl.gov) (v2.5.0 or later)\\n\\n## Quickstart (~2 min)\\n\\n1. Install Nextflow:\\n    ```\\n    curl -fsSL get.nextflow.io | bash\\n    ```\\n\\n2. Make a test run:\\n    ```\\n    ./nextflow run guigolab/sqtlseeker2-nf -with-docker\\n    ```\\n\\n    **Note**: set `-with-singularity` to use Singularity instead of Docker. \\n\\n## Pipeline usage\\n\\nLaunching the pipeline with the `--help` parameter shows the help message:\\n\\n```\\nnextflow run sqtlseeker2-nf --help\\n```\\n\\n```\\nN E X T F L O W  ~  version 0.27.2\\nLaunching `sqtlseeker2.nf` [admiring_lichterman] - revision: 28c86caf1c\\n\\nsqtlseeker2-nf ~ A pipeline for splicing QTL mapping\\n----------------------------------------------------\\nRun sQTLseekeR2 on a set of data.\\n\\nUsage: \\n    sqtlseeker2-nf [options]\\n\\nOptions:\\n--genotype GENOTYPE_FILE    the genotype file\\n--trexp EXPRESSION_FILE     the transcript expression file\\n--metadata METADATA_FILE    the metadata file\\n--genes GENES_FILE          the gene location file\\n--dir DIRECTORY             the output directory\\n--mode MODE                 the run mode: nominal or permuted (default: nominal)\\n--win WINDOW                the cis window in bp (default: 5000)\\n--covariates COVARIATES     include covariates in the model (default: false)\\n--fdr FDR                   false discovery rate level (default: 0.05)\\n--min_md MIN_MD             minimum effect size reported (default: 0.05)\\n--svqtl SVQTLS              report svQTLs (default: false)\\n\\nAdditional parameters for mode = nominal:\\n--ld LD                     threshold for LD-based variant clustering (default: 0, no clustering)\\n--kn KN                     number of genes per batch in nominal pass (default: 10)\\n\\nAdditional parameters for mode = permuted:\\n--kp KP                     number of genes per batch in permuted pass (default: 10)\\n--max_perm MAX_PERM         maximum number of permutations (default: 1000)\\n```\\n\\n## Input files and format\\n\\n`sqtlseeker2-nf` takes as input files the following:\\n\\n* **Genotype file.**\\nContains the genotype of each sample, coded as follows: 0 for REF/REF, 1 for REF/ALT, 2 for ALT/ALT, -1 for missing value.\\nThe first four columns should be: `chr`, `start`, `end` and `snpId`. This file needs to be sorted by coordinate.\\n\\n* **Transcript expression file.**\\nContains the expression of each transcript in each sample (e.g. read counts, RPKM, TPM).\\nIt is not recommended to use transformed (log, quantile, or any non-linear transformation) expression.\\nColumns `trId` and `geneId`, corresponding to the transcript and gene IDs, are required. \\n\\n* **Metadata file.** Contains the covariate information for each sample. \\nIn addition, it defines the groups or conditions for which sQTL mapping will be performed.\\nThe first columns should be: `indId`, `sampleId`, `group`, followed by the covariates.\\nThis file defines which samples will be tested.\\n\\n* **Gene location file.**\\nContains the location of each gene. Columns `chr`, `start`, `end` and `geneId` are required. \\nThis file defines which genes will be tested.\\n\\nExample [data](data) is available for the test run.\\n\\n## Pipeline results\\n\\nsQTL mapping results are saved into the folder specified with the `--dir` parameter. By default it is the `result` folder within the current working directory.\\n\\nOutput files are organinzed into subfolders corresponding to the different `groups` specified in the metadata file: \\n\\n```\\nresult\\n└── groups\\n    ├── group1                            \\n    │\\xa0\\xa0 ├── all-tests.nominal.tsv          \\n    │\\xa0\\xa0 ├── all-tests.permuted.tsv         \\n    │\\xa0\\xa0 ├── sqtls-${level}fdr.nominal.tsv      \\n    │\\xa0\\xa0 └── sqtls-${level}fdr.permuted.tsv     \\n    ├── group2\\n   ...\\n```\\n\\nNote: if only a nominal pass was run, files `*.permuted.tsv` will not be present.\\n\\nOutput files contain the following information:\\n\\n`all-tests.nominal.tsv`\\n\\n* geneId: gene name\\t\\n* snpId: variant name\\n* F: test statistic\\n* nb.groups: number of genotype groups\\n* md: maximum difference in relative expression between genotype groups (sQTL effect size)\\n* tr.first/tr.second: the transcript IDs of the two transcripts that change the most, in opposite directions\\n* info: number of individuals in each genotype group, including missing values (-1,0,1,2)\\n* pv: nominal P-value\\n\\nif `--svqtl true`\\n* F.svQTL: svQTL test statistic\\n* nb.perms.svQTL: number of permutations for svQTL test\\n* pv.svQTL: svQTL nominal P-value \\n\\nif `--ld ${r2}`\\n* LD: other variants in linkage disequilibrium with snpId above a given r<sup>2</sup> threshold > 0\\n\\n`sqtls-${level}fdr.nominal.tsv` (in addition to the previous)\\n\\n* fdr: false discovery rate (computed across all nominal tests)\\n* fdr.svQTL: svQTL FDR\\n\\n`all-tests.permuted.tsv`\\n\\n* geneId: gene name\\n* variants.cis: number of variants tested in *cis*\\n* LD: median linkage disequilibrium in the region (r<sup>2</sup>)\\n* best.snp: ID of the top variant\\n* best.nominal.pv: P-value of the top variant\\n* shape1: first parameter value of the fitted beta distribution\\n* shape2: second parameter value of the fitted beta distribution (effective number of independent tests in the region)\\n* nb.perm: number of permutations\\n* pv.emp.perm: empirical P-value, computed based on permutations\\n* pv.emp.beta: empirical P-value, computed based on the fitted beta distribution\\n* runtime: run time in minutes\\n\\n`sqtls-${level}fdr.nominal.tsv` (in addition to the previous)\\n\\n* fdr: false discovery rate (computed across empirical P-values)\\n* p_tn: gene-level threshold for nominal P-values\\n\\n## Cite sqtlseeker2-nf\\n\\nIf you find `sqtlseeker2-nf` useful in your research please cite the related publication:\\n\\nGarrido-Martín, D., Borsari, B., Calvo, M., Reverter, F., Guigó, R. Identification and analysis of splicing quantitative trait loci across multiple tissues in the human genome. *Nat Commun* 12, 727 (2021). [https://doi.org/10.1038/s41467-020-20578-2](https://doi.org/10.1038/s41467-020-20578-2)\\n\\n'\n",
      " '# mvgwas-nf\\n\\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.1-blue.svg)](http://nextflow.io)\\n[![CI-checks](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml/badge.svg)](https://github.com/guigolab/sqtlseeker2-nf/actions/workflows/ci.yaml)\\n\\nA pipeline for multi-trait genome-wide association studies (GWAS) using [MANTA](https://github.com/dgarrimar/manta).\\n\\nThe pipeline performs the following analysis steps:\\n\\n* Split genotype file \\n* Preprocess phenotype and covariate data\\n* Test for association between phenotypes and genetic variants\\n* Collect summary statistics\\n\\nThe pipeline uses [Nextflow](http://www.nextflow.io) as the execution backend. Please check [Nextflow documentation](http://www.nextflow.io/docs/latest/index.html) for more information.\\n\\n## Requirements\\n\\n- Unix-like operating system (Linux, MacOS, etc.)\\n- Java 8 or later \\n- [Docker](https://www.docker.com/) (v1.10.0 or later) or [Singularity](http://singularity.lbl.gov) (v2.5.0 or later)\\n\\n## Quickstart (~2 min)\\n\\n1. Install Nextflow:\\n    ```\\n    curl -fsSL get.nextflow.io | bash\\n    ```\\n\\n2. Make a test run:\\n    ```\\n    nextflow run dgarrimar/mvgwas-nf -with-docker\\n    ```\\n\\n**Notes**: move the `nextflow` executable to a directory in your `$PATH`. Set `-with-singularity` to use Singularity instead of Docker. \\n\\n(*) Alternatively you can clone this repository:\\n```\\ngit clone https://github.com/dgarrimar/mvgwas-nf\\ncd mvgwas-nf\\nnextflow run mvgwas.nf -with-docker\\n```\\n\\n## Pipeline usage\\n\\nLaunching the pipeline with the `--help` parameter shows the help message:\\n\\n```\\nnextflow run mvgwas.nf --help\\n```\\n\\n```\\nN E X T F L O W  ~  version 20.04.1\\nLaunching `mvgwas.nf` [amazing_roentgen] - revision: 56125073b7\\n\\nmvgwas-nf: A pipeline for multivariate Genome-Wide Association Studies\\n==============================================================================================\\nPerforms multi-trait GWAS using using MANTA (https://github.com/dgarrimar/manta)\\n\\nUsage:\\nnextflow run mvgwas.nf [options]\\n\\nParameters:\\n--pheno PHENOTYPES          phenotype file\\n--geno GENOTYPES            indexed genotype VCF file\\n--cov COVARIATES            covariate file\\n--l VARIANTS/CHUNK          variants tested per chunk (default: 10000)\\n--t TRANSFOMATION           phenotype transformation: none, sqrt, log (default: none)\\n--i INTERACTION             test for interaction with a covariate: none, <covariate> (default: none)\\n--ng INDIVIDUALS/GENOTYPE   minimum number of individuals per genotype group (default: 10)\\n--dir DIRECTORY             output directory (default: result)\\n--out OUTPUT                output file (default: mvgwas.tsv)\\n```\\n\\n## Input files and format\\n\\n`mvgwas-nf` requires the following input files:\\n\\n* **Genotypes.** \\n[bgzip](http://www.htslib.org/doc/bgzip.html)-compressed and indexed [VCF](https://samtools.github.io/hts-specs/VCFv4.3.pdf) genotype file.\\n\\n* **Phenotypes.**\\nTab-separated file with phenotype measurements (quantitative) for each sample (i.e. *n* samples x *q* phenotypes).\\nThe first column should contain sample IDs. Columns should be named.\\n\\n* **Covariates.**\\nTab-separated file with covariate measurements (quantitative or categorical) for each sample (i.e. *n* samples x *k* covariates). \\nThe first column should contain sample IDs. Columns should be named. \\n\\nExample [data](data) is available for the test run.\\n\\n## Pipeline results\\n\\nAn output text file containing the multi-trait GWAS summary statistics (default: `./result/mvgwas.tsv`), with the following information:\\n\\n* `CHR`: chromosome\\n* `POS`: position\\n* `ID`: variant ID\\n* `REF`: reference allele\\n* `ALT`: alternative allele\\n* `F`: pseudo-F statistic\\n* `R2`: fraction of variance explained by the variant\\n* `P`: P-value\\n\\nThe output folder and file names can be modified with the `--dir` and `--out` parameters, respectively.\\n\\n## Cite mvgwas-nf\\n\\nIf you find `mvgwas-nf` useful in your research please cite the related publication:\\n\\nGarrido-Martín, D., Calvo, M., Reverter, F., Guigó, R. A fast non-parametric test of association for multiple traits. *bioRxiv* (2022). [https://doi.org/10.1101/2022.06.06.493041](https://doi.org/10.1101/2022.06.06.493041)\\n'\n",
      " '# ROIforMSI\\nSource codes for manuscript \"Delineating Regions-of-interest for Mass Spectrometry Imaging by Multimodally Corroborated Spatial Segmentation\"\\n\\n\\n\"ExampleWorkflow.ipynb\" is a methods document to demonstrate the workflow of our multimodal fusion-based spatial segmentation.\\n\\n\\n\"Utilities.py\" contains all the tools to implement our method.\\n\\n\\n\"gui.py\" and \"registration_gui.py\" are files to implement linear and nonlinear registration.\\n\\n(Licence: GPL-3)'\n",
      " '# MoP2- DSL2 version of Master of Pores\\n[![Docker Build Status](https://img.shields.io/docker/automated/biocorecrg/nanopore.svg)](https://cloud.docker.com/u/biocorecrg/repository/docker/biocorecrg/nanopore/builds)\\n[![mop2-CI](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml/badge.svg)](https://github.com/biocorecrg/MoP2/actions/workflows/build.yml)\\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\\n[![Nextflow version](https://img.shields.io/badge/Nextflow-21.04.1-brightgreen)](https://www.nextflow.io/)\\n[![Nextflow DSL2](https://img.shields.io/badge/Nextflow-DSL2-brightgreen)](https://www.nextflow.io/)\\n[![Singularity version](https://img.shields.io/badge/Singularity-v3.2.1-green.svg)](https://www.sylabs.io/)\\n[![Docker version](https://img.shields.io/badge/Docker-v20.10.8-blue)](https://www.docker.com/)\\n\\n<br/>\\n\\n![MOP2](https://github.com/biocorecrg/MoP2/blob/main/img/master_red.jpg?raw=true)\\n\\n\\nInspired by Metallica\\'s [Master Of Puppets](https://www.youtube.com/watch?v=S7blkui3nQc)\\n\\n## Install\\nPlease install nextflow and singularity or docker before.\\n\\nThen download the repo:\\n\\n```\\ngit clone --depth 1 --recurse-submodules git@github.com:biocorecrg/MOP2.git\\n```\\n\\nYou can use INSTALL.sh to download the version 3.4.5 of guppy or you can replace it with the version you prefer. Please consider that the support of VBZ compression of fast5 started with version 3.4.X. \\n\\n```\\ncd MoP2; sh INSTALL.sh\\n```\\n\\n## Testing\\nYou can replace ```-with-singularity``` with ```-with-docker``` if you want to use the docker engine.\\n\\n```\\ncd mop_preprocess\\nnextflow run mop_preprocess.nf -with-singularity -bg -profile local > log\\n\\n```\\n\\n## Reference\\nIf you use this tool, please cite our papers:\\n\\n[\"Nanopore Direct RNA Sequencing Data Processing and Analysis Using MasterOfPores\"\\nCozzuto L, Delgado-Tejedor A, Hermoso Pulido T, Novoa EM, Ponomarenko J. *N. Methods Mol Biol. 2023*;2624:185-205. doi: 10.1007/978-1-0716-2962-8_13.](https://link.springer.com/protocol/10.1007/978-1-0716-2962-8_13)\\n\\n[\"MasterOfPores: A Workflow for the Analysis of Oxford Nanopore Direct RNA Sequencing Datasets\"\\nLuca Cozzuto, Huanle Liu, Leszek P. Pryszcz, Toni Hermoso Pulido, Anna Delgado-Tejedor, Julia Ponomarenko, Eva Maria Novoa.\\n*Front. Genet., 17 March 2020.* https://doi.org/10.3389/fgene.2020.00211](https://www.frontiersin.org/articles/10.3389/fgene.2020.00211/full)\\n\\n\\n## Documentation\\nThe documentation is available at [https://biocorecrg.github.io/MOP2/docs/](https://biocorecrg.github.io/MOP2/docs/about.html)\\n'\n",
      " 'A workflow for the analysis of pox virus genomes sequenced as half-genomes (for ITR resolution) in a tiled-amplicon approach'\n",
      " '# IGVreport-nf \\n\\n- [Description](#description)\\n  - [Diagram](#diagram)\\n  - [User guide](#user-guide)\\n  - [Workflow summaries](#workflow-summaries)\\n      - [Metadata](#metadata)\\n      - [Component tools](#component-tools)\\n      - [Required (minimum)\\n        inputs/parameters](#required-minimum-inputsparameters)\\n  - [Additional notes](#additional-notes)\\n  - [Help/FAQ/Troubleshooting](#helpfaqtroubleshooting)\\n  - [Acknowledgements/citations/credits](#acknowledgementscitationscredits)\\n\\n## Description \\n\\nQuickly generate [IGV `.html` reports](https://github.com/igvteam/igv-reports) for a genomic region of interest in the human genome (hg38). Bcftools is used to subset a VCF to a region of interest, the subset VCF is then passed to IGV-reports, which generates a report consisting of a table of genomic sites or regions and associated IGV views for each site. The reports can be opened by any web browser as a static page.  \\n\\n### Diagram \\n\\n```mermaid\\ngraph LR;\\n    VCF-->|bcftools view|SubsetVCF;\\n    SubsetVCF-->|IGVtools|HTMLreport;\\n    AlignmentBAM-->|IGVtools|HTMLreport;\\n```\\n\\n### User guide\\n\\nThis workflow uses containers for all steps and can run using Singularity or Docker. It requires Nextflow and either Singularity or Docker be installed. For instructions on installing Nextflow, see their [documentation](https://www.nextflow.io/docs/latest/getstarted.html).\\n\\n**This workflow currently only generates reports for the human reference genome assembly, Hg38.** \\n\\nThe workflow runs three processes: \\n1. The provided VCF file is subset to a region of interest using Bcftools view \\n2. The Subset VCF file is then indexed using Bcftools index \\n3. The subset VCF and provided Bam file are used to generate the html report for the region of interest. \\n\\nTo start clone this repository: \\n```\\ngit clone https://github.com/Sydney-Informatics-Hub/IGVreport-nf.git\\n```\\n\\nFrom the IGVreport-nf directory, run the pipeline: \\n```\\nnextflow run main.nf --sample <sampleID> \\\\\\n    --bam <path/to/bam> \\\\\\n    --vcf <path/to/vcf> \\\\\\n    --chr <chrID> --start <begin bp> --stop <end bp>     \\n```\\n\\nThis will create a report in a directory titled `./Report`. You can rename this directory at runtime using the flag `--outDir`. All runtime summary reports will be available in the `./runInfo` directory.  \\n\\n### Workflow summaries\\n\\n#### Metadata \\n\\n|metadata field     | workflow_name / workflow_version  |\\n|-------------------|:---------------------------------:|\\n|Version            | 1.0                               |\\n|Maturity           | under development                 |\\n|Creators           | Georgie Samaha                    |\\n|Source             | NA                                |\\n|License            | GPL-3.0 license                   |\\n|Workflow manager   | NextFlow                          |\\n|Container          | None                              |\\n|Install method     | NA                                |\\n|GitHub             | github.com/Sydney-Informatics-Hub/IGVreport-nf    |\\n|bio.tools \\t        | NA                                |\\n|BioContainers      | NA                                | \\n|bioconda           | NA                                |\\n\\n#### Component tools\\n\\n* nextflow>=20.07.1\\n* singularity or docker\\n* bcftools/1.16\\n* igv-reports/1.6.1\\n\\n#### Required (minimum) inputs/parameters\\n\\n* An indexed alignment file in Bam format \\n* A gzipped and indexed vcf file\\n\\n## Additional notes\\n\\n## Help/FAQ/troubleshooting \\n\\n## Acknowledgements/citations/credits\\n\\nThis workflow was developed by the Sydney Informatics Hub, a Core Research Facility of the University of Sydney and the Australian BioCommons which is enabled by NCRIS via Bioplatforms Australia. \\n'\n",
      " \"# SNP-Calling\\nGATK Variant calling pipeline for genomic data using Nextflow\\n\\n[![nextflow](https://img.shields.io/badge/nextflow-%E2%89%A522.04.5-brightgreen.svg)](http://nextflow.io)\\n\\n## Quickstart\\n\\nInstall Nextflow using the following command: \\n\\n    curl -s https://get.nextflow.io | bash\\n  \\nIndex reference genome:\\n\\n  `$ bwa index /path/to/reference/genome.fa`\\n \\n  `$ samtools faidx /path/to/reference/genome.fa`\\n  \\n  `$ gatk CreateSequenceDictionary -R /path/to/genome.fa -O genome.dict`\\n\\nLaunch the pipeline execution with the following command:\\n\\n    nextflow run jdetras/snp-calling -r main -profile docker\\n  \\n## Pipeline Description\\n\\nThe variant calling pipeline follows the recommended practices from GATK. The input genomic data are aligned to a reference genome using BWA. The alignemnt files are processed using Picard Tools. Variant calling is done using samtools and GATK. \\n\\n## Input files\\n\\nThe input files required to run the pipeline:\\n* Genomic sequence paired reads, `*_{1,2}.fq.gz`\\n* Reference genome, `*.fa`\\n\\n## Pipeline parameters\\n\\n### Usage\\nUsage: `nextflow run jdetras/snp-calling -profile docker [options]`\\n\\nOptions:\\n\\n* `--reads` \\n* `--genome`\\n* `--output`\\n\\nExample: \\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz' --genome '/path/to/reference/genome.fa' --output '/path/to/output'`\\n\\n#### `--reads`\\n\\n* The path to the FASTQ read files.\\n* Wildcards (*, ?) can be used to declare multiple reads. Use single quotes when wildcards are used. \\n* Default parameter: `$projectDir/data/reads/*_{1,2}.fq.gz`\\n\\nExample: \\n  `$ nextflow run jdetras/snp-calling -profile docker --reads '/path/to/reads/*_{1,2}.fq.gz'`\\n  \\n#### `--genome`\\n\\n* The path to the genome file in fasta format.\\n* The extension is `.fa`.\\n* Default parameter: `$projectDir/data/reference/genome.fa`\\n\\nExample:\\n  `$ nextflow run jdetras/snp-calling -profile docker --genome /path/to/reference/genome.fa`\\n    \\n#### `--output`\\n\\n* The path to the directory for the output files.\\n* Default parameter: `$projectDir/output`\\n\\n## Software\\n\\n* [BWA 0.7.17](http://bio-bwa.sourceforge.net/)\\n* [Samtools 1.3.1](http://www.htslib.org/)\\n* [GATK 4.2.6.1](https://gatk.broadinstitute.org/) \\n\"\n",
      " '## Introduction\\n\\n**wombat-p pipelines** is a bioinformatics analysis pipeline that bundles different workflow for the analysis of label-free proteomics data with the purpose of comparison and benchmarking. It allows using files from the [proteomics metadata standard SDRF](https://github.com/bigbio/proteomics-metadata-standard).\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. We used one of the [nf-core](https://nf-co.re/) templates. \\n\\n## Pipeline summary\\n\\nThis work contains four major different workflows for the analysis or label-free proteomics data, originating from LC-MS experiments.\\n1. [MaxQuant](https://www.maxquant.org/) + [NormalyzerDE](https://normalyzerde.immunoprot.lth.se/)\\n2. [SearchGui](http://compomics.github.io/projects/searchgui) + [Proline](https://www.profiproteomics.fr/proline/) + [PolySTest](https://bitbucket.org/veitveit/polystest)\\n3. [Compomics tools](http://compomics.github.io/) + [FlashLFQ](https://github.com/smith-chem-wisc/FlashLFQ) + [MSqRob](https://github.com/statOmics/MSqRob)\\n4. Tools from the [Trans-Proteomic Pipeline](http://tools.proteomecenter.org/TPP.php) + [ROTS](https://bioconductor.org/packages/release/bioc/html/ROTS.html)\\n\\nInitialization and parameterization of the workflows is based on tools from the [SDRF pipelines](https://github.com/bigbio/sdrf-pipelines), the [ThermoRawFileParser](http://compomics.github.io/projects/ThermoRawFileParser) with our own contributions and additional programs from the wombat-p organizaion [https://github.com/wombat-p/Utilities] as well as our [fork](https://github.com/elixir-proteomics-community/sdrf-pipelines). This includes setting a generalized set of data analysis parameters and the calculation of a multiple benchmarks.\\n\\n## Credits\\n\\nnf-core/wombat was originally written by the members of the ELIXIR Implementation study  [Comparison, benchmarking and dissemination of proteomics data analysis pipelines](https://elixir-europe.org/internal-projects/commissioned-services/proteomics-pipelines) under the lead of Veit Schwämmle and major participation of David Bouyssié and Fredrik Levander.\\n\\n## Citations\\n\\nManuscript in preparation\\n\\n\\nAs the workflows are using an nf-core template, we refer to the publication:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).'\n",
      " '[![ci](https://github.com/zavolanlab/zarp/workflows/CI/badge.svg?branch=dev)](https://github.com/zavolanlab/zarp/actions?query=workflow%3Aci)\\n[![GitHub license](https://img.shields.io/github/license/zavolanlab/zarp?color=orange)](https://github.com/zavolanlab/zarp/blob/dev/LICENSE)\\n[![DOI:10.1101/2021.11.18.469017](http://img.shields.io/badge/DOI-10.1101/2021.11.18.469017-B31B1B.svg)](https://doi.org/10.1101/2021.11.18.469017)\\n\\n\\n<div align=\"left\">\\n    <img width=\"20%\" align=\"left\" src=https://raw.githubusercontent.com/zavolanlab/zarp/2bdf65deae5d4ffacc4b1a600d7d9ed425614255/images/zarp_logo.svg>\\n</div> \\n\\n\\n# **ZARP** ([Zavolan-Lab](https://www.biozentrum.unibas.ch/research/researchgroups/overview/unit/zavolan/research-group-mihaela-zavolan/) Automated RNA-Seq Pipeline) \\n...is a generic RNA-Seq analysis workflow that allows \\nusers to process and analyze Illumina short-read sequencing libraries with minimum effort. The workflow relies on \\npublicly available bioinformatics tools and currently handles single or paired-end stranded bulk RNA-seq data.\\nThe workflow is developed in [Snakemake](https://snakemake.readthedocs.io/en/stable/), a widely used workflow management system in the bioinformatics\\ncommunity.\\n\\nAccording to the current ZARP implementation, reads are analyzed (pre-processed, aligned, quantified) with state-of-the-art\\ntools to give meaningful initial insights into the quality and composition of an RNA-Seq library, reducing hands-on time for bioinformaticians and giving experimentalists the possibility to rapidly assess their data. Additional reports summarise the results of the individual steps and provide useful visualisations.\\n\\n\\n> **Note:** For a more detailed description of each step, please refer to the [workflow\\n> documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md).\\n\\n\\n## Requirements\\n\\nThe workflow has been tested on:\\n- CentOS 7.5\\n- Debian 10\\n- Ubuntu 16.04, 18.04\\n\\n> **NOTE:**\\n> Currently, we only support **Linux** execution. \\n\\n\\n# Installation\\n\\n## 1. Clone the repository\\n\\nGo to the desired directory/folder on your file system, then clone/get the \\nrepository and move into the respective directory with:\\n\\n```bash\\ngit clone https://github.com/zavolanlab/zarp.git\\ncd zarp\\n```\\n\\n## 2. Conda and Mamba installation\\n\\nWorkflow dependencies can be conveniently installed with the [Conda](http://docs.conda.io/projects/conda/en/latest/index.html)\\npackage manager. We recommend that you install [Miniconda](https://docs.conda.io/en/latest/miniconda.html) \\nfor your system (Linux). Be sure to select Python 3 option. \\nThe workflow was built and tested with `miniconda 4.7.12`.\\nOther versions are not guaranteed to work as expected.\\n\\nGiven that Miniconda has been installed and is available in the current shell the first\\ndependency for ZARP is the [Mamba](https://github.com/mamba-org/mamba) package manager, which needs to be installed in\\nthe `base` conda environment with:\\n\\n```bash\\nconda install mamba -n base -c conda-forge\\n```\\n\\n## 3. Dependencies installation\\n\\nFor improved reproducibility and reusability of the workflow,\\neach individual step of the workflow runs either in its own [Singularity](https://sylabs.io/singularity/)\\ncontainer or in its own [Conda](http://docs.conda.io/projects/conda/en/latest/index.html) virtual environemnt. \\nAs a consequence, running this workflow has very few individual dependencies. \\nThe **container execution** requires Singularity to be installed on the system where the workflow is executed. \\nAs the functional installation of Singularity requires root privileges, and Conda currently only provides Singularity\\nfor Linux architectures, the installation instructions are slightly different depending on your system/setup:\\n\\n### For most users\\n\\nIf you do *not* have root privileges on the machine you want\\nto run the workflow on *or* if you do not have a Linux machine, please [install\\nSingularity](https://sylabs.io/guides/3.5/admin-guide/installation.html) separately and in privileged mode, depending\\non your system. You may have to ask an authorized person (e.g., a systems\\nadministrator) to do that. This will almost certainly be required if you want\\nto run the workflow on a high-performance computing (HPC) cluster. \\n\\n> **NOTE:**\\n> The workflow has been tested with the following Singularity versions:  \\n>  * `v2.6.2`\\n>  * `v3.5.2`\\n\\nAfter installing Singularity, install the remaining dependencies with:\\n```bash\\nmamba env create -f install/environment.yml\\n```\\n\\n\\n### As root user on Linux\\n\\nIf you have a Linux machine, as well as root privileges, (e.g., if you plan to\\nrun the workflow on your own computer), you can execute the following command\\nto include Singularity in the Conda environment:\\n\\n```bash\\nmamba env update -f install/environment.root.yml\\n```\\n\\n## 4. Activate environment\\n\\nActivate the Conda environment with:\\n\\n```bash\\nconda activate zarp\\n```\\n\\n# Extra installation steps (optional)\\n\\n## 5. Non-essential dependencies installation\\n\\nMost tests have additional dependencies. If you are planning to run tests, you\\nwill need to install these by executing the following command _in your active\\nConda environment_:\\n\\n```bash\\nmamba env update -f install/environment.dev.yml\\n```\\n\\n## 6. Successful installation tests\\n\\nWe have prepared several tests to check the integrity of the workflow and its\\ncomponents. These can be found in subdirectories of the `tests/` directory. \\nThe most critical of these tests enable you to execute the entire workflow on a \\nset of small example input files. Note that for this and other tests to complete\\nsuccessfully, [additional dependencies](#installing-non-essential-dependencies) \\nneed to be installed. \\nExecute one of the following commands to run the test workflow \\non your local machine:\\n* Test workflow on local machine with **Singularity**:\\n```bash\\nbash tests/test_integration_workflow/test.local.sh\\n```\\n* Test workflow on local machine with **Conda**:\\n```bash\\nbash tests/test_integration_workflow_with_conda/test.local.sh\\n```\\nExecute one of the following commands to run the test workflow \\non a [Slurm](https://slurm.schedmd.com/documentation.html)-managed high-performance computing (HPC) cluster:\\n\\n* Test workflow with **Singularity**:\\n\\n```bash\\nbash tests/test_integration_workflow/test.slurm.sh\\n```\\n* Test workflow with **Conda**:\\n\\n```bash\\nbash tests/test_integration_workflow_with_conda/test.slurm.sh\\n```\\n\\n> **NOTE:** Depending on the configuration of your Slurm installation you may\\n> need to adapt file `slurm-config.json` (located directly under `profiles`\\n> directory) and the arguments to options `--cores` and `--jobs`\\n> in the file `config.yaml` of a respective profile.\\n> Consult the manual of your workload manager as well as the section of the\\n> Snakemake manual dealing with [profiles].\\n\\n# Running the workflow on your own samples\\n\\n1. Assuming that your current directory is the repository\\'s root directory,\\ncreate a directory for your workflow run and move into it with:\\n\\n    ```bash\\n    mkdir config/my_run\\n    cd config/my_run\\n    ```\\n\\n2. Create an empty sample table and a workflow configuration file:\\n\\n    ```bash\\n    touch samples.tsv\\n    touch config.yaml\\n    ```\\n\\n3. Use your editor of choice to populate these files with appropriate\\nvalues. Have a look at the examples in the `tests/` directory to see what the\\nfiles should look like, specifically:\\n\\n    - [samples.tsv](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/samples.tsv)\\n    - [config.yaml](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/config.yaml)\\n\\n    - For more details and explanations, refer to the [pipeline-documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md)\\n\\n\\n4. Create a runner script. Pick one of the following choices for either local\\nor cluster execution. Before execution of the respective command, you need to\\nremember to update the argument of the `--singularity-args` option of a\\nrespective profile (file: `profiles/{profile}/config.yaml`) so that\\nit contains a comma-separated list of _all_ directories\\ncontaining input data files (samples and any annotation files etc) required for\\nyour run.\\n\\n    Runner script for _local execution_:\\n\\n    ```bash\\n    cat << \"EOF\" > run.sh\\n    #!/bin/bash\\n\\n    snakemake \\\\\\n        --profile=\"../../profiles/local-singularity\" \\\\\\n        --configfile=\"config.yaml\"\\n\\n    EOF\\n    ```\\n\\n    **OR**\\n\\n    Runner script for _Slurm cluster exection_ (note that you may need\\n    to modify the arguments to `--jobs` and `--cores` in the file:\\n    `profiles/slurm-singularity/config.yaml` depending on your HPC\\n    and workload manager configuration):\\n\\n    ```bash\\n    cat << \"EOF\" > run.sh\\n    #!/bin/bash\\n    mkdir -p logs/cluster_log\\n    snakemake \\\\\\n        --profile=\"../profiles/slurm-singularity\" \\\\\\n        --configfile=\"config.yaml\"\\n    EOF\\n    ```\\n\\n    When running the pipeline with *conda* you should use `local-conda` and\\n    `slurm-conda` profiles instead.\\n\\n5. Start your workflow run:\\n\\n    ```bash\\n    bash run.sh\\n    ```\\n\\n# Sample downloads from SRA\\n\\nAn independent Snakemake workflow `workflow/rules/sra_download.smk` is included\\nfor the download of SRA samples with [sra-tools].\\n\\n> Note: as of Snakemake 7.3.1, only profile conda is supported. \\n> Singularity fails because the *sra-tools* Docker container only has `sh` \\nbut `bash` is required.\\n\\n> Note: The workflow uses the implicit temporary directory \\nfrom snakemake, which is called with [resources.tmpdir].\\n\\nThe workflow expects the following config:\\n* `samples`, a sample table (tsv) with column *sample* containing *SRR* identifiers,\\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv).\\n* `outdir`, an output directory\\n* `samples_out`, a pointer to a modified sample table with location of fastq files\\n* `cluster_log_dir`, the cluster log directory.\\n\\nFor executing the example one can use the following\\n(with activated *zarp* environment):\\n\\n```bash\\nsnakemake --snakefile=\"workflow/rules/sra_download.smk\" \\\\\\n          --profile=\"profiles/local-conda\" \\\\\\n          --config samples=\"tests/input_files/sra_samples.tsv\" \\\\\\n                   outdir=\"results/sra_downloads\" \\\\\\n                   samples_out=\"results/sra_downloads/sra_samples.out.tsv\" \\\\\\n                   log_dir=\"logs\" \\\\\\n                   cluster_log_dir=\"logs/cluster_log\"\\n```\\nAfter successful execution, `results/sra_downloads/sra_samples.out.tsv` should contain:\\n```tsv\\nsample\\tfq1\\tfq2\\nSRR18552868\\tresults/sra_downloads/SRR18552868/SRR18552868.fastq.gz\\t\\nSRR18549672\\tresults/sra_downloads/SRR18549672/SRR18549672_1.fastq.gz\\tresults/sra_downloads/SRR18549672/SRR18549672_2.fastq.gz\\n```\\n\\n\\n# Metadata completion with HTSinfer\\nAn independent Snakemake workflow `workflow/rules/htsinfer.smk` that populates the `samples.tsv` required by ZARP with the sample specific parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size`. Those parameters are inferred from the provided `fastq.gz` files by [HTSinfer](https://github.com/zavolanlab/htsinfer).\\n\\n> Note: The workflow uses the implicit temporary directory \\nfrom snakemake, which is called with [resources.tmpdir].\\n\\n\\nThe workflow expects the following config:\\n* `samples`, a sample table (tsv) with column *sample* containing sample identifiers, as well as columns *fq1* and *fq2* containing the paths to the input fastq files\\nsee example [here](https://github.com/zavolanlab/zarp/blob/main/tests/input_files/sra_samples.tsv). If the table contains further ZARP compatible columns (see [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table)), the values specified there by the user are given priority over htsinfer\\'s results. \\n* `outdir`, an output directory\\n* `samples_out`, path to a modified sample table with inferred parameters\\n* `records`, set to 100000 per default\\n  \\nFor executing the example one can use the following\\n(with activated *zarp* environment):\\n```bash\\ncd tests/test_htsinfer_workflow\\nsnakemake \\\\\\n    --snakefile=\"../../workflow/rules/htsinfer.smk\" \\\\\\n    --restart-times=0 \\\\\\n    --profile=\"../../profiles/local-singularity\" \\\\\\n    --config outdir=\"results\" \\\\\\n             samples=\"../input_files/htsinfer_samples.tsv\" \\\\\\n             samples_out=\"samples_htsinfer.tsv\" \\\\\\n    --notemp \\\\\\n    --keep-incomplete\\n```\\n\\nHowever, this call will exit with an error, as not all parameters can be inferred from the example files. The argument `--keep-incomplete` makes sure the `samples_htsinfer.tsv` file can nevertheless be inspected. \\n\\nAfter successful execution - if all parameters could be either inferred or were specified by the user - `[OUTDIR]/[SAMPLES_OUT]` should contain a populated table with parameters `seqmode`, `f1_3p`, `f2_3p`, `organism`, `libtype` and `index_size` for all input samples as described in the [pipeline documentation](https://github.com/zavolanlab/zarp/blob/main/pipeline_documentation.md#read-sample-table).\\n\\n'\n",
      " 'The containerised pipeline for profiling shotgun metagenomic data is derived from the [MGnify](https://www.ebi.ac.uk/metagenomics/) pipeline raw-reads analyses, a well-established resource used for analyzing microbiome data.\\nKey components:\\n- Quality control and decontamination\\n- rRNA and ncRNA detection using Rfam database\\n- Taxonomic classification of SSU and LSU regions \\n- Abundance analysis with mOTUs'\n",
      " \"# RASflow: RNA-Seq Analysis Snakemake Workflow\\nRASflow is a modular, flexible and user-friendly RNA-Seq analysis workflow. \\n\\nRASflow can be applied to both model and non-model organisms. It supports mapping RNA-Seq raw reads to both genome and transcriptome (can be downloaded from public database or can be homemade by users) and it can do both transcript- and gene-level Differential Expression Analysis (DEA) when transcriptome is used as mapping reference. It requires little programming skill for basic use. If you're good at programming, you can do more magic with RASflow!\\n\\nYou can help support RASflow by citing our publication:\\n\\n**Zhang, X., Jonassen, I. RASflow: an RNA-Seq analysis workflow with Snakemake. BMC Bioinformatics 21, 110 (2020). https://doi.org/10.1186/s12859-020-3433-x**\\n\"\n",
      " '\\n\\n# MoMofy\\nModule for integrative Mobilome prediction\\n\\n<img src=\"media/momofy_schema.png\" width=\"500\"/>\\n\\nBacteria can acquire genetic material through horizontal gene transfer, allowing them to rapidly adapt to changing environmental conditions. These mobile genetic elements can be classified into three main categories: plasmids, phages, and integrons. Autonomous elements are those capable of excising themselves from the chromosome, reintegrating elsewhere, and potentially modifying the host\\'s physiology. Small integrative elements like insertion sequences usually contain one or two genes and are frequently present in multiple copies in the genome, whereas large elements like integrative conjugative elements, often carry multiple cargo genes. The acquisition of large mobile genetic elements may provide genes for defence against other mobile genetic elements or impart new metabolic capabilities to the host.\\n\\nMoMofy is a wraper that integrates the ouptput of different tools designed for the prediction of autonomous integrative mobile genetic elements in prokaryotic genomes and metagenomes. \\n\\n## Contents\\n- [ Workflow ](#wf)\\n- [ Setup ](#sp)\\n- [ MoMofy install and dependencies ](#install)\\n- [ Usage ](#usage)\\n- [ Inputs ](#in)\\n- [ Outputs ](#out)\\n- [ Tests ](#test)\\n- [ Performance ](#profile)\\n- [ Citation ](#cite)\\n\\n\\n<a name=\"wf\"></a>\\n## Workflow\\n\\n<img src=\"media/momofy_workflow.png\" width=\"1400\"/>\\n\\n\\n<a name=\"sp\"></a>\\n## Setup\\n\\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses Singularity containers making installation trivial and results highly reproducible.\\nExplained in this section section, there is one manual step required to build the singularity image for [ICEfinder](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/index.php), as we can\\'t distribute that software due to license issues.\\n\\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\\n\\n<a name=\"install\"></a>\\n## MoMofy install and dependencies\\n\\nTo install MoMofy, clone this repo by:\\n\\n```bash\\n$ git clone https://github.com/EBI-Metagenomics/momofy.git\\n```\\n\\nThe mobileOG-database is required to run an extra step of annotation on the mobilome coding sequences. The first time you run MoMofy, you will need to download the [Beatrix 1.6 v1](https://mobileogdb.flsi.cloud.vt.edu/entries/database_download) database, move the tarball to `/PATH/momofy/databases`, decompress it, and run the script to format the db for diamond:\\n\\n```bash\\n$ mv beatrix-1-6_v1_all.zip /PATH/momofy/databases\\n$ cd /PATH/momofy/databases\\n$ unzip beatrix-1-6_v1_all.zip\\n$ nextflow run /PATH/momofy/format_mobileOG.nf\\n```\\n\\nMost of the tools are available on [quay.io](https://quay.io) and no install is needed. \\n\\nIn the case of ICEfinder, you will need to contact the author to get a copy of the software, visit the [ICEfinder website](https://bioinfo-mml.sjtu.edu.cn/ICEfinder/download.html) for more information. Once you have the `ICEfinder_linux.tar.gz` tarball, move it to `momofy/templates` and build the singularity image using the following command:\\n\\n```bash\\n$ mv ICEfinder_linux.tar.gz /PATH/momofy/templates/\\n$ cd /PATH/momofy/templates/\\n$ sudo singularity build ../../singularity/icefinder-v1.0-local.sif icefinder-v1.0-local.def\\n```\\n\\nPaliDIS is an optional step on the workflow and the install is optional as well. Visit [PaliDIS repo](https://github.com/blue-moon22/PaliDIS) for installing instructions.\\n\\nIf you are aim to run the pipeline in a system with jobs scheduler as LSF or SGE, set up a config file and provide it as part of the arguments as follows:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --assembly contigs.fasta -c /PATH/configs/some_cluster.config\\n```\\n\\nYou can find an example in the `configs` directory of this repo.\\n\\n\\n<a name=\"usage\"></a>\\n## Usage\\n\\nRunning the tool with `--help` option will display the following message:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --help\\nN E X T F L O W  ~  version 21.10.0\\nLaunching `momofy.nf` [gigantic_pare] - revision: XXXXX\\n\\n\\tMoMofy is a wraper that integrates the ouptput of different tools designed for the prediction of autonomous integrative mobile genetic elements in prokaryotic genomes and metagenomes.\\n\\n        Usage:\\n         The basic command for running the pipeline is as follows:\\n\\n         nextflow run momofy.nf --assembly contigs.fasta\\n\\n         Mandatory arguments:\\n          --assembly                     (Meta)genomic assembly in fasta format (uncompress)\\n\\n         Optional arguments:\\n          --user_genes                    User annotation files. See --prot_fasta and --prot_gff [ default = false ]\\n          --prot_gff                      Annotation file in GFF3 format. Mandatory with --user_genes true\\n          --prot_fasta                    Fasta file of aminoacids. Mandatory with --user_genes true\\n          --palidis                       Incorporate PaliDIS predictions to final output [ default = false ]\\n          --palidis_fasta                 Fasta file of PaliDIS insertion sequences. Mandatory with --palidis true\\n          --palidis_info                  Information file of PaliDIS insertion sequences. Mandatory with --palidis true\\n          --gff_validation                Run a step of format validation on the GFF3 file output [ default = true ]\\n          --outdir                        Output directory to place final MoMofy results [ default = MoMofy_results ]\\n          --help                          This usage statement [ default = false ]\\n```\\n\\n<a name=\"in\"></a>\\n## Inputs\\n\\nTo run MoMofy in multiple samples, create a directory per sample and launch the tool from the sample directory. The only mandatory input is the (meta)genomic assembly file in fasta format (uncompress).\\n\\nBasic run:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --assembly contigs.fasta\\n```\\n\\nNote that the final output in gff format is created by adding information to PROKKA output. If you have your own protein prediction files, provide the gff and the fasta file of amino acid sequences (both uncompressed files are mandatory with this option). These files will be used for Diamond annotation and CDS coordinates mapping to the MGEs boundaries. If any original annotation is present in the gff file, it will remained untouched.\\n\\nRunning MoMofy with user\\'s genes prediction:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --assembly contigs.fasta \\\\\\n    --user_genes true \\\\\\n    --prot_fasta proteins.faa \\\\\\n    --prot_gff annotation.gff \\\\\\n```\\n\\nIf you want to incorporate PaliDIS predictions to the final output, provide the path of the two outputs of PaliDIS (fasta file of insertion sequences and the information for each insertion sequence file).\\n\\nTo run MoMofy incorporating PaliDIS results:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --assembly contigs.fasta \\\\\\n    --palidis true \\\\\\n    --palidis_fasta insertion_sequences.fasta \\\\\\n    --palidis_info insertion_sequences_info.txt \\\\\\n```\\n\\nThen, if you have protein files and PaliDIS outputs, you can run:\\n\\n```bash\\n$ nextflow run /PATH/momofy/momofy.nf --assembly contigs.fasta \\\\\\n    --user_genes true \\\\\\n    --prot_fasta proteins.faa \\\\\\n    --prot_gff annotation.gff \\\\\\n    --palidis true \\\\\\n    --palidis_fasta insertion_sequences.fasta \\\\\\n    --palidis_info insertion_sequences_info.txt \\\\\\n```\\n\\nA GFF validation process is used to detect formatting errors in the final GFF3 output. This process can be skipped adding `--gff_validation false` to the command.\\n\\n\\n<a name=\"out\"></a>\\n## Outputs\\n\\nResults will be written by default in the `MoMofy_results` directory inside the sample dir unless the user define `--outdir` option. There you will find the following output files:\\n\\n```bash\\nMoMofy_results/\\n├── discarded_mge.txt\\n├── momofy_predictions.fna\\n├── momofy_predictions.gff\\n└── nested_integrons.txt\\n```\\n\\nThe main MoMofy output files are the `momofy_predictions.fna` containing the nucleotide sequences of every prediction, and the `momofy_predictions.gff` containing the mobilome annotation plus any other feature annotated by PROKKA or in the gff file provided by the user with the option `--user_genes`. The labels used in the Type column of the gff file corresponds to the following nomenclature according to the [Sequence Ontology resource](http://www.sequenceontology.org/browser/current_svn/term/SO:0000001):\\n\\n| Type in gff file  | Sequence ontology ID | Element description | Reporting tool |\\n| ------------- | ------------- | ------------- | ------------- |\\n| insertion_sequence | [SO:0000973](http://www.sequenceontology.org/browser/current_svn/term/SO:0000973) | Insertion sequence | ISEScan, PaliDIS |\\n| terminal_inverted_repeat_element | [SO:0000481](http://www.sequenceontology.org/browser/current_svn/term/SO:0000481) | Terminal Inverted Repeat (TIR) flanking insertion sequences | ISEScan, PaliDIS |\\n| integron  | [SO:0000365](http://www.sequenceontology.org/browser/current_svn/term/SO:0000365) | Integrative mobilizable element | IntegronFinder, ICEfinder |\\n| attC_site | [SO:0000950](http://www.sequenceontology.org/browser/current_svn/term/SO:0000950) | Integration site of DNA integron | IntegronFinder |\\n| conjugative_transposon  | [SO:0000371](http://www.sequenceontology.org/browser/current_svn/term/SO:0000371) | Integrative Conjugative Element | ICEfinder |\\n| direct_repeat | [SO:0000314](http://www.sequenceontology.org/browser/current_svn/term/SO:0000371) | Flanking regions on mobilizable elements | ICEfinder |\\n| CDS | [SO:0000316](http://www.sequenceontology.org/browser/current_svn/term/SO:0000316) | Coding sequence | Prodigal |\\n\\n\\nThe file `discarded_mge.txt` contains a list of predictions that were discarded, along with the reason for their exclusion. Possible reasons include:\\n\\n1. overlapping\\tFor insertion sequences only, ISEScan prediction is discarded if an overlap with PaliDIS is found. \\n2. mge<500bp\\tDiscarded by length.\\n3. no_cds\\tIf there are no genes encoded in the prediction.\\n\\nThe file `nested_integrons.txt` is a report of overlapping predictions reported by IntegronFinder and ICEfinder. No predictions are discarded in this case.\\n\\nAdditionally, you will see the directories containing the main outputs of each tool.\\n\\n<a name=\"test\"></a>\\n## Tests\\n\\nNextflow tests are executed with [nf-test](https://github.com/askimed/nf-test). It takes around 3 min in executing.\\n\\nRun:\\n\\n```bash\\n$ cd /PATH/momofy\\n$ nf-test test *.nf.test\\n```\\n\\n<a name=\"profile\"></a>\\n## Performance\\n\\nMoMofy performance was profiled using 460 public metagenomic assemblies and co-assemblies of chicken gut (ERP122587, ERP125074, and ERP131894) with sizes ranging from ~62 K to ~893 M assembled bases. We used the metagenomic assemblies, CDS prediction and annotation files generated by MGnify v5 pipeline, and PaliDIS outputs generated after downsampling the number of reads to 10 M. MoMofy was run adding the following options: `-with-report -with-trace -with-timeline timeline.out`.\\n\\n\\n<p align=\"center\" width=\"100%\">\\n   <img src=\"media/run_time.png\" width=\"40%\"/>\\n</p>\\n<p align=\"center\" width=\"100%\">\\n   <img src=\"media/peak_rss.png\" width=\"40%\"/>\\n   <img src=\"media/peak_vmem.png\" width=\"40%\"/>\\n</p>\\n\\n\\n<a name=\"cite\"></a>\\n## Citation\\n\\nIf you use MoMofy on your data analysis, please cite:\\n\\nXXXXX\\n\\n\\nMoMofy is a wrapper that integrates the output of the following tools and DBs:\\n\\n1) ISEScan v1.7.2.3 [Xie et al., Bioinformatics, 2017](https://doi.org/10.1093/bioinformatics/btx433)\\n2) IntegronFinder2 v2.0.2 [Néron et al., Microorganisms, 2022](https://doi.org/10.3390/microorganisms10040700)\\n3) ICEfinder v1.0 [Liu et al., Nucleic Acids Research, 2019](https://doi.org/10.1093/nar/gky1123)\\n4) PaliDIS [Carr et al., biorxiv, 2022](https://doi.org/10.1101/2022.06.27.497710)\\n\\nDatabases:\\n- MobileOG-DB Beatrix 1.6 v1 [Brown et al., Appl Environ Microbiol, 2022](https://doi.org/10.1128/aem.00991-22)\\n'\n",
      " '# BatchConvert  ![DOI:10.5281](https://zenodo.org/badge/doi/10.5281/zenodo.7955974.svg)\\n\\nA command line tool for converting image data into either of the standard file formats OME-TIFF or OME-Zarr. \\n\\nThe tool wraps the dedicated file converters bfconvert and bioformats2raw to convert into OME-TIFF or OME-Zarr,\\nrespectively. The workflow management system NextFlow is used to perform conversion in parallel for batches of images. \\n\\nThe tool also wraps s3 and Aspera clients (go-mc and aspera-cli, respectively). Therefore, input and output locations can \\nbe specified as local or remote storage and file transfer will be performed automatically. The conversion can be run on \\nHPC with Slurm.  \\n\\n![](figures/diagram.png)\\n\\n## Installation & Dependencies\\n\\n**Important** note: The package has been so far only tested on Ubuntu 20.04.\\n\\nThe minimal dependency to run the tool is NextFlow, which should be installed and made accessible from the command line.\\n\\nIf conda exists on your system, you can install BatchConvert together with NextFlow using the following script:\\n```\\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\\\ \\nsource BatchConvert/installation/install_with_nextflow.sh\\n```\\n\\n\\nIf you already have NextFlow installed and accessible from the command line (or if you prefer to install it manually \\ne.g., as shown [here](https://www.nextflow.io/docs/latest/getstarted.html)), you can also install BatchConvert alone, using the following script:\\n```\\ngit clone https://github.com/Euro-BioImaging/BatchConvert.git && \\\\ \\nsource BatchConvert/installation/install.sh\\n```\\n\\n\\nOther dependencies (which will be **automatically** installed):\\n- bioformats2raw (entrypoint bioformats2raw)\\n- bftools (entrypoint bfconvert)\\n- go-mc (entrypoint mc)\\n- aspera-cli (entrypoint ascp)\\n\\nThese dependencies will be pulled and cached automatically at the first execution of the conversion command. \\nThe mode of dependency management can  be specified by using the command line option ``--profile`` or `-pf`. Depending \\non how this option is specified, the dependencies will be acquired / run either via conda or via docker/singularity containers. \\n\\nSpecifying ``--profile conda`` (default) will install the dependencies to an \\nenvironment at ``./.condaCache`` and use this environment to run the workflow. This option \\nrequires that miniconda/anaconda is installed on your system.    \\n\\nAlternatively, specifying ``--profile docker`` or ``--profile singularity`` will pull a docker or \\nsingularity image with the dependencies, respectively, and use this image to run the workflow.\\nThese options assume that the respective container runtime (docker or singularity) is available on \\nyour system. If singularity is being used, a cache directory will be created at the path \\n``./.singularityCache`` where the singularity image is stored. \\n\\nFinally, you can still choose to install the dependencies manually and use your own installations to run\\nthe workflow. In this case, you should specify ``--profile standard`` and make sure the entrypoints\\nspecified above are recognised by your shell.  \\n\\n\\n## Configuration\\n\\nBatchConvert can be configured to have default options for file conversion and transfer. Probably, the most important sets of parameters\\nto be configured include credentials for the remote ends. The easiest way to configure remote stores is by running the interactive \\nconfiguration command as indicated below.\\n\\n### Configuration of the s3 object store\\n\\nRun the interactive configuration command: \\n\\n`batchconvert configure_s3_remote`\\n\\nThis will start a sequence of requests for s3 credentials such as name, url, access, etc. Provide each requested credential and click\\nenter. Continue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should roughly look like this:\\n\\n```\\noezdemir@pc-ellenberg108:~$ batchconvert configure_s3_remote\\nenter remote name (for example s3)\\ns3\\nenter url:\\nhttps://s3.embl.de\\nenter access key:\\n\"your-access-key\"\\nenter secret key:\\n\"your-secret-key\"\\nenter bucket name:\\n\"your-bucket\"\\nConfiguration of the default s3 credentials is complete\\n```\\n\\n\\n### Configuration of the BioStudies user space\\n\\nRun the interactive configuration command: \\n\\n`batchconvert configure_bia_remote`\\n\\nThis will prompt a request for the secret directory to connect to. Enter the secret directory for your user space and click enter. \\nUpon completing the configuration, the sequence of commands should roughly look like this:\\n\\n```\\noezdemir@pc-ellenberg108:~$ batchconvert configure_bia_remote\\nenter the secret directory for BioImage Archive user space:\\n\"your-secret-directory\"\\nconfiguration of the default bia credentials is complete\\n```\\n\\n### Configuration of the slurm options\\n\\nBatchConvert can also run on slurm clusters. In order to configure the slurm parameters, run the interactive configuration command: \\n\\n`batchconvert configure_slurm`\\n\\nThis will start a sequence of requests for slurm options. Provide each requested option and click enter. \\nContinue this cycle until the process is finished. Upon completing the configuration, the sequence of commands should \\nroughly look like this:\\n\\n```\\noezdemir@pc-ellenberg108:~$ batchconvert configure_slurm\\nPlease enter value for queue_size\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the current value ´50´\\ns\\nPlease enter value for submit_rate_limit\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the current value ´10/2min´\\ns\\nPlease enter value for cluster_options\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the current value ´--mem-per-cpu=3140 --cpus-per-task=16´\\ns\\nPlease enter value for time\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the current value ´6h´\\ns\\nconfiguration of the default slurm parameters is complete\\n```\\n\\n### Configuration of the default conversion parameters\\n\\nWhile all conversion parameters can be specified as command line arguments, it can\\nbe useful for the users to set their own default parameters to avoid re-entering those\\nparameters for subsequent executions. BatchConvert allows for interactive configuration of \\nconversion in the same way as configuration of the remote stores described above.\\n\\nTo configure the conversion into OME-TIFF, run the following command:\\n\\n`batchconvert configure_ometiff`\\n\\nThis will prompt the user to enter a series of parameters, which will then be saved as the \\ndefault parameters to be passed to the `batchconvert ometiff` command. Upon completing the \\nconfiguration, the sequence of commands should look similar to:\\n\\n```\\noezdemir@pc-ellenberg108:~$ batchconvert configure_ometiff\\nPlease enter value for noflat\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bfconvert defaults\"\\ns\\nPlease enter value for series\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bfconvert defaults\"\\ns\\nPlease enter value for timepoint\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bfconvert defaults\"\\ns\\n...\\n...\\n...\\n...\\n...\\n...\\nConfiguration of the default parameters for \\'bfconvert\\' is complete\\n```\\n\\n\\nTo configure the conversion into OME-Zarr, run the following command:\\n\\n`batchconvert configure_omezarr`\\n\\nSimilarly, this will prompt the user to enter a series of parameters, which will then be saved as the \\ndefault parameters to be passed to the `batchconvert omezarr` command. Upon completing the configuration, \\nthe sequence of commands should look similar to:\\n\\n```\\noezdemir@pc-ellenberg108:~$ batchconvert configure_omezarr\\nPlease enter value for resolutions_zarr\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bioformats2raw defaults\"\\ns\\nPlease enter value for chunk_h\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bioformats2raw defaults\"\\ns\\nPlease enter value for chunk_w\\nClick enter if this parameter is not applicable\\nEnter \"skip\" or \"s\" if you would like to keep the parameter´s current value, which is \"bioformats2raw defaults\"\\n...\\n...\\n...\\n...\\n...\\n...\\nConfiguration of the default parameters for \\'bioformats2raw\\' is complete\\n```\\n\\nIt is important to note that the initial defaults for the conversion parameters are the same as the defaults\\nof the backend tools bfconvert and bioformats2raw, as noted in the prompt excerpt above. Through interactive configuration, \\nthe user is overriding these initial defaults and setting their own defaults. It is possible to reset the initial\\ndefaults by running the following command.\\n\\n`batchconvert reset_defaults`\\n\\nAnother important point is that any of these configured parameters can be overridden by passing a value to that\\nparameter in the commandline. For instance, in the following command, the value of 20 will be assigned to `chunk_h` parameter \\neven if the value for the same parameter might be different in the configuration file. \\n\\n`batchconvert omezarr --chunk_h 20 \"path/to/input\" \"path/to/output\"`\\n\\n\\n## Examples\\n\\n### Local conversion\\n\\n#### Parallel conversion of files to separate OME-TIFFs / OME-Zarrs:\\nConvert a batch of images on your local storage into OME-TIFF format. \\nNote that the `input_path` in the command given below is typically a \\ndirectory with multiple image files but a single image file can also be passed:\\\\\\n`batchconvert ometiff -pf conda \"input_path\" \"output_path\"` \\n\\nNote that if this is your first conversion with the profile `conda`, \\nit will take a while for a conda environment with the dependencies to be\\ncreated. All the subsequent conversion commands with the profile `conda`,\\nhowever, will use this environment, and thus show no such delay.\\n\\nSince conda is the default profile, it does not have to be \\nexplicitly included in the command line. Thus, the command can be shortened to:\\\\\\n`batchconvert ometiff \"input_path\" \"output_path\"`\\n\\nConvert only the first channel of the images:\\\\\\n`batchconvert ometiff -chn 0 \"input_path\" \"output_path\"`\\n\\nCrop the images being converted along x and y axis by 150 pixels:\\\\\\n`batchconvert ometiff -cr 0,0,150,150 \"input_path\" \"output_path\"`\\n\\nConvert into OME-Zarr instead:\\\\\\n`batchconvert omezarr \"input_path\" \"output_path\"`\\n\\nConvert into OME-Zarr with 3 resolution levels:\\\\\\n`batchconvert omezarr -rz 3 \"input_path\" \"output_path\"`\\n\\nSelect a subset of images with a matching string such as \"mutation\":\\\\\\n`batchconvert omezarr -p mutation \"input_path\" \"output_path\"`\\n\\nSelect a subset of images using wildcards. Note that the use of \"\" around \\nthe input path is necessary when using wildcards:\\\\\\n`batchconvert omezarr \"input_path/*D3*.oir\" \"output_path\"`\\n\\nConvert by using a singularity container instead of conda environment (requires\\nsingularity to be installed on your system):\\\\\\n`batchconvert omezarr -pf singularity \"input_path/*D3*.oir\" \"output_path\"`\\n\\nConvert by using a docker container instead of conda environment (requires docker\\nto be installed on your system):\\\\\\n`batchconvert omezarr -pf docker \"input_path/*D3*.oir\" \"output_path\"`\\n\\nNote that similarly to the case with the profile `conda`, the first execution of\\na conversion with the profile `singularity` or `docker` will take a while for the\\ncontainer image to be pulled. All the subsequent conversion commands using a \\ncontainer option will use this image, and thus show no such delay. \\n\\nConvert local data and upload the output to an s3 bucket. Note that the output \\npath is created relative to the bucket specified in your s3 configuration:\\\\\\n`batchconvert omezarr -dt s3 \"input_path\" \"output_path\"`\\n\\nReceive input files from an s3 bucket, convert locally and upload the output to \\nthe same bucket. Note that wildcards cannot be used when the input is from s3. \\nUse pattern matching option `-p` for selecting a subset of input files:\\\\\\n`batchconvert omezarr -p mutation -st s3 -dt s3 \"input_path\" \"output_path\"`\\n\\nReceive input files from your private BioStudies user space and convert them locally.\\nUse pattern matching option `-p` for selecting a subset of input files:\\\\\\n`batchconvert omezarr -p mutation -st bia \"input_path\" \"output_path\"`\\n\\nReceive an input from an s3 bucket, convert locally and upload the output to your \\nprivate BioStudies user space. Use pattern matching option `-p` for selecting a subset \\nof input files:\\\\\\n`batchconvert omezarr -p mutation -st s3 -dt bia \"input_path\" \"output_path\"`\\n\\nNote that in all the examples shown above, BatchConvert treats each input file as separate,\\nstandalone data point, disregarding the possibility that some of the input files might belong to \\nthe same multidimensional array. Thus, each input file is converted to an independent \\nOME-TIFF / OME-Zarr and the number of outputs will thus equal the number of selected input files.\\nAn alternative scenario is discussed below.\\n\\n#### Parallel conversion of file groups by stacking multiple files into single OME-TIFFs / OME-Zarrs:\\n\\nWhen the flag `--merge_files` is specified, BatchConvert tries to detect which input files might \\nbelong to the same multidimensional array based on the patterns in the filenames. Then a \"grouped conversion\" \\nis performed, meaning that the files belonging to the same dataset will be incorporated into \\na single OME-TIFF / OME-Zarr series, in that files will be concatenated along specific dimension(s) \\nduring the conversion. Multiple file groups in the input directory can be detected and converted \\nin parallel. \\n\\nThis feature uses Bio-Formats\\'s pattern files as described [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\\nHowever, BatchConvert generates pattern files automatically, allowing the user to directly use the \\ninput directory in the conversion command. BatchConvert also has the option of specifying the \\nconcatenation axes in the command line, which is especially useful in cases where the filenames \\nmay not contain dimension information.  \\n\\nTo be able to use the `--merge files` flag, the input file names must obey certain rules:\\n1. File names in the same group must be uniform, except for one or more **numeric field(s)**, which\\nshould show incremental change across the files. These so-called **variable fields** \\nwill be detected and used as the dimension(s) of concatenation.\\n2. The length of variable fields must be uniform within the group. For instance, if the\\nvariable field has values reaching multi-digit numbers, leading \"0\"s should be included where needed \\nin the file names to make the variable field length uniform within the group.\\n3. Typically, each variable field should follow a dimension specifier. What patterns can be used as \\ndimension specifiers are explained [here](https://docs.openmicroscopy.org/bio-formats/6.6.0/formats/pattern-file.html).\\nHowever, BatchConvert also has the option `--concatenation_order`, which allows the user to\\nspecify from the command line, the dimension(s), along which the files must be concatenated.\\n4. File names that are unique and cannot be associated with any group will be assumed as\\nstandalone images and converted accordingly. \\n\\nBelow are some examples of grouped conversion commands in the context of different possible use-case scenarios:\\n\\n**Example 1:**\\n\\nThis is an example of a folder with non-uniform filename lengths:\\n```\\ntime-series/test_img_T2\\ntime-series/test_img_T4\\ntime-series/test_img_T6\\ntime-series/test_img_T8\\ntime-series/test_img_T10\\ntime-series/test_img_T12\\n```\\nIn this example, leading zeroes are missing in the variable fields of some filenames. \\nA typical command to convert this folder to a single OME-TIFF would look like: \\\\\\n`batchconvert --ometiff --merge_files \"input_dir/time-series\" \"output_path\"`\\n\\nHowever, this command would fail to create a single OME-Zarr folder due to the non-uniform \\nlengths of the filenames. Instead, the files would be split into two groups based on the\\nfilename length, leading to two separate OME-Zarrs with names:\\n\\n`test_img_TRange{2-8-2}.ome.zarr` and `test_img_TRange{10-12-2}.ome.zarr`\\n\\nHere is the corrected version of the folder for the above example-\\n```\\ntime-series/test_img_T02\\ntime-series/test_img_T04\\ntime-series/test_img_T06\\ntime-series/test_img_T08\\ntime-series/test_img_T10\\ntime-series/test_img_T12\\n```\\n\\nExecuting the same command on this folder would result in a single OME-Zarr with the name:\\n`test_img_TRange{02-12-2}.ome.zarr`\\n\\n**Example 2**- \\n\\nIn this example, the filename lengths are uniform but the incrementation within the variable field is not.\\n```\\ntime-series/test_img_T2\\ntime-series/test_img_T4\\ntime-series/test_img_T5\\ntime-series/test_img_T7\\n```\\n\\nA typical command to convert this folder to a single OME-Zarr would look like: \\\\\\n`batchconvert --omezarr --merge_files \"input_dir/time-series\" \"output_path\"`\\n\\nHowever, the command would fail to assume these files as a single group due to the\\nnon-uniform incrementation in the variable field of the filenames. Instead, the dataset \\nwould be split into two groups, leading to two separate OME-Zarrs with the following names:\\n`test_img_TRange{2-4-2}.ome.zarr` and `test_img_TRange{5-7-2}.ome.zarr`  \\n\\n\\n**Example 3**\\n\\nThis is an example of a case where the conversion attempts to concatenate files along two\\ndimensions, channel and time.\\n```\\nmultichannel_time-series/test_img_C1-T1\\nmultichannel_time-series/test_img_C1-T2\\nmultichannel_time-series/test_img_C1-T3\\nmultichannel_time-series/test_img_C2-T1\\nmultichannel_time-series/test_img_C2-T2\\n```\\nTo convert this folder to a single OME-Zarr, one could try the following command: \\\\\\n`batchconvert --omezarr --merge_files \"input_dir/multichannel_time-series\" \"output_path\"`\\n\\nHowever, since the channel-2 does not have the same number of timeframes as the channel-1, \\nBatchConvert will fail to assume these two channels as part of the same series and\\nwill instead split the two channels into two separate OME-Zarrs. \\n\\nThe output would look like: \\\\\\n`test_img_C1-TRange{1-3-1}.ome.zarr` \\\\\\n`test_img_C2-TRange{1-2-1}.ome.zarr`\\n\\nTo be able to really incorporate all files into a single OME-Zarr, the folder should have equal\\nnumber of images corresponding to both channels, as shown below:\\n```\\nmultichannel_time-series/test_img_C1-T1\\nmultichannel_time-series/test_img_C1-T2\\nmultichannel_time-series/test_img_C1-T3\\nmultichannel_time-series/test_img_C2-T1\\nmultichannel_time-series/test_img_C2-T2\\nmultichannel_time-series/test_img_C2-T3\\n```\\nThe same conversion command on this version of the input folder would result in a single \\nOME-Zarr with the name: \\\\\\n`test_img_CRange{1-2-1}-TRange{1-3-1}.ome.zarr`\\n\\n\\n**Example 4**\\n\\nThis is another example of a case, where there are multiple filename patterns in the input folder.\\n\\n```\\nfolder_with_multiple_groups/test_img_C1-T1\\nfolder_with_multiple_groups/test_img_C1-T2\\nfolder_with_multiple_groups/test_img_C2-T1\\nfolder_with_multiple_groups/test_img_C2-T2\\nfolder_with_multiple_groups/test_img_T1-Z1\\nfolder_with_multiple_groups/test_img_T1-Z2\\nfolder_with_multiple_groups/test_img_T1-Z3\\nfolder_with_multiple_groups/test_img_T2-Z1\\nfolder_with_multiple_groups/test_img_T2-Z2\\nfolder_with_multiple_groups/test_img_T2-Z3\\n```\\n\\nOne can convert this folder with- \\\\\\n`batchconvert --omezarr --merge_files \"input_dir/folder_with_multiple_groups\" \"output_path\"`\\n \\nBatchConvert will detect the two patterns in this folder and perform two grouped conversions. \\nThe output folders will be named as `test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and \\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`. \\n\\n\\n**Example 5**\\n\\nNow imagine that we have the same files as in the example 4 but the filenames of the\\nfirst group lack any dimension specifier, so we have the following folder:\\n\\n```\\nfolder_with_multiple_groups/test_img_1-1\\nfolder_with_multiple_groups/test_img_1-2\\nfolder_with_multiple_groups/test_img_2-1\\nfolder_with_multiple_groups/test_img_2-2\\nfolder_with_multiple_groups/test_img_T1-Z1\\nfolder_with_multiple_groups/test_img_T1-Z2\\nfolder_with_multiple_groups/test_img_T1-Z3\\nfolder_with_multiple_groups/test_img_T2-Z1\\nfolder_with_multiple_groups/test_img_T2-Z2\\nfolder_with_multiple_groups/test_img_T2-Z3\\n```\\n\\nIn such a scenario, BatchConvert allows the user to specify the concatenation axes \\nvia `--concatenation_order` option. This option expects comma-separated strings of dimensions \\nfor each group. In this example, the user must provide a string of 2 characters, such as `ct` for \\nchannel and time, for group 1, since there are two variable fields for this group. Since group 2 \\nalready has dimension specifiers (T and Z as specified in the filenames preceding the variable fields),\\nthe user does not need to specify anything for this group, and can enter `auto` or `aa` for automatic\\ndetection of the specifiers. \\n\\nSo the following line can be used to convert this folder: \\\\\\n`batchconvert --omezarr --merge_files --concatenation_order ct,aa \"input_dir/folder_with_multiple_groups\" \"output_path\"`\\n\\nThe resulting OME-Zarrs will have the names:\\n`test_img_CRange{1-2-1}-TRange{1-2-1}.ome.zarr` and\\n`test_img_TRange{1-2-1}-ZRange{1-3-1}.ome.zarr`\\n\\nNote that `--concatenation_order` will override any dimension specifiers already\\nexisting in the filenames.\\n\\n\\n**Example 6**\\n\\nThere can be scenarios where the user may want to have further control over the axes along \\nwhich to concatenate the images. For example, the filenames might contain the data acquisition\\ndate, which can be recognised by BatchConvert as a concatenation axis in the automatic \\ndetection mode. An example of such a fileset might look like:\\n\\n```\\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T1\\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T2\\nfilenames_with_dates/test_data_date03.03.2023_imageZ1-T3\\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T1\\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T2\\nfilenames_with_dates/test_data_date03.03.2023_imageZ2-T3\\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T1\\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T2\\nfilenames_with_dates/test_data_date04.03.2023_imageZ1-T3\\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T1\\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T2\\nfilenames_with_dates/test_data_date04.03.2023_imageZ2-T3\\n```\\n\\nOne may try the following command to convert this folder:\\n\\n`batchconvert --omezarr --merge_files \"input_dir/filenames_with_dates\" \"output_path\"`\\n\\nSince the concatenation axes are not specified, this command would try to create\\na single OME-Zarr with name: `test_data_dateRange{03-04-1}.03.2023_imageZRange{1-2-1}-TRange{1-3-1}`.\\n\\nIn order to force BatchConvert to ignore the date field, the user can restrict the concatenation \\naxes to the last two numeric fields. This can be done by using a command such as: \\\\\\n`batchconvert --omezarr --merge_files --concatenation_order aa \"input_dir/filenames_with_dates\" \"output_path\"` \\\\\\nThis command will avoid concatenation along the date field, and therefore, there will be two\\nOME-Zarrs corresponding to the two dates. The number of characters being passed to the \\n`--concatenation_order` option specifies the number of numeric fields (starting from the right \\nend of the filename) that are recognised by the BatchConvert as valid concatenation axes. \\nPassing `aa`, therefore, means that the last two numeric fields must be recognised as \\nconcatenation axes and the dimension type should be automatically detected (`a` for automatic). \\nIn the same logic, one could, for example, convert each Z section into a separate OME-Zarr by \\nspecifying `--concatenation_order a`.\\n\\n\\n\\n### Conversion on slurm\\n\\nAll the examples given above can also be run on slurm by specifying `-pf cluster` option. \\nNote that this option automatically uses the singularity profile:\\\\\\n`batchconvert omezarr -pf cluster -p .oir \"input_path\" \"output_path\"`\\n\\n\\n\\n\\n\\n\\n\\n'\n",
      " \"**Assembly and quantification metatranscriptome using metagenome data**.\\n\\nVersion: see VERSION\\n\\n## Introduction\\n\\n**MetaGT** is a bioinformatics analysis pipeline used for improving and quantification \\nmetatranscriptome assembly using metagenome data. The pipeline supports Illumina sequencing \\ndata and complete metagenome and metatranscriptome assemblies. The pipeline involves the \\nalignment of metatranscriprome assembly to the metagenome assembly with further extracting CDSs,\\nwhich are covered by transcripts.\\n\\nThe pipeline is built using Nextflow, a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible. The Nextflow DSL2 implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\\n\\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg)](https://bioconda.github.io/)\\n\\n## Quick Start\\n\\n1. Install [`nextflow`](https://nf-co.re/usage/installation)\\n\\n2. Install any of [`Conda`](https://conda.io/miniconda.html) for full pipeline reproducibility \\n\\n3. Download the pipeline, e.g. by cloning metaGT GitHub repository:\\n\\n    ```bash\\n    git clone git@github.com:ablab/metaGT.git\\n    ```\\n   \\n4. Test it on a minimal dataset by running:\\n\\n    ```bash\\n    nextflow run metaGT -profile test,conda\\n    ```\\n   \\n5. Start running your own analysis!\\n    > Typical command for analysis using reads:\\n\\n    ```bash\\n    nextflow run metaGT -profile <conda> --dna_reads '*_R{1,2}.fastq.gz' --rna_reads '*_R{1,2}.fastq.gz'\\n    ```\\n    > Typical command for analysis using multiple files with reads:\\n\\n    ```bash\\n    nextflow run metaGT -profile <conda> --dna_reads '*.yaml' --rna_reads '*.yaml' --yaml\\n    ```\\n    > Typical command for analysis using assemblies:\\n\\n    ```bash\\n    nextflow run metaGT -profile <conda> --genome '*.fasta' --transcriptome '*.fasta'\\n    ```\\n## Pipeline Summary\\nOptionally, if raw reades are used:\\n\\n<!-- TODO nf-core: Fill in short bullet-pointed list of default steps of pipeline -->\\n\\n* Sequencing quality control (`FastQC`)\\n* Assembly metagenome or metatranscriptome (`metaSPAdes, rnaSPAdes `)\\n\\nBy default, the pipeline currently performs the following:\\n\\n* Annotation metagenome (`Prokka`)\\n* Aligning metatranscriptome on metagenome (`minimap2`)\\n* Annotation unaligned transcripts (`TransDecoder`)\\n* Clustering covered CDS and CDS from unaligned transcripts (`MMseqs2`)\\n* Quantifying abundances of transcripts (`kallisto`)\\n\\n## Citation\\n\\nMetaGT was developed by Daria Shafranskaya and Andrey Prjibelski.\\nIf you use it in your research please cite:\\n\\n[MetaGT: A pipeline for de novo assembly of metatranscriptomes with the aid of metagenomic data](https://doi.org/10.3389/fmicb.2022.981458)\\n\\n## Feedback and bug report\\n\\nIf you have any questions, please leave an issue at out [GitHub page](https://github.com/ablab/metaGT/issues).\\n\"\n",
      " \"# WRF/EMEP Linear Workflow\\n\\nExample Common Workflow Language (CWL) workflow and tool descriptors for running the \\nWeather Research and Forecase (WRF) and EMEP models.\\n\\nThis workflow is designed for a single model domain. Example datasets for testing this \\nworkflow can be downloaded from Zenodo.\\n\\n\\n## Requirements:\\n\\n* docker or singularity\\n* conda\\n* cwltool\\n* Toil - optional, useful for running on HPC or distributed computing systems\\n\\n### CWL / Toil Installation:\\n\\nThe workflow runner (either cwltool, or Toil) can be installed using either conda or pip.\\nEnvironment files for conda are included, and can be used as shown below:\\n* cwltool only:\\n  * `conda env create --file install/env_cwlrunner.yml --name cwl`\\n* Toil & cwltool:\\n  * `conda env create --file install/env_toil.yml --name toil`\\n\\n### Setup for Example Workflow\\n\\n* Download the example dataset from Zenodo: https://doi.org/10.5281/zenodo.7817216\\n* Extract into the `input_files` directory:\\n  * `tar -zxvf wrf_emep_UK_example_inputs.tar.gz -C input_files --strip-components=1`\\n\\n## Running the Workflow\\n\\nThe full workflow is broken into several logical steps:\\n1. ERA5 download\\n2. WPS 1st step: Geogrid geography file creation\\n3. WPS process: ungribbing of ERA5 data, and running of metgrid to produce meteorology files.\\n4. WRF process: generation of WRF input files by REAL, and running of WRF model\\n5. EMEP model: running of EMEP chemistry and transport model\\n\\nSteps 1 and 3 require you to register with the CDS service, in order to download ERA5 data\\nbefore using in the WPS process.\\nSteps 2 and 5 require you to download extra input data - the instructions on how to do this\\nare included in the README.txt files in the relevant input data directories.\\n\\nA full workflow for all steps is provided here. But each separate step can by run on it's \\nown too, following the instructions given below. We recommend running step 4 first, to \\nexplore how the REAL & WRF workflow works, before trying the other steps.\\n\\n### 1. ERA5 download.\\n\\nBefore running the ERA5 download tool, ensure that you have reqistered for the CDS service, \\nsigned the ERA5 licensing agreement, and saved the CDS API key (`.cdsapirc`) in your \\nworking directory.\\n\\nTo run the ERA5 download tool use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] workflows/era5_workflow.cwl example_workflow_configurations/era5_download_settings.yaml\\n```\\nNote that the `--cachedir CACHE` option sets the working directory cache, which enables the\\nreuse of any steps previously run (and the restarting of the workflow from this point).\\nThe `--singularity` option is needed if you are using singularity instead of docker.\\n\\n### 2. WPS: Geogrid geography file creation\\n\\nBefore running the geogrid tool you will need to download the geography data from the\\n[UCAR website](https://www2.mmm.ucar.edu/wrf/users/download/get_sources_wps_geog.html).\\nThese should be extracted into the `input_files/geogrid_geog_input` directory.\\n\\nTo run the geogrid program use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] workflows/geogrid_workflow.cwl example_workflow_configurations/wps_geogrid_cwl_settings.yaml\\n```\\n\\n### 3. WPS: Creation of meteorology input files\\n\\nBefore running the WPS process you will have to download the ERA5 datafiles (which will be\\ncalled `preslev_[YYYYMMDD].grib` and `surface_[YYYYMMDD].grib`) and copy these to the directory\\n`input_files/wps_era5_input`. If you have also run geogrid in step 2 you can replace the \\n`geo_em.d01.nc` file in the `input_files/wps_geogrid_input` directory with the file that \\ngeogrid created.\\n\\nTo run the wps metgrid process use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] workflows/wps_workflow.cwl example_workflow_configurations/wps_metgrid_cwl_settings.yaml\\n```\\n\\n### 4. WRF: Creation of WRF input files, and running WRF model\\n\\nThe WRF model can be run without any prepreparation, except for the downloading of the \\ninput data from Zenodo. However, if you have created new meteorology files (`met_em*`) using\\nWPS you can replace the files in the `input_files/wrf_met_input` directory with these.\\n\\nTo run the WRF process (including REAL) use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] workflows/wrf_workflow.cwl example_workflow_configurations/wrf_real_cwl_settings.yaml\\n``` \\n\\n### 5. EMEP: Running EMEP chemistry and transport model\\n\\nBefore running the EMEP model you will need to download the EMEP input dataset. This can be\\ndone using the `catalog.py` tool, following the instructions in the `input_files/emep_input/README.txt`\\nfile. If you have run WRF you can also replace the `wrfout*` data files in the \\n`input_Files/emep_wrf_input` directory with those you have created.\\n\\nTo run the EMEP model use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] workflows/emep_workflow.cwl example_workflow_configurations/emep_cwl_settings.yaml\\n```\\n\\n### Full Workflow\\n\\nBefore running the full workflow make sure you have carried out the setup tasks described\\nabove.\\n\\nTo run the full workflow use the following command:\\n```\\ncwltool [--cachdir CACHE] [--singularity] wrf_emep_full_workflow.cwl example_workflow_configurations/wrf_emep_full_workflow_cwl_settings.yaml\\n```\\n\\n## Notes\\n\\n### WRF filenames\\n\\nIn order to work with singularity, all filenames need to exclude special characters.\\nTo ensure that all WRF filenames comply with this requirement, you will need to add the \\n`nocolons = .true.` option to your WPS, REAL and WRF namelists to ensure this.\\n\\n### MPI parallel processing\\n\\nThe WPS processes all run in single thread mode. REAL, WRF and EMEP have been compiled with\\nMPI support. The default cores for each of these is 2, 9 and 9, respectively. The \\nsettings file can be edited to modify these requirements.\\n\\n### Caching intermediate workflow steps\\n\\nTo cache the data from individual steps you can use the `--cachedir <cache-dir>` optional flag.\\n\\n\\n## License and Copyright \\n\\nThese workflow scripts have been developed by the [Research IT](https://research-it.manchester.ac.uk/) \\nat the [University of Manchester](https://www.manchester.ac.uk/).\\n\\nCopyright 2023 [University of Manchester, UK](https://www.manchester.ac.uk/).\\n\\nLicensed under the MIT license, see the LICENSE file for details.\"\n",
      " 'cccccc'\n",
      " '# Gene similariy anaylsis across physiological systems in IMPC phenotype data\\n\\nA Jupyter Notebook tool for analysing user specified genes across the different physiological systems in IMPC data.\\n\\n**_Input_**\\n\\nThe tool takes as input a list of gene ids (MGI ids or Gene Symbol ids). The elemnts in the list could be separated by a comma, semicolumn, tab or newline.\\n\\n**_Operation_**\\n\\nThe program will create an heatmap representing the number of phenotypes and the mp term list for each gene contained in an [IMPC physiological system](https://www.mousephenotype.org/help/data-visualization/gene-pages/phenogrid/).\\nUsing the slider, adjust the treshold to set the minimum count to be displayed in the heatmap. \\n\\nNB: Genes without phenotypes in any physiological system will not be displayed. Also, the labels of the heatmap will use Gene Symbols independently from the type of id used in the input.\\n\\n**_Tool access:_** [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AndreaFurlani/Jupyter_interactive_plots/main?urlpath=voila%2Frender%2FInteractive_plots.ipynb)'\n",
      " '# **Phenotype similarity analysis**\\n\\nA Jupyter Notebook for analyzing phenotyping similarities across user specified genes. Phenotypes are retrieved from the MGI resource\\n\\n**_Input_**\\n\\nThe tool takes as input a list of gene ids (MGI ids or Gene Symbol ids). The elemnts in the list could be separated by a comma, semicolumn, tab or newline.\\n\\n**_Operation_**\\n\\nThe Notebook will create a table where row and columns names are the Gene Symbols of the input elements and each cell will contain the name of the common phenotypes shared by those genes. Then an interactive heatmap will be displayed, showing also the count of those phenotypes. Using the slider, adjust the treshold to set the minimum count to be displayed in the heatmap.\\n\\nNB: Genes with only counts below the treshold will be not displayed in the heatmap. Also, the labels of the heatmap will use Gene Symbols independently from the type of id used in the input.\\n\\n**_Tool access:_** [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/AndreaFurlani/Jupyter_alliance/main?urlpath=voila%2Frender%2FAlliance_API_query.ipynb)\\n'\n",
      " '# MGnify genomes catalogue pipeline\\n\\n[MGnify](https://www.ebi.ac.uk/metagenomics/) A pipeline to perform taxonomic and functional annotation and to generate a catalogue from a set of isolate and/or metagenome-assembled genomes (MAGs) using the workflow described in the following publication:\\n\\nGurbich TA, Almeida A, Beracochea M, Burdett T, Burgin J, Cochrane G, Raj S, Richardson L, Rogers AB, Sakharova E, Salazar GA and Finn RD. (2023) [MGnify Genomes: A Resource for Biome-specific Microbial Genome Catalogues.](https://www.sciencedirect.com/science/article/pii/S0022283623000724) <i>J Mol Biol</i>. doi: https://doi.org/10.1016/j.jmb.2023.168016\\n\\nDetailed information about existing MGnify catalogues: https://docs.mgnify.org/src/docs/genome-viewer.html\\n\\n### Tools used in the pipeline\\n| Tool/Database                                           | Version           | Purpose |\\n|---------------------------------------------------------|-------------------|----------- |\\n| CheckM2                                                 | 1.0.1             | Determining genome quality       |\\n| dRep                                                    | 3.2.2             | Genome clustering       |\\n| Mash                                                    | 2.3               | Sketch for the catalogue; placement of genomes into clusters (update only); strain tree      |\\n| GUNC                                                    | 1.0.3             | Quality control       |\\n| GUNC DB                                                 | 2.0.4             | Database for GUNC       |\\n| GTDB-Tk                                                 | 2.3.0             | Assigning taxonomy; generating alignments       |\\n| GTDB                                                    | r214              | Database for GTDB-Tk       |\\n| Prokka                                                  | 1.14.6            | Protein annotation       |\\n| IQ-TREE 2                                               | 2.2.0.3           | Generating a phylogenetic tree       |\\n| Kraken 2                                                | 2.1.2             | Generating a kraken database       |\\n| Bracken                                                 | 2.6.2             | Generating a bracken database       |\\n| MMseqs2                                                 | 13.45111          | Generating a protein catalogue       |\\n| eggNOG-mapper                                           | 2.1.11            | Protein annotation (eggNOG, KEGG, COG,  CAZy)       |\\n| eggNOG DB                                               | 5.0.2             | Database for eggNOG-mapper       |\\n| Diamond                                                 | 2.0.11            | Protein annotation (eggNOG)       |\\n| InterProScan                                            | 5.62-94.0         | Protein annotation (InterPro, Pfam)       |\\n| CRISPRCasFinder                                         | 4.3.2             | Annotation of CRISPR arrays       |\\n| AMRFinderPlus                                           | 3.11.4            |   Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation     |\\n| AMRFinderPlus DB                                        | 3.11 2023-02-23.1 | Database for AMRFinderPlus      |\\n| SanntiS                                                 | 0.9.3.2           | Biosynthetic gene cluster annotation       |\\n| Infernal                                                | 1.1.4             | RNA predictions       |\\n| tRNAscan-SE                                             | 2.0.9             | tRNA predictions       |\\n| Rfam                                                    | 14.9              | Identification of SSU/LSU rRNA and other ncRNAs       |\\n| Panaroo                                                 | 1.3.2             | Pan-genome computation       |\\n| Seqtk                                                   | 1.3               | Generating a gene catalogue       |\\n| VIRify                                                  | 2.0.1             | Viral sequence annotation       |\\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0.1             | Mobilome annotation       |\\n| samtools                                                | 1.15              | FASTA indexing       |\\n\\n## Setup\\n\\n### Environment\\n\\nThe pipeline is implemented in [Nextflow](https://www.nextflow.io/).\\n\\nRequirements:\\n- [singulairty](https://sylabs.io/docs/) or [docker](https://www.docker.com/)\\n\\n#### Reference databases\\n\\nThe pipeline needs the following reference databases and configuration files (roughtly ~150G):\\n\\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/gunc_db_2.0.4.dmnd.gz\\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/eggnog_db_5.0.2.tgz\\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/rfam_14.9/\\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/kegg_classes.tsv\\n- ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/continent_countries.csv\\n- https://data.ace.uq.edu.au/public/gtdb/data/releases/release214/214.0/auxillary_files/gtdbtk_r214_data.tar.gz\\n- ftp://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/3.11/2023-02-23.1\\n- https://zenodo.org/records/4626519/files/uniref100.KO.v1.dmnd.gz\\n\\n### Containers\\n\\nThis pipeline requires [singularity](https://sylabs.io/docs/) or [docker](https://www.docker.com/) as the container engine to run pipeline.\\n\\nThe containers are hosted in [biocontainers](https://biocontainers.pro/) and [quay.io/microbiome-informatics](https://quay.io/organization/microbiome-informatics) repository.\\n\\nIt\\'s possible to build the containers from scratch using the following script:\\n\\n```bash\\ncd containers && bash build.sh\\n```\\n\\n## Running the pipeline\\n\\n## Data preparation\\n\\n1. You need to pre-download your data to directories and make sure that genomes are uncompressed. Scripts to fetch genomes from ENA ([fetch_ena.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ena.py)) and NCBI ([fetch_ncbi.py](https://github.com/EBI-Metagenomics/genomes-pipeline/blob/master/bin/fetch_ncbi.py)) are provided and need to be executed separately from the pipeline. If you have downloaded genomes from both ENA and NCBI, put them into separate folders.\\n\\n2. When genomes are fetched from ENA using the `fetch_ena.py` script, a CSV file with contamination and completeness statistics is also created in the same directory where genomes are saved to. If you are downloading genomes using a different approach, a CSV file needs to be created manually (each line should be genome accession, % completeness, % contamination). The ENA fetching script also pre-filters genomes to satisfy the QS50 cut-off (QS = % completeness - 5 * % contamination).\\n\\n3. You will need the following information to run the pipeline:\\n - catalogue name (for example, zebrafish-faecal)\\n - catalogue version (for example, 1.0)\\n - catalogue biome (for example, root:Host-associated:Human:Digestive system:Large intestine:Fecal)\\n - min and max accession number to be assigned to the genomes (only MGnify specific). Max - Min = #total number of genomes (NCBI+ENA)\\n\\n### Execution\\n\\nThe pipeline is built in [Nextflow](https://www.nextflow.io), and utilized containers to run the software (we don\\'t support conda ATM).\\nIn order to run the pipeline it\\'s required that the user creates a profile that suits their needs, there is an `ebi` profile in `nexflow.config` that can be used as template.\\n\\nAfter downloading the databases and adjusting the config file:\\n\\n```bash\\nnextflow run EBI-Metagenomics/genomes-pipeline -c <custom.config> -profile <profile> \\\\\\n--genome-prefix=MGYG \\\\\\n--biome=\"root:Host-associated:Fish:Digestive system\" \\\\\\n--ena_genomes=<path to genomes> \\\\\\n--ena_genomes_checkm=<path to genomes quality data> \\\\\\n--mgyg_start=0 \\\\\\n--mgyg_end=10 \\\\\\n--preassigned_accessions=<path to file with preassigned accessions if using>\\n--catalogue_name=zebrafish-faecal \\\\\\n--catalogue_version=\"1.0\" \\\\\\n--ftp_name=\"zebrafish-faecal\" \\\\\\n--ftp_version=\"v1.0\" \\\\\\n--outdir=\"<path-to-results>\"\\n```\\n\\n### Development\\n\\nInstall development tools (including pre-commit hooks to run Black code formatting).\\n\\n```bash\\npip install -r requirements-dev.txt\\npre-commit install\\n```\\n\\n#### Code style\\n\\nUse Black, this tool is configured if you install the pre-commit tools as above.\\n\\nTo manually run them: black .\\n\\n### Testing\\n\\nThis repo has 2 set of tests, python unit tests for some of the most critical python scripts and [nf-test](https://github.com/askimed/nf-test) scripts for the nextflow code.\\n\\nTo run the python tests\\n\\n```bash\\npip install -r requirements-test.txt\\npytest\\n```\\n\\nTo run the nextflow ones the databases have to downloaded manually, we are working to improve this.\\n\\n```bash\\nnf-test test tests/*\\n```\\n'\n",
      " '# Introduction\\n\\n`katdetectr` is an *R* package for the detection, characterization and visualization of localized hypermutated regions, often referred to as *kataegis*.\\n\\nPlease see the [Application Note](https://www.biorxiv.org/content/10.1101/2022.07.11.499364v1) (under submission) for additional background, details and performance evaluations of `katdetectr`.\\n\\nThe general workflow of `katdetectr` can be summarized as follows:\\n\\n1. Import of genomic variants; VCF, MAF or VRanges objects.\\n2. Detection of kataegis foci.\\n3. Visualization of segmentation and kataegis foci.\\n\\nPlease see the [vignette](https://bioconductor.org/packages/release/bioc/vignettes/katdetectr/inst/doc/General_overview.html) for an overview of the workflow in a step-by-step manner on publicly-available datasets which are included within this package.\\n\\n\\n## Installation\\n\\nDownload katdetectr from BioConductor:\\n```R\\nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\\n    install.packages(\"BiocManager\")\\n\\nBiocManager::install(\"katdetectr\")\\n\\n```\\n'\n",
      " 'From the R1 and R2 fastq files of a single samples, make a scRNAseq counts matrix, and perform basic QC with scanpy. Then, do further processing by making a UMAP and clustering. Produces a processed AnnData \\n\\nDepreciated: use individual workflows insead for multiple samples'\n",
      " 'Take an anndata file, and perform basic QC with scanpy. Produces a filtered AnnData object.'\n",
      " 'Basic processing of a QC-filtered Anndata Object. UMAP, clustering e.t.c '\n",
      " 'Rbbt implementation of the Covid-19 pilot workflow from the Personalized Medicine Center of Excellence.\\n\\nThis workflow processes single cell data to personalize boolean models that are then used in a multi-scale cellular simulation using PhysiBoSS.'\n",
      " 'Correlation between Phenotypic and In Silico Detection of Antimicrobial Resistance in Salmonella enterica in Canada Using Staramr. \\n\\nDoi: [10.3390/microorganisms10020292](https://doi.org/10.3390/microorganisms10020292)\\n'\n",
      " '# rquest-omop-worker-workflows\\n\\nSource for workflow definitions for the open source RQuest OMOP Worker tool developed for Hutch/TRE-FX\\n\\nNote: ARM workflows are currently broken. x86 ones work.\\n\\n## Inputs\\n\\n###\\xa0Body\\nSample input payload:\\n\\n```json\\n{\\n  \"task_id\": \"job-2023-01-13-14: 20: 38-<project>\",\\n  \"project\": \"<project>\",\\n  \"owner\": \"<owner>\",\\n  \"cohort\": {\\n    \"groups\": [\\n      {\\n        \"rules\": [\\n          {\\n            \"varname\": \"OMOP\",\\n            \"varcat\": \"Person\",\\n            \"type\": \"TEXT\",\\n            \"oper\": \"=\",\\n            \"value\": \"8507\"\\n          }\\n        ],\\n        \"rules_oper\": \"AND\"\\n      }\\n    ],\\n    \"groups_oper\": \"OR\"\\n  },\\n  \"collection\": \"<collection>\",\\n  \"protocol_version\": \"<version>\",\\n  \"char_salt\": \"<char_salt>\",\\n  \"uuid\": \"<uuid>\"\\n}\\n```\\n\\n### Database access\\n\\nCurrently this workflow requires inputs for connecting to the database it will run queries against.\\n\\nIn future this may be moved to environment variables.'\n",
      " 'This workflow correspond to the Genome-wide alternative splicing analysis training. It allows to analyze isoform switching by making use of IsoformSwitchAnalyzeR.'\n",
      " 'The radiation source ELBE (Electron Linac for beams with high Brilliance and low Emittance) at the Helmholtz Centre Dresden Rossendorf (HZDR) can produce several kinds of secondary radiations. THz radiation is one of them and can be used with a typical pulse frequency of 100 kHz as a stimulation source for elementary low-energy degrees of freedom in matter. To sample the whole THz wave the laser path length is modified by moving specific mirrors. The raw data contains for each mirror position a binary file storing the signal spectra and a folder with gray scaled tiff files storing the jitter timing. This Workflow is equivalent to the first part of the standalone jupyter notebook https://github.com/hzdr/TELBE-raw-data-evaluation/blob/main/sorting_binning.ipynb\\n\\nIn the job file the folder < FOLDER_BASE> and < FOLDER_SUB> needs to be specified and the parameters as a json string like < PARAMS> = { \"rep\": 100000, \"t_exp\": 1, \"N_sample\": 96, \"offset\": 0, \"pixel_to_ps\": 0.0115, \"Stage_zero\": 0 }\\n\\nThe python file which is used is originally published in gitlab https://codebase.helmholtz.cloud/science2workflow/telbe-sorting-binning/-/blob/master/src/ The workflow can automatically be monitored in Heliport if the project number < HELIPORT_PROJECT> is provided.\\n'\n",
      " '# CWL-assembly\\n[![Codacy Badge](https://api.codacy.com/project/badge/Grade/684724bbc0134960ab41748f4a4b732f)](https://www.codacy.com/app/mb1069/CWL-assembly?utm_source=github.com&amp;utm_medium=referral&amp;utm_content=EBI-Metagenomics/CWL-assembly&amp;utm_campaign=Badge_Grade)\\n[![Build Status](https://travis-ci.org/EBI-Metagenomics/CWL-assembly.svg?branch=develop)](https://travis-ci.org/EBI-Metagenomics/CWL-assembly)\\n\\n## Description\\n\\nThis repository contains two workflows for metagenome and metatranscriptome assembly of short read data. MetaSPAdes is used as default for paired-end data, and MEGAHIT for single-end data and co-assemblies. MEGAHIT can be specified as the default assembler in the yaml file if preferred. Steps include:\\n\\n  * _QC_: removal of short reads, low quality regions, adapters and host decontamination\\n  * _Assembly_: with metaSPADES or MEGAHIT\\n  * _Post-assembly_: Host and PhiX decontamination, contig length filter (500bp), stats generation\\n\\n## Requirements - How to install\\n\\nThis pipeline requires a conda environment with cwltool, blastn, and metaspades. If created with `requirements.yml`, the environment will be called `cwl_assembly`. \\n\\n```\\nconda env create -f requirements.yml\\nconda activate cwl_assembly\\npip install cwltool==3.1.20230601100705\\n```\\n\\n## Databases\\n\\nYou will need to pre-download fasta files for host decontamination and generate the following databases accordingly:\\n  * bwa index\\n  * blast index\\n    \\nSpecify the locations in the yaml file when running the pipeline.\\n\\n## Main pipeline executables\\n\\n  * `src/workflows/metagenome_pipeline.cwl`\\n  * `src/workflows/metatranscriptome_pipeline.cwl`\\n\\n## Example command\\n\\n```cwltool --singularity --outdir ${OUTDIR} ${CWL} ${YML}```\\n\\n`$CWL` is going to be one of the executables mentioned above\\n`$YML` should be a config yaml file including entries among what follows. \\nYou can find a yml template in the `examples` folder.\\n\\n## Example output directory structure\\n```\\nRoot directory\\n    ├── megahit\\n    │   └── 001 -------------------------------- Assembly root directory\\n    │       ├── assembly_stats.json ------------ Human-readable assembly stats file\\n    │       ├── coverage.tab ------------------- Coverage file\\n    │       ├── log ---------------------------- CwlToil+megahit output log\\n    |       ├── options.json ------------------- Megahit input options\\n    │       ├── SRR6257420.fasta.gz ------------ Archived and trimmed assembly\\n    │       └── SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\\n    ├── metaspades\\n    │   └── 001 -------------------------------- Assembly root directory\\n    │       ├── assembly_graph.fastg ----------- Assembly graph\\n    │       ├── assembly_stats.json ------------ Human-readable assembly stats file\\n    │       ├── coverage.tab ------------------- Coverage file\\n    |       ├── params.txt --------------------- Metaspades input options\\n    │       ├── spades.log --------------------- Metaspades output log\\n    │       ├── SRR6257420.fasta.gz ------------ Archived and trimmed assembly\\n    │       └── SRR6257420.fasta.gz.md5 -------- MD5 hash of above archive\\n    │\\xa0\\n    └── raw ------------------------------------ Raw data directory\\n        ├── SRR6257420.fastq.qc_stats.tsv ------ Stats for cleaned fastq\\n        ├── SRR6257420_fastp_clean_1.fastq.gz -- Cleaned paired-end file_1\\n        └── SRR6257420_fastp_clean_2.fastq.gz -- Cleaned paired-end file_2\\n```\\n'\n",
      " \"# EukRecover\\nPipeline to recover eukaryotic MAGs using CONCOCT, metaBAT2 and EukCC's merging algorythm.\\n\\nNeeds paired end shotgun metagenomic reads.\\n\\n## Environment\\n\\nEukrecover requires an environment with snakemake and metaWRAP.\\n\\n## Quickstart\\n\\nDefine your samples in the file `samples.csv`.\\nThis file needs to have the columns project and run to identify each metagenome. \\n\\nThis pipeline does not support co-binning, but feel free to change it. \\n\\nClone this repro wherever you want to run the pipeline:\\n```\\ngit clone https://github.com/openpaul/eukrecover/\\n```\\n\\n\\nYou can then run the snakemake like so\\n\\n```\\nsnakemake --use-singularity\\n```\\n\\nThe pipeline used dockerhub to fetch all tools, so make sure you have singularity installed.\\n\\n\\n\\n## Prepare databases\\nThe pipeline will setup databases for you, but if you already have a EukCC or a BUSCO 5 database you can use them \\nby specifying the location in the file `config/config.yaml`\\n\\n\\n## Output:\\nIn the folder results you will find a folder `MAGs` which will contain a folder\\n`fa` containing the actual MAG fastas.\\nIn addition you will find stats for each MAG in the table `QC.csv`.\\n\\nThis table contains the following columns:\\n\\nname,eukcc_compl,eukcc_cont,BUSCO_C,BUSCO_M,BUSCO_D,BUSCO_F,BUSCO_tax,N50,bp\\n\\n\\n\\n## Citation:\\n\\nIf you use this pipeline please make sure to cite all used software. \\n\\nFor this please reffer to the used rules.\\n\"\n",
      " '# COVID-19 Multiscale Modelling of the Virus and Patients’ Tissue Workflow\\n\\n## Table of Contents\\n\\n- [COVID-19 Multiscale Modelling of the Virus and Patients’ Tissue Workflow](#covid-19-multiscale-modelling-of-the-virus-and-patients-tissue-workflow)\\n  - [Table of Contents](#table-of-contents)\\n  - [Description](#description)\\n  - [Contents](#contents)\\n    - [Building Blocks](#building-blocks)\\n    - [Workflows](#workflows)\\n    - [Resources](#resources)\\n    - [Tests](#tests)\\n  - [Instructions](#instructions)\\n    - [Local machine](#local-machine)\\n      - [Requirements](#requirements)\\n      - [Usage steps](#usage-steps)\\n    - [MareNostrum 4](#marenostrum-4)\\n      - [Requirements in MN4](#requirements-in-mn4)\\n      - [Usage steps in MN4](#usage-steps-in-mn4)\\n    - [Mahti or Puhti](#mahti-or-puhti)\\n      - [Requirements](#requirements)\\n      - [Steps](#steps)\\n  - [License](#license)\\n  - [Contact](#contact)\\n\\n## Description\\n\\nUses multiscale simulations to predict patient-specific SARS‑CoV‑2 severity subtypes\\n(moderate, severe or control), using single-cell RNA-Seq data, MaBoSS and PhysiBoSS.\\nBoolean models are used to determine the behaviour of individual agents as a function\\nof extracellular conditions and the concentration of different  substrates, including\\nthe number of virions. Predictions of severity subtypes are based on a meta-analysis of\\npersonalised model outputs simulating cellular apoptosis regulation in epithelial cells\\ninfected by SARS‑CoV‑2.\\n\\nThe workflow uses the following building blocks, described in order of execution:\\n\\n1. High-throughput mutant analysis\\n2. Single-cell processing\\n3. Personalise patient\\n4. PhysiBoSS\\n5. Analysis of all simulations\\n\\nFor details on individual workflow steps, see the user documentation for each building block.\\n\\n[`GitHub repository`](<https://github.com/PerMedCoE/covid-19-workflow>)\\n\\n\\n## Contents\\n\\n### Building Blocks\\n\\nThe ``BuildingBlocks`` folder contains the script to install the\\nBuilding Blocks used in the COVID-19 Workflow.\\n\\n### Workflows\\n\\nThe ``Workflow`` folder contains the workflows implementations.\\n\\nCurrently contains the implementation using PyCOMPSs and Snakemake (in progress).\\n\\n### Resources\\n\\nThe ``Resources`` folder contains dataset files.\\n\\n### Tests\\n\\nThe ``Tests`` folder contains the scripts that run each Building Block\\nused in the workflow for the given small dataset.\\nThey can be executed individually for testing purposes.\\n\\n## Instructions\\n\\n### Local machine\\n\\nThis section explains the requirements and usage for the COVID19 Workflow in a laptop or desktop computer.\\n\\n#### Requirements\\n\\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html) / [Snakemake](https://snakemake.readthedocs.io/en/stable/)\\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\\n\\n#### Usage steps\\n\\n1. Clone this repository:\\n\\n  ```bash\\n  git clone https://github.com/PerMedCoE/covid-19-workflow.git\\n  ```\\n\\n2. Install the Building Blocks required for the COVID19 Workflow:\\n\\n  ```bash\\n  covid-19-workflow/BuildingBlocks/./install_BBs.sh\\n  ```\\n\\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\\n\\n  - Required images:\\n      - MaBoSS.singularity\\n      - meta_analysis.singularity\\n      - PhysiCell-COVID19.singularity\\n      - single_cell.singularity\\n\\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\\n\\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\\n  1. Clone the `BuildingBlocks` repository\\n     ```bash\\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\\n     ```\\n  2. Build the required Building Block images\\n     ```bash\\n     cd BuildingBlocks/Resources/images\\n     sudo singularity build MaBoSS.sif MaBoSS.singularity\\n     sudo singularity build meta_analysis.sif meta_analysis.singularity\\n     sudo singularity build PhysiCell-COVID19.sif PhysiCell-COVID19.singularity\\n     sudo singularity build single_cell.sif single_cell.singularity\\n     cd ../../..\\n     ```\\n\\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\\n\\n4. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflows/PyCOMPSs\\n   ```\\n\\n5. Execute `./run.sh`\\n\\n**If using Snakemake in local PC** (make sure that SnakeMake is installed):\\n\\n4. Go to `Workflow/SnakeMake` folder\\n\\n   ```bash\\n   cd Workflows/SnakeMake\\n   ```\\n\\n5. Execute `./run.sh`\\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\\n\\n\\n### MareNostrum 4\\n\\nThis section explains the requirements and usage for the COVID19 Workflow in the MareNostrum 4 supercomputer.\\n\\n#### Requirements in MN4\\n\\n- Access to MN4\\n\\nAll Building Blocks are already installed in MN4, and the COVID19 Workflow available.\\n\\n#### Usage steps in MN4\\n\\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\\n\\n   ```bash\\n   export COMPSS_PYTHON_VERSION=3\\n   module load COMPSs/3.1\\n   module load singularity/3.5.2\\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\\n   module load permedcoe\\n   ```\\n\\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\\n\\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`COVID19WORKFLOW_DATASET` environment variable).\\n\\n2. Get a copy of the pilot workflow into your desired folder\\n\\n   ```bash\\n   mkdir desired_folder\\n   cd desired_folder\\n   get_covid19workflow\\n   ```\\n\\n3. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflow/PyCOMPSs\\n   ```\\n\\n4. Execute `./launch.sh`\\n\\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\\n\\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\\n\\n  After the execution, a `results` folder will be available with with COVID19 Workflow results.\\n\\n### Mahti or Puhti\\n\\nThis section explains how to run the COVID19 workflow on CSC supercomputers using SnakeMake.\\n\\n#### Requirements\\n\\n- Install snakemake (or check if there is a version installed using `module spider snakemake`)\\n- Install workflow, using the same steps as for the local machine. With the exception that containers have to be built elsewhere.\\n\\n#### Steps\\n\\n\\n1. Go to `Workflow/SnakeMake` folder\\n\\n   ```bash\\n   cd Workflow/SnakeMake\\n   ```\\n\\n2. Edit `launch.sh` with the correct partition, account, and resource specifications.  \\n\\n3. Execute `./launch.sh`\\n\\n  > :warning: Snakemake provides a `--cluster` flag, but this functionality should be avoided as it\\'s really not suited for HPC systems.\\n\\n## License\\n\\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\\n\\n## Contact\\n\\n<https://permedcoe.eu/contact/>\\n\\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\\n\\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\\n'\n",
      " '# Cancer Invasion Workflow\\n\\n## Table of Contents\\n\\n- [Cancer Invasion Workflow](#cancer-invasion-workflow)\\n  - [Table of Contents](#table-of-contents)\\n  - [Description](#description)\\n  - [Contents](#contents)\\n    - [Building Blocks](#building-blocks)\\n    - [Workflows](#workflows)\\n    - [Resources](#resources)\\n    - [Tests](#tests)\\n  - [Instructions](#instructions)\\n    - [Local machine](#local-machine)\\n      - [Requirements](#requirements)\\n      - [Usage steps](#usage-steps)\\n    - [MareNostrum 4](#marenostrum-4)\\n      - [Requirements in MN4](#requirements-in-mn4)\\n      - [Usage steps in MN4](#usage-steps-in-mn4)\\n    - [Mahti or Puhti](#mahti-or-puhti)\\n      - [Requirements](#requirements)\\n      - [Steps](#steps)\\n  - [License](#license)\\n  - [Contact](#contact)\\n\\n## Description\\n\\nUses multiscale simulations to describe cancer progression into invasion.\\n\\nThe workflow uses the following building blocks, described in order of execution:\\n\\n1. PhysiBoSS-Invasion\\n\\nFor details on individual workflow steps, see the user documentation for each building block.\\n\\n[`GitHub repository`](<https://github.com/PerMedCoE/cancer-invasion-workflow>)\\n\\n\\n## Contents\\n\\n### Building Blocks\\n\\nThe ``BuildingBlocks`` folder contains the script to install the\\nBuilding Blocks used in the Cancer Invasion Workflow.\\n\\n### Workflows\\n\\nThe ``Workflow`` folder contains the workflows implementations.\\n\\nCurrently contains the implementation using PyCOMPSs and Snakemake (in progress).\\n\\n### Resources\\n\\nThe ``Resources`` folder contains dataset files.\\n\\n### Tests\\n\\nThe ``Tests`` folder contains the scripts that run each Building Block\\nused in the workflow for the given small dataset.\\nThey can be executed individually for testing purposes.\\n\\n## Instructions\\n\\n### Local machine\\n\\nThis section explains the requirements and usage for the Cancer Invasion Workflow in a laptop or desktop computer.\\n\\n#### Requirements\\n\\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html) / [Snakemake](https://snakemake.readthedocs.io/en/stable/)\\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\\n\\n#### Usage steps\\n\\n1. Clone this repository:\\n\\n  ```bash\\n  git clone https://github.com/PerMedCoE/cancer-invasion-workflow\\n  ```\\n\\n2. Install the Building Blocks required for the Cancer Invasion Workflow:\\n\\n  ```bash\\n  cancer-invasion-workflow/BuildingBlocks/./install_BBs.sh\\n  ```\\n\\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\\n\\n  - Required images:\\n      - PhysiCell-Invasion.singularity\\n\\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\\n\\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\\n  1. Clone the `BuildingBlocks` repository\\n     ```bash\\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\\n     ```\\n  2. Build the required Building Block images\\n     ```bash\\n     cd BuildingBlocks/Resources/images\\n     sudo singularity build PhysiCell-Invasion.sif PhysiCell-Invasion.singularity\\n     cd ../../..\\n     ```\\n\\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\\n\\n4. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflows/PyCOMPSs\\n   ```\\n\\n5. Execute `./run.sh`\\n\\n**If using Snakemake in local PC** (make sure that SnakeMake is installed):\\n\\n4. Go to `Workflow/SnakeMake` folder\\n\\n   ```bash\\n   cd Workflows/SnakeMake\\n   ```\\n\\n5. Execute `./run.sh`\\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\\n\\n\\n### MareNostrum 4\\n\\nThis section explains the requirements and usage for the Cancer Invasion Workflow in the MareNostrum 4 supercomputer.\\n\\n#### Requirements in MN4\\n\\n- Access to MN4\\n\\nAll Building Blocks are already installed in MN4, and the Cancer Invasion Workflow available.\\n\\n#### Usage steps in MN4\\n\\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\\n\\n   ```bash\\n   export COMPSS_PYTHON_VERSION=3\\n   module load COMPSs/3.1\\n   module load singularity/3.5.2\\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\\n   module load permedcoe\\n   ```\\n\\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\\n\\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`CANCERINVASIONWORKFLOW_DATASET` environment variable).\\n\\n2. Get a copy of the pilot workflow into your desired folder\\n\\n   ```bash\\n   mkdir desired_folder\\n   cd desired_folder\\n   get_cancerinvasionworkflow\\n   ```\\n\\n3. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflow/PyCOMPSs\\n   ```\\n\\n4. Execute `./launch.sh`\\n\\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\\n\\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\\n\\n  After the execution, a `results` folder will be available with with Cancer Invasion Workflow results.\\n\\n### Mahti or Puhti\\n\\nThis section explains how to run the Cancer Invasion workflow on CSC supercomputers using SnakeMake.\\n\\n#### Requirements\\n\\n- Install snakemake (or check if there is a version installed using `module spider snakemake`)\\n- Install workflow, using the same steps as for the local machine. With the exception that containers have to be built elsewhere.\\n\\n#### Steps\\n\\n\\n1. Go to `Workflow/SnakeMake` folder\\n\\n   ```bash\\n   cd Workflow/SnakeMake\\n   ```\\n\\n2. Edit `launch.sh` with the correct partition, account, and resource specifications.  \\n\\n3. Execute `./launch.sh`\\n\\n  > :warning: Snakemake provides a `--cluster` flag, but this functionality should be avoided as it\\'s really not suited for HPC systems.\\n\\n## License\\n\\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\\n\\n## Contact\\n\\n<https://permedcoe.eu/contact/>\\n\\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\\n\\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\\n'\n",
      " '# Single drug prediction Workflow\\n## Table of Contents\\n\\n- [Single drug prediction Workflow](#single-drug-prediction-workflow)\\n  - [Table of Contents](#table-of-contents)\\n  - [Description](#description)\\n  - [Contents](#contents)\\n    - [Building Blocks](#building-blocks)\\n    - [Workflows](#workflows)\\n    - [Resources](#resources)\\n    - [Tests](#tests)\\n  - [Instructions](#instructions)\\n    - [Local machine](#local-machine)\\n      - [Requirements](#requirements)\\n      - [Usage steps](#usage-steps)\\n    - [MareNostrum 4](#marenostrum-4)\\n      - [Requirements in MN4](#requirements-in-mn4)\\n      - [Usage steps in MN4](#usage-steps-in-mn4)\\n  - [License](#license)\\n  - [Contact](#contact)\\n\\n## Description\\n\\nComplementarily, the workflow supports single drug response predictions to provide a baseline prediction in cases where drug response information for a given drug and cell line is not available. As an input, the workflow needs basal gene expression data for a cell, the drug targets (they need to be known for untested drugs) and optionally CARNIVAL features (sub-network activity predicted with CARNIVAL building block) and predicts log(IC50) values. This workflow uses a custom matrix factorization approach built with Google JAX and trained with gradient descent. The workflow can be used both for training a model, and for predicting new drug responses.\\n\\nThe workflow uses the following building blocks in order of execution (for training a model):\\n\\n1. Carnival_gex_preprocess\\n    - Preprocessed the basal gene expression data from GDSC. The input is a matrix of Gene x Sample expression data.\\n2. Progeny\\n    - Using the preprocessed data, it estimates pathway activities for each column in the data (for each sample). It returns a matrix of Pathways x Samples with activity values for 11 pathways.\\n3. Omnipath\\n    - It downloads latest Prior Knowledge Network of signalling. This building block can be ommited if there exists already a csv file with the network.\\n4. TF Enrichment\\n    - For each sample, transcription factor activities are estimated using Dorothea.\\n5. CarnivalPy\\n    - Using the TF activities estimated before, it runs Carnival to obtain a sub-network consistent with the TF activities (for each sample).\\n6. Carnival_feature_merger\\n    - Preselect a set of genes by the user (if specified) and merge the features with the basal gene expression data.\\n7. ML Jax Drug Prediction\\n    - Trains a model using the combined features to predict IC50 values from GDSC.\\n\\nFor details on individual workflow steps, please check the scripts that use each individual building block in the workflow [`GitHub repository`](<https://github.com/PerMedCoE/single_drug_prediction>)\\n\\n## Contents\\n\\n### Building Blocks\\n\\nThe ``BuildingBlocks`` folder contains the script to install the\\nBuilding Blocks used in the Single Drug Prediction Workflow.\\n\\n### Workflows\\n\\nThe ``Workflow`` folder contains the workflows implementations.\\n\\nCurrently contains the implementation using PyCOMPSs.\\n\\n### Resources\\n\\nThe ``Resources`` folder contains a small dataset for testing purposes.\\n\\n### Tests\\n\\nThe ``Tests`` folder contains the scripts that run each Building Block\\nused in the workflow for a small dataset.\\nThey can be executed individually *without PyCOMPSs installed* for testing\\npurposes.\\n\\n## Instructions\\n\\n### Local machine\\n\\nThis section explains the requirements and usage for the Single Drug Prediction Workflow in a laptop or desktop computer.\\n\\n#### Requirements\\n\\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html)\\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\\n\\n#### Usage steps\\n\\n1. Clone this repository:\\n\\n  ```bash\\n  git clone https://github.com/PerMedCoE/single-drug-prediction-workflow.git\\n  ```\\n\\n2. Install the Building Blocks required for the COVID19 Workflow:\\n\\n  ```bash\\n  single-drug-prediction-workflow/BuildingBlocks/./install_BBs.sh\\n  ```\\n\\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\\n\\n  - Required images:\\n      - toolset.singularity\\n      - carnivalpy.singularity\\n      - ml-jax.singularity\\n\\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\\n\\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\\n  1. Clone the `BuildingBlocks` repository\\n     ```bash\\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\\n     ```\\n  2. Build the required Building Block images\\n     ```bash\\n     cd BuildingBlocks/Resources/images\\n     ## Download new BB singularity files\\n     wget https://github.com/saezlab/permedcoe/archive/refs/heads/master.zip\\n     unzip master.zip\\n     cd permedcoe-master/containers\\n     ## Build containers\\n     cd toolset\\n     sudo /usr/local/bin/singularity build toolset.sif toolset.singularity\\n     mv toolset.sif ../../../\\n     cd ..\\n     cd carnivalpy\\n     sudo /usr/local/bin/singularity build carnivalpy.sif carnivalpy.singularity\\n     mv carnivalpy.sif ../../../\\n     cd ..\\n     cd ml-jax\\n     sudo /usr/local/bin/singularity build ml-jax.sif ml-jax.singularity\\n     mv ml-jax.sif ../../../tf-jax.sif\\n     cd ..\\n     cd ../..\\n     ## Cleanup\\n     rm -rf permedcoe-master\\n     rm master.zip\\n     cd ../../..\\n     ```\\n\\n     > :warning: **TIP**: The singularity containers **can to be downloaded** from: https://cloud.sylabs.io/library/pablormier\\n\\n\\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\\n\\n4. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflows/PyCOMPSs\\n   ```\\n\\n5. Execute `./run.sh`\\n\\n  The execution is prepared to use the singularity images that **MUST** be placed into `BuildingBlocks/Resources/images` folder. If they are located in any other folder, please update the `run.sh` script setting the `PERMEDCOE_IMAGES` to the images folder.\\n\\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\\n\\n### MareNostrum 4\\n\\nThis section explains the requirements and usage for the Single Drug Prediction Workflow in the MareNostrum 4 supercomputer.\\n\\n#### Requirements in MN4\\n\\n- Access to MN4\\n\\nAll Building Blocks are already installed in MN4, and the Single Drug Prediction Workflow available.\\n\\n#### Usage steps in MN4\\n\\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\\n\\n   ```bash\\n   export COMPSS_PYTHON_VERSION=3\\n   module load COMPSs/3.1\\n   module load singularity/3.5.2\\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\\n   module load permedcoe\\n   ```\\n\\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\\n\\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`SINGLE_DRUG_PREDICTION_WORKFLOW_DATASET` environment variable).\\n\\n2. Get a copy of the pilot workflow into your desired folder\\n\\n   ```bash\\n   mkdir desired_folder\\n   cd desired_folder\\n   get_single_drug_prediction_workflow\\n   ```\\n\\n3. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflow/PyCOMPSs\\n   ```\\n\\n4. Execute `./launch.sh`\\n\\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\\n\\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\\n\\n  After the execution, a `results` folder will be available with with Single Drug Prediction Workflow results.\\n\\n## License\\n\\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\\n\\n## Contact\\n\\n<https://permedcoe.eu/contact/>\\n\\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\\n\\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\\n'\n",
      " '# Drug Synergies Screening Workflow\\n\\n## Table of Contents\\n\\n- [Drug Synergies Screening Workflow](#drug-synergies-screening-workflow)\\n  - [Table of Contents](#table-of-contents)\\n  - [Description](#description)\\n  - [Contents](#contents)\\n    - [Building Blocks](#building-blocks)\\n    - [Workflows](#workflows)\\n    - [Resources](#resources)\\n    - [Tests](#tests)\\n  - [Instructions](#instructions)\\n    - [Local machine](#local-machine)\\n      - [Requirements](#requirements)\\n      - [Usage steps](#usage-steps)\\n    - [MareNostrum 4](#marenostrum-4)\\n      - [Requirements in MN4](#requirements-in-mn4)\\n      - [Usage steps in MN4](#usage-steps-in-mn4)\\n  - [License](#license)\\n  - [Contact](#contact)\\n\\n## Description\\n\\nThis pipeline simulates a drug screening on personalised cell line models. It automatically builds Boolean models of interest, then uses cell lines data (expression, mutations, copy number variations) to personalise them as MaBoSS models. Finally, this pipeline simulates multiple drug intervention on these MaBoSS models, and lists drug synergies of interest.\\n\\nThe workflow uses the following building blocks, described in order of execution:\\n\\n1. Build model from species\\n2. Personalise patient\\n3. MaBoSS\\n4. Print drug results\\n\\nFor details on individual workflow steps, see the user documentation for each building block.\\n\\n[`GitHub repository`](https://github.com/PerMedCoE/drug-synergies-workflow>)\\n\\n## Contents\\n\\n### Building Blocks\\n\\nThe ``BuildingBlocks`` folder contains the script to install the\\nBuilding Blocks used in the Drug Synergies Workflow.\\n\\n### Workflows\\n\\nThe ``Workflow`` folder contains the workflows implementations.\\n\\nCurrently contains the implementation using PyCOMPSs.\\n\\n### Resources\\n\\nThe ``Resources`` folder contains a small dataset for testing purposes.\\n\\n### Tests\\n\\nThe ``Tests`` folder contains the scripts that run each Building Block\\nused in the workflow for a small dataset.\\nThey can be executed individually *without PyCOMPSs installed* for testing\\npurposes.\\n\\n## Instructions\\n\\n### Local machine\\n\\nThis section explains the requirements and usage for the Drug Synergies Workflow in a laptop or desktop computer.\\n\\n#### Requirements\\n\\n- [`permedcoe`](https://github.com/PerMedCoE/permedcoe) package\\n- [PyCOMPSs](https://pycompss.readthedocs.io/en/stable/Sections/00_Quickstart.html)\\n- [Singularity](https://sylabs.io/guides/3.0/user-guide/installation.html)\\n\\n#### Usage steps\\n\\n1. Clone this repository:\\n\\n  ```bash\\n  git clone https://github.com/PerMedCoE/drug-synergies-workflow.git\\n  ```\\n\\n2. Install the Building Blocks required for the COVID19 Workflow:\\n\\n  ```bash\\n  drug-synergies-workflow/BuildingBlocks/./install_BBs.sh\\n  ```\\n\\n3. Get the required Building Block images from the project [B2DROP](https://b2drop.bsc.es/index.php/f/444350):\\n\\n  - Required images:\\n      - PhysiCell-COVID19.singularity\\n      - printResults.singularity\\n      - MaBoSS_sensitivity.singularity\\n      - FromSpeciesToMaBoSSModel.singularity\\n\\n  The path where these files are stored **MUST be exported in the `PERMEDCOE_IMAGES`** environment variable.\\n\\n  > :warning: **TIP**: These containers can be built manually as follows (be patient since some of them may take some time):\\n  1. Clone the `BuildingBlocks` repository\\n     ```bash\\n     git clone https://github.com/PerMedCoE/BuildingBlocks.git\\n     ```\\n  2. Build the required Building Block images\\n     ```bash\\n     cd BuildingBlocks/Resources/images\\n     sudo singularity build PhysiCell-COVID19.sif PhysiCell-COVID19.singularity\\n     sudo singularity build printResults.sif printResults.singularity\\n     sudo singularity build MaBoSS_sensitivity.sif MaBoSS_sensitivity.singularity\\n     sudo singularity build FromSpeciesToMaBoSSModel.sif FromSpeciesToMaBoSSModel.singularity\\n     cd ../../..\\n     ```\\n\\n**If using PyCOMPSs in local PC** (make sure that PyCOMPSs in installed):\\n\\n4. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflows/PyCOMPSs\\n   ```\\n\\n5. Execute `./run.sh`\\n  > **TIP**: If you want to run the workflow with a different dataset, please update the `run.sh` script setting the `dataset` variable to the new dataset folder and their file names.\\n\\n### MareNostrum 4\\n\\nThis section explains the requirements and usage for the Drug Synergies Workflow in the MareNostrum 4 supercomputer.\\n\\n#### Requirements in MN4\\n\\n- Access to MN4\\n\\nAll Building Blocks are already installed in MN4, and the Drug Synergies Workflow available.\\n\\n#### Usage steps in MN4\\n\\n1. Load the `COMPSs`, `Singularity` and `permedcoe` modules\\n\\n   ```bash\\n   export COMPSS_PYTHON_VERSION=3\\n   module load COMPSs/3.1\\n   module load singularity/3.5.2\\n   module use /apps/modules/modulefiles/tools/COMPSs/libraries\\n   module load permedcoe\\n   ```\\n\\n   > **TIP**: Include the loading into your `${HOME}/.bashrc` file to load it automatically on the session start.\\n\\n   This commands will load COMPSs and the permedcoe package which provides all necessary dependencies, as well as the path to the singularity container images (`PERMEDCOE_IMAGES` environment variable) and testing dataset (`DRUG_SYNERGIES_WORKFLOW_DATASET` environment variable).\\n\\n2. Get a copy of the pilot workflow into your desired folder\\n\\n   ```bash\\n   mkdir desired_folder\\n   cd desired_folder\\n   get_drug_synergies_workflow\\n   ```\\n\\n3. Go to `Workflow/PyCOMPSs` folder\\n\\n   ```bash\\n   cd Workflow/PyCOMPSs\\n   ```\\n\\n4. Execute `./launch.sh`\\n\\n  This command will launch a job into the job queuing system (SLURM) requesting 2 nodes (one node acting half master and half worker, and other full worker node) for 20 minutes, and is prepared to use the singularity images that are already deployed in MN4 (located into the `PERMEDCOE_IMAGES` environment variable). It uses the dataset located into `../../Resources/data` folder.\\n\\n  > :warning: **TIP**: If you want to run the workflow with a different dataset, please edit the `launch.sh` script and define the appropriate dataset path.\\n\\n  After the execution, a `results` folder will be available with with Drug Synergies Workflow results.\\n\\n## License\\n\\n[Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0)\\n\\n## Contact\\n\\n<https://permedcoe.eu/contact/>\\n\\nThis software has been developed for the [PerMedCoE project](https://permedcoe.eu/), funded by the European Commission (EU H2020 [951773](https://cordis.europa.eu/project/id/951773)).\\n\\n![](https://permedcoe.eu/wp-content/uploads/2020/11/logo_1.png \"PerMedCoE\")\\n'\n",
      " 'Genome-wide alternative splicing analysis v.2'\n",
      " 'Abstract CWL Automatically generated from the Galaxy workflow file: Copy of Genome-wide alternative splicing analysis'\n",
      " \"**Name:** Matrix Multiplication  \\n**Contact Person:** support-compss@bsc.es  \\n**Access Level:** public  \\n**License Agreement:** Apache2  \\n**Platform:** COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\n# Versions\\nThere are three versions of Matrix Multiplication, depending on the data types used to store the blocks.\\n## Version 1\\n''files'', where the matrix blocks are stored in files.\\n## Version 2\\n''objects'', where the matrix blocks are represented by objects.\\n## Version 3\\n''arrays'', where the matrix blocks are stored in arrays.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss matmul.files.Matmul numberOfBlocks blockSize\\nruncompss matmul.objects.Matmul numberOfBlocks blockSize\\nruncompss matmul.arrays.Matmul numberOfBlocks blockSize\\n``` \\n\\nwhere:\\n  * numberOfBlocks: Number of blocks inside each matrix\\n  * blockSize: Size of each block\\n\\n\\n# Execution Example\\n```\\nruncompss matmul.objects.Matmul 16 4\\nruncompss matmul.files.Matmul 16 4\\nruncompss matmul.arrays.Matmul 16 4  \\n```\\n\\n# Build\\n## Option 1: Native java\\n```\\ncd ~/tutorial_apps/java/matmul/; javac src/main/java/matmul/*/*.java\\ncd src/main/java/; jar cf matmul.jar matmul/\\ncd ../../../; mv src/main/java/matmul.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\ncd ~/tutorial_apps/java/matmul/\\nmvn clean package\\n```\\n\"\n",
      " '**Name:** Matrix multiplication with Files  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/matmul_files.py numberOfBlocks blockSize\\n```\\n\\nwhere:\\n* numberOfBlocks: Number of blocks inside each matrix\\n* blockSize: Size of each block\\n\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/matmul_files.py 4 4\\nruncompss src/matmul_files.py 4 4\\npython -m pycompss src/matmul_files.py 4 4\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " '# Protein Conformational ensembles generation\\n\\n## Workflow included in the [ELIXIR 3D-Bioinfo](https://elixir-europe.org/communities/3d-bioinfo) Implementation Study:\\n\\n### Building on PDBe-KB to chart and characterize the conformation landscape of native proteins\\n\\nThis tutorial aims to illustrate the process of generating **protein conformational ensembles** from** 3D structures **and analysing its **molecular flexibility**, step by step, using the **BioExcel Building Blocks library (biobb)**.\\n\\n## Conformational landscape of native proteins\\n**Proteins** are **dynamic** systems that adopt multiple **conformational states**, a property essential for many **biological processes** (e.g. binding other proteins, nucleic acids, small molecule ligands, or switching between functionaly active and inactive states). Characterizing the different **conformational states** of proteins and the transitions between them is therefore critical for gaining insight into their **biological function** and can help explain the effects of genetic variants in **health** and **disease** and the action of drugs.\\n\\n**Structural biology** has become increasingly efficient in sampling the different **conformational states** of proteins. The **PDB** has currently archived more than **170,000 individual structures**, but over two thirds of these structures represent **multiple conformations** of the same or related protein, observed in different crystal forms, when interacting with other proteins or other macromolecules, or upon binding small molecule ligands. Charting this conformational diversity across the PDB can therefore be employed to build a useful approximation of the **conformational landscape** of native proteins.\\n\\nA number of resources and **tools** describing and characterizing various often complementary aspects of protein **conformational diversity** in known structures have been developed, notably by groups in Europe. These tools include algorithms with varying degree of sophistication, for aligning the 3D structures of individual protein chains or domains, of protein assemblies, and evaluating their degree of **structural similarity**. Using such tools one can **align structures pairwise**, compute the corresponding **similarity matrix**, and identify ensembles of **structures/conformations** with a defined **similarity level** that tend to recur in different PDB entries, an operation typically performed using **clustering** methods. Such workflows are at the basis of resources such as **CATH, Contemplate, or PDBflex** that offer access to **conformational ensembles** comprised of similar **conformations** clustered according to various criteria. Other types of tools focus on differences between **protein conformations**, identifying regions of proteins that undergo large **collective displacements** in different PDB entries, those that act as **hinges or linkers**, or regions that are inherently **flexible**.\\n\\nTo build a meaningful approximation of the **conformational landscape** of native proteins, the **conformational ensembles** (and the differences between them), identified on the basis of **structural similarity/dissimilarity** measures alone, need to be **biophysically characterized**. This may be approached at **two different levels**. \\n- At the **biological level**, it is important to link observed **conformational ensembles**, to their **functional roles** by evaluating the correspondence with **protein family classifications** based on sequence information and **functional annotations** in public databases e.g. Uniprot, PDKe-Knowledge Base (KB). These links should provide valuable mechanistic insights into how the **conformational and dynamic properties** of proteins are exploited by evolution to regulate their **biological function**. <br><br>\\n\\n- At the **physical level** one needs to introduce **energetic consideration** to evaluate the likelihood that the identified **conformational ensembles** represent **conformational states** that the protein (or domain under study) samples in isolation. Such evaluation is notoriously **challenging** and can only be roughly approximated by using **computational methods** to evaluate the extent to which the observed **conformational ensembles** can be reproduced by algorithms that simulate the **dynamic behavior** of protein systems. These algorithms include the computationally expensive **classical molecular dynamics (MD) simulations** to sample local thermal fluctuations but also faster more approximate methods such as **Elastic Network Models** and **Normal Node Analysis** (NMA) to model low energy **collective motions**. Alternatively, **enhanced sampling molecular dynamics** can be used to model complex types of **conformational changes** but at a very high computational cost. \\n\\nThe **ELIXIR 3D-Bioinfo Implementation Study** *Building on PDBe-KB to chart and characterize the conformation landscape of native proteins* focuses on:\\n\\n1. Mapping the **conformational diversity** of proteins and their homologs across the PDB. \\n2. Characterize the different **flexibility properties** of protein regions, and link this information to sequence and functional annotation.\\n3. Benchmark **computational methods** that can predict a biophysical description of protein motions.\\n\\nThis notebook is part of the third objective, where a list of **computational resources** that are able to predict **protein flexibility** and **conformational ensembles** have been collected, evaluated, and integrated in reproducible and interoperable workflows using the **BioExcel Building Blocks library**. Note that the list is not meant to be exhaustive, it is built following the expertise of the implementation study partners.\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'IDR is based on OMERO and thus all what we show in this notebook can be easily adjusted for use against another OMERO server, e.g. your institutional OMERO server instance.\\n\\nThe main objective of this notebook is to demonstrate how public resources such as the IDR can be used to train your neural network or validate software tools.\\n\\nThe authors of the PLOS Biology paper, \"Nessys: A new set of tools for the automated detection of nuclei within intact tissues and dense 3D cultures\" published in August 2019: https://doi.org/10.1371/journal.pbio.3000388, considered several image segmenation packages, but they did not use the approach described in this notebook.\\n\\nWe will analyse the data using Cellpose and compare the output with the original segmentation produced by the authors. StarDist was not considered by the authors. Our workflow shows how public repository can be accessed and data inside it used to validate software tools or new algorithms.\\n\\nWe will use an image (id=6001247) referenced in the paper. The image can be viewed online in the Image Data Resource (IDR).\\n\\nWe will use a predefined model from Cellpose as a starting point. Steps to access data from IDR could be re-used if you wish to create a new model (outside the scope of this notebook).\\n\\n## Launch\\nThis notebook uses the [environment_cellpose.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/environment_cellpose.yml) file.\\n\\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/setup.md).'\n",
      " 'The notebook shows how to load an IDR image with labels.\\n\\nThe image is referenced in the paper \"NesSys: a novel method for accurate nuclear segmentation in 3D\" published August 2019 in PLOS Biology: https://doi.org/10.1371/journal.pbio.3000388 and can be viewed online in the Image Data Resource.\\n\\nIn this notebook, the image is loaded together with the labels and analyzed using StarDist. The StarDist analysis produces a segmentation, which is then viewed side-by-side with the original segmentations produced by the authors of the paper obtained via the loaded labels.\\n\\n## Launch\\nThis notebook uses the [environment_stardist.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/environment_stardist.yml) file.\\n\\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/setup.md).'\n",
      " '## Learning Objectives\\n- How to access genomic resource via its Python API\\n- How to access image resource via its Python API\\n- Relate image data to genomic data\\n\\n## Diabetes related genes expressed in pancreas\\n\\nThis notebook looks at the question **Which diabetes related genes are expressed in the pancreas?** Tissue and disease can be modified.\\n\\nSteps:\\n\\n- Query [humanmine.org](https://www.humanmine.org/humanmine), an integrated database of Homo sapiens genomic data using the intermine API to find the genes.\\n- Using the list of found genes, search in the [Image Data Resource (IDR)](https://idr.openmicroscopy.org/) for images linked to the genes, tissue and disease.\\n- Analyse the images found.\\n\\n## Launch\\nThis notebook uses the [environment.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/environment.yml) file.\\n\\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_4/setup.md).\\n'\n",
      " '## Learning objectives\\n- Read data to analyse from an object store.\\n- Analyse data in parallel using Dask.\\n- Show how to use public resources to train neural network.\\n- Load labels associated to the original data\\n- Compare results with ground truth.\\n\\nThe authors of the PLOS Biology paper, \"Nessys: A new set of tools for the automated detection of nuclei within intact tissues and dense 3D cultures\" published in August 2019: https://doi.org/10.1371/journal.pbio.3000388, considered several image segmenation packages, but they did not use the approach described in this notebook.\\n\\nWe will analyse the data using [Cellpose](https://www.cellpose.org/) and compare the output with the original segmentation produced by the authors. Cellpose was not considered by the authors. Our workflow shows how public repository can be accessed and data inside it used to validate software tools or new algorithms.\\n\\nWe will use a predefined model from Cellpose as a starting point.\\n\\n## Launch\\nThis notebook uses the [environment.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_5/environment.yml) file.\\n\\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_5/setup.md).'\n",
      " 'The image is referenced in the paper \"NesSys: a novel method for accurate nuclear segmentation in 3D\" published August 2019 in PLOS Biology: https://doi.org/10.1371/journal.pbio.3000388 and can be viewed online in the [Image Data Resource](https://idr.openmicroscopy.org/webclient/?show=image-6001247).\\n\\nThis original image was converted into the Zarr format. The analysis results produced by the authors of the paper were converted into labels and linked to the Zarr file which was placed into a public S3 repository.\\n\\nIn this notebook, the Zarr file is then loaded together with the labels from the S3 storage and analyzed using [StarDist](https://github.com/stardist/stardist). The StarDist analysis produces a segmentation, which is then viewed side-by-side with the original segmentations produced by the authors of the paper obtained via the loaded labels.\\n\\n## Launch\\nThis notebook uses the [environment_stardist.yml](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_5/environment_stardist.yml) file.\\n\\nSee [Setup](https://github.com/ome/EMBL-EBI-imaging-course-05-2023/blob/main/Day_5/setup.md).'\n",
      " '# Introduction\\n\\nThis repository contains all the custom scripts used in the evaluation and comparison of [Katdetectr](https://github.com/ErasmusMC-CCBC/evaluation_katdetectr/tree/main) as described in the corresponding Technical Note (under submission).\\n\\n# Usage\\n\\nAll required files were deposited on [Zenodo](https://zenodo.org/record/6623289#.YqBxHi8Rr0o%5D).\\nThese can directly be downloaded using `zen4R` and be used as input.\\n\\n```R\\n# Increase the timeout (due to some large files).\\noptions(timeout=5000)\\n\\n# Download the required files into the data/ folder (~1GB).\\nzen4R::download_zenodo(doi = \"10.5281/zenodo.6810477\", path = \\'data/\\')\\n```'\n",
      " 'This publication corresponds to the Research Objects (RO) of the Baseline Use Case proposed in T.5.2 (WP5) in the BY-COVID project on “COVID-19 Vaccine(s) effectiveness in preventing SARS-CoV-2 infection”.'\n",
      " '## Purge dups\\n\\nThis snakemake pipeline is designed to be run using as input a contig-level genome and pacbio reads. This pipeline has been tested with `snakemake v7.32.4`. Raw long-read sequencing files and the input contig genome assembly must be given in the `config.yaml` file. To execute the workflow run:\\n\\n`snakemake --use-conda --cores N`\\n\\nOr configure the cluster.json and run using the `./run_cluster` command'\n",
      " '\\nThis repository hosts Metabolome Annotation Workflow (MAW). The workflow takes MS2 .mzML format data files as an input in R. It performs spectral database dereplication using R Package Spectra and compound database dereplication using SIRIUS OR MetFrag . Final candidate selection is done in Python using RDKit and PubChemPy.'\n",
      " 'Simulations and figures supporting the manuscript \"Timing of spring events changes under modelled future climate scenarios in a mesotrophic lake\"'\n",
      " 'Loads a single cell counts matrix into an annData format - adding a column called sample with the sample name.  (Input format - matrix.mtx, features.tsv and barcodes.tsv)'\n",
      " 'Takes fastqs and reference data, to produce a single cell counts matrix into and save in annData format - adding a column called sample with the sample name.  '\n",
      " 'Take a scRNAseq counts matrix from a single sample, and perform basic QC with scanpy. Then, do further processing by making a UMAP and clustering. Produces a processed AnnData \\nobject.\\n\\nDepreciated: use individual workflows insead for multiple samples'\n",
      " \"**Name:** SparseLU  \\n**Contact Person:** support-compss@bsc.es  \\n**Access Level:** public  \\n**License Agreement:** Apache2  \\n**Platform:** COMPSs  \\n\\n# Description\\nThe Sparse LU application computes an LU matrix factorization on a sparse blocked matrix. The matrix size (number of blocks) and the block size are parameters of the application. \\n\\nAs the algorithm progresses, the area of the matrix that is accessed is smaller; concretely, at each iteration, the 0th row and column of the current matrix are discarded. On the other hand, due to the sparseness of the matrix, some of its blocks might not be allocated and, therefore, no work is generated for them.\\n\\nWhen executed with COMPSs, Sparse LU produces several types of task with different granularity and numerous dependencies between them.\\n\\n# Versions\\nThere are three versions of Sparse LU, depending on the data types used to store the blocks.\\n## Version 1\\n''files'', where the matrix blocks are stored in files.\\n## Version 2\\n''objects'', where the matrix blocks are represented by objects.\\n## Version 3\\n''arrays'', where the matrix blocks are stored in arrays.\\n\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss sparseLU.files.SparseLU numberOfBlocks blockSize\\nruncompss sparseLU.objects.SparseLU numberOfBlocks blockSize\\nruncompss sparseLU.arrays.SparseLU numberOfBlocks blockSize\\n```\\n\\nwhere:\\n  * numberOfBlocks: Number of blocks inside each matrix\\n  * blockSize: Size of each block\\n\\n\\n# Execution Example\\n```\\nruncompss sparseLU.objects.SparseLU 16 4 \\nruncompss sparseLU.files.SparseLU 16 4\\nruncompss sparseLU.arrays.SparseLU 16 4 \\n```\\n\\n\\n# Build\\n## Option 1: Native java\\n```\\ncd application_sources/; javac src/main/java/sparseLU/*/*.java\\ncd src/main/java/; jar cf sparseLU.jar sparseLU/\\ncd ../../../; mv src/main/java/sparseLU.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\ncd application_sources/\\nmvn clean package\\n```\\n\"\n",
      " 'The project allowed us to manage and build structured code scripts on the Jupyter Notebook, a simple web application which is user-friendly, flexible to use in the research community. The script is developed to address the specific needs of research between different platforms of dataset.\\nThese stakeholders have developed their own platforms for the annotation and standardisation of both data and metadata produced within their respective field.\\n-The INFRAFRONTIER - European Mutant Mouse Archive (EMMA) comprises over 7200 mutant mouse lines that are extensively integrated and enriched with other public dataset.\\n-The EU-OpenScreen offers compound screening protocols containing several metadata and will contribute to the development of tools for linking to the chemical entity database.\\n-The IDR Image Data Resource is a public repository of reference image datasets from published scientific studies, where the community can submit, search and access high-quality bio-image data. \\n-The CIM-XNAT is an XNAT deployment of the Molecular Imaging Center at UniTo that offers a suite of tools for uploading preclinical images.\\nTo address the challenges of integrating several EU-RI datasets with focus on preclinical and discovery research bioimaging, our aim is to develop cross researching queries through a web based interface  to combine the resources of the RIs for integrating the information associated with data belonging to the involved RIs. Furthermore, the open-source tool provides users with free, open access to collections of datasets distributed over multiple sources that result from searches by specific keywords. \\nThe script allows the cross research in different fields of research as: Species, Strain, Gene, Cell line, Disease model, Chemical Compound.\\nThe novel aspects of this tool are mainly:\\na) user friendly, e.g. the user has the flexibility to research among the dataset easily with a simple API, intuitive for researchers and biomedical users.  \\nb) the possibility of making a research between different platforms and repositories, from a unique simple way. \\nc) the workflow project follows the FAIR principles in the treatment of data and datasets. \\nThe access to Notebook Jupyter needs the installation of Anaconda, which consents to open the web application. \\nInside the Jupyter, the script was built using Python. The query code is also easy to download and share in a .ipynb file.\\nA visual representation of the detailed results (dataset, metadata, information, query results) of the workflow can be printed immediately after the query run. \\n'\n",
      " 'SINGLE-END workflow. \\nAlign reads on fasta reference/assembly using bwa mem, get a consensus, variants, mutation explanations. \\n\\nIMPORTANT: \\n* For \"bcftools call\" consensus step, the --ploidy file is in \"Données partagées\" (Shared Data) and must be imported in your history to use the worflow by providing this file (tells bcftools to consider haploid variant calling). \\n* SELECT the mot ADAPTED VADR MODEL for annotation (see vadr parameters).'\n",
      " 'PAIRED-END workflow. Align reads on fasta reference/assembly using bwa mem, get a consensus, variants, mutation explanations.\\n\\nIMPORTANT: \\n* For \"bcftools call\" consensus step, the --ploidy file is in \"Données partagées\" (Shared Data) and must be imported in your history to use the worflow by providing this file (tells bcftools to consider haploid variant calling). \\n* SELECT THE MOST ADAPTED VADR MODEL for annotation (see vadr parameters).\\n'\n",
      " 'SARS-CoV-2 variant prediction using Read It And Keep, fastp, bbmap and iVar'\n",
      " 'A CWL-based pipeline for processing RNA-Seq data (FASTQ format) and performing differential gene/transcript expression analysis. \\n\\nOn the respective GitHub folder are available:\\n\\n- The CWL wrappers for the workflow\\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\\n- A table of metadata (``mrna_cll_subsets_phenotypes.csv``), based on the same validation analysis, to serve as an input example for the design of comparisons during differential expression analysis\\n\\nBriefly, the workflow performs the following steps:\\n\\n1. Quality control of Illumina reads (FastQC)\\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\\n3. (Optional)  custom processing of the reads using FASTA/Q Trimmer (part of the FASTX-toolkit) \\n4. Mapping to reference genome (HISAT2)\\n5. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\\n6. Sorting mapped reads based on chromosomal coordinates (samtools)\\n\\nSubsequently, two independent workflows are implemented for differential expression analysis at the transcript and gene level. \\n\\n**First**, following the [reference protocol](https://doi.org/10.1038/nprot.2016.095) for HISAT, StringTie and Ballgown transcript expression analysis, StringTie along with a reference transcript annotation GTF (Gene Transfer Format) file (if one is available) is used to:\\n\\n- Assemble transcripts for each RNA-Seq sample using the previous read alignments (BAM files)\\n- Generate a global, non-redundant set of transcripts observed in any of the RNA-Seq samples\\n- Estimate transcript abundances and generate read coverage tables for each RNA-Seq sample, based on the global, merged set of transcripts (rather than the reference) which is observed across all samples\\n\\nBallgown program is then used to load the coverage tables generated in the previous step and perform statistical analyses for differential expression at the transcript level. Notably, the StringTie - Ballgown protocol applied here was selected to include potentially novel transcripts in the analysis. \\n\\n**Second**, featureCounts is used to count reads that are mapped to selected genomic features, in this case genes by default, and generate a table of read counts per gene and sample. This table is passed as input to DESeq2 to perform differential expression analysis at the gene level. Both Ballgown and DESeq2 R scripts, along with their respective CWL wrappers, were designed to receive as input various parameters, such as experimental design, contrasts of interest, numeric thresholds, and hidden batch effects.\\n'\n",
      " 'A CWL-based pipeline for processing ChIP-Seq data (FASTQ format) and performing: \\n\\n- Peak calling\\n- Consensus peak count table generation\\n- Detection of super-enhancer regions\\n- Differential binding analysis\\n\\nOn the respective GitHub folder are available:\\n\\n- The CWL wrappers for the workflow\\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\\n- Tables of metadata (``EZH2_metadata_CLL.csv`` and ``H3K27me3_metadata_CLL.csv``), based on the same validation analysis, to serve as input examples for the design of comparisons during differential binding analysis\\n- A list of ChIP-Seq blacklisted regions (human genome version 38; hg38) from the ENCODE project, which is can be used as input for the workflow, is provided in BED format (``hg38-blacklist.v2.bed``)\\n\\nBriefly, the workflow performs the following steps:\\n\\n1. Quality control of short reads (FastQC)\\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trimmomatic)\\n3. Mapping to reference genome (HISAT2)\\n5. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\\n6. Sorting mapped reads based on chromosomal coordinates (samtools)\\n7. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\\n8. Re-sorting based on chromosomal coordinates (samtools)\\n9. Removal of duplicate reads (samtools)\\n10. Index creation for coordinate-sorted BAM files to enable fast random access (samtools)\\n11. Production of quality metrics and files for the inspection of the mapped ChIP-Seq reads, taking into consideration the experimental design (deeptools2):\\n - Read coverages for genomic regions of two or more BAM files are computed (multiBamSummary). The results are produced in compressed numpy array (NPZ) format and are used to calculate and visualize pairwise correlation values between the read coverages (plotCorrelation). \\n - Estimation of sequencing depth, through genomic position (base pair) sampling, and visualization is performed for multiple BAM files (plotCoverage).\\n - Cumulative read coverages for each indexed BAM file are plotted by counting and sorting all reads overlapping a “window” of specified length (plotFingerprint).\\n - Production of coverage track files (bigWig), with the coverage calculated as the number of reads per consecutive windows of predefined size (bamCoverage), and normalized through various available methods (e.g., Reads Per Kilobase per Million mapped reads; RPKM). The coverage track files are used to calculate scores per selected genomic regions (computeMatrix), typically genes, and a heatmap, based on the scores associated with these genomic regions, is produced (plotHeatmap).\\n12. Calling potential binding positions (peaks) to the genome (peak calling) (MACS2)\\n13. Generation of consensus peak count table for the application of custom analyses on MACS2 peak calling results (bedtools)\\n14. Detection of super-enhancer regions (Rank Ordering of Super-Enhancers; ROSE)\\n15. Differential binding analyses (DiffBind) for:\\n - MACS2 peak calling results\\n - ROSE-detected super-enhancer regions \\n '\n",
      " 'A CWL-based pipeline for calling small germline variants, namely SNPs and small INDELs, by processing data from Whole-genome Sequencing (WGS) or Targeted Sequencing (e.g., Whole-exome sequencing; WES) experiments. \\n\\nOn the respective GitHub folder are available:\\n\\n- The CWL wrappers and subworkflows for the workflow\\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\\n\\nBriefly, the workflow performs the following steps:\\n\\n1. Quality control of Illumina reads (FastQC)\\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\\n3. Mapping to reference genome (BWA-MEM)\\n4. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\\n5. Sorting mapped reads based on read names (samtools)\\n6. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\\n7. Re-sorting mapped reads based on chromosomal coordinates (samtools)\\n8. Adding basic Read-Group information regarding sample name, platform unit, platform (e.g., ILLUMINA), library and identifier (picard AddOrReplaceReadGroups)\\n9. Marking PCR and/or optical duplicate reads (picard MarkDuplicates)\\n10. Collection of summary statistics (samtools) \\n11. Creation of indexes for coordinate-sorted BAM files to enable fast random access (samtools)\\n12. Splitting the reference genome into a predefined number of intervals for parallel processing (GATK SplitIntervals)\\n\\nAt this point the application of multi-sample workflow follows, during which multiple samples are concatenated into a single, unified VCF (Variant Calling Format) file, which contains the variant information for all samples:\\n\\n13. Application of Base Quality Score Recalibration (BQSR) (GATK BaseRecalibrator and ApplyBQSR tools)\\n14. Variant calling in gVCF (genomic VCF) mode (-ERC GVCF) (GATK HaplotypeCaller)  \\n15. Merging of all genomic interval-split gVCF files for each sample (GATK MergeVCFs)\\n16. Generation of the unified VCF file (GATK CombineGVCFs and GenotypeGVCFs tools)\\n17. Separate annotation for SNP and INDEL variants, using the Variant Quality Score Recalibration (VQSR) method (GATK VariantRecalibrator and ApplyVQSR tools)\\n18. Variant filtering based on the information added during VQSR and/or custom filters (bcftools)\\n19. Normalization of INDELs (split multiallelic sites) (bcftools)\\n20. Annotation of the final dataset of filtered variants with genomic, population-related and/or clinical information (ANNOVAR)\\n'\n",
      " 'A CWL-based pipeline for calling small germline variants, namely SNPs and small INDELs, by processing data from Whole-genome Sequencing (WGS) or Targeted Sequencing (e.g., Whole-exome sequencing; WES) experiments.\\n\\nOn the respective GitHub folder are available:\\n\\n- The CWL wrappers and subworkflows for the workflow\\n- A pre-configured YAML template, based on validation analysis of publicly available HTS data\\n\\nBriefly, the workflow performs the following steps:\\n\\n1. Quality control of Illumina reads (FastQC)\\n2. Trimming of the reads (e.g., removal of adapter and/or low quality sequences) (Trim galore)\\n3. Mapping to reference genome (BWA-MEM)\\n4. Convertion of mapped reads from SAM (Sequence Alignment Map) to BAM (Binary Alignment Map) format (samtools)\\n5. Sorting mapped reads based on read names (samtools)\\n6. Adding information regarding paired end reads (e.g., CIGAR field information) (samtools)\\n7. Re-sorting mapped reads based on chromosomal coordinates (samtools)\\n8. Adding basic Read-Group information regarding sample name, platform unit, platform (e.g., ILLUMINA), library and identifier (picard AddOrReplaceReadGroups)\\n9. Marking PCR and/or optical duplicate reads (picard MarkDuplicates)\\n10. Collection of summary statistics (samtools) \\n11. Creation of indexes for coordinate-sorted BAM files to enable fast random access (samtools)\\n12. Splitting the reference genome into a predefined number of intervals for parallel processing (GATK SplitIntervals)\\n\\nAt this point the application of single-sample workflow follows, during which multiple samples are accepted as input and they are not merged into a unified VCF file but are rather processed separately in each step of the workflow, leading to the production of a VCF file for each sample:\\n\\n13. Application of Base Quality Score Recalibration (BQSR) (GATK BaseRecalibrator, GatherBQSRReports and ApplyBQSR tools)\\n14. Variant calling (GATK HaplotypeCaller)  \\n15. Merging of all genomic interval-split gVCF files for each sample (GATK MergeVCFs)\\n16. Separate annotation of SNPs and INDELs based on pretrained Convolutional Neural Network (CNN) models (GATK SelectVariants, CNNScoreVariants and FilterVariantTranches tools)\\n17. (Optional) Independent step of hard-filtering (GATK VariantFiltration)\\n18. Variant filtering based on the information added during VQSR and/or custom filters (bcftools)\\n19. Normalization of INDELs (split multiallelic sites) (bcftools)\\n20. Annotation of the final dataset of filtered variants with genomic, population-related and/or clinical information (ANNOVAR)\\n'\n",
      " \"# prepareChIPs\\n\\nThis is a simple `snakemake` workflow template for preparing **single-end** ChIP-Seq data.\\nThe steps implemented are:\\n\\n1. Download raw fastq files from SRA\\n2. Trim and Filter raw fastq files using `AdapterRemoval`\\n3. Align to the supplied genome using `bowtie2`\\n4. Deduplicate Alignments using `Picard MarkDuplicates`\\n5. Call Macs2 Peaks using `macs2`\\n\\nA pdf of the rulegraph is available [here](workflow/rules/rulegraph.pdf)\\n\\nFull details for each step are given below.\\nAny additional parameters for tools can be specified using `config/config.yml`, along with many of the requisite paths\\n\\nTo run the workflow with default settings, simply run as follows (after editing `config/samples.tsv`)\\n\\n```bash\\nsnakemake --use-conda --cores 16\\n```\\n\\nIf running on an HPC cluster, a snakemake profile will required for submission to the queueing system and appropriate resource allocation.\\nPlease discuss this will your HPC support team.\\nNodes may also have restricted internet access and rules which download files may not work on many HPCs.\\nPlease see below or discuss this with your support team\\n\\nWhilst no snakemake wrappers are explicitly used in this workflow, the underlying scripts are utilised where possible to minimise any issues with HPC clusters with restrictions on internet access.\\nThese scripts are based on `v1.31.1` of the snakemake wrappers\\n\\n### Important Note Regarding OSX Systems\\n\\nIt should be noted that this workflow is **currently incompatible with OSX-based systems**. \\nThere are two unsolved issues\\n\\n1. `fasterq-dump` has a bug which is specific to conda environments. This has been updated in v3.0.3 but this patch has not yet been made available to conda environments for OSX. Please check [here](https://anaconda.org/bioconda/sra-tools) to see if this has been updated.\\n2. The following  error appears in some OSX-based R sessions, in a system-dependent manner:\\n```\\nError in grid.Call(C_textBounds, as.graphicsAnnot(x$label), x$x, x$y,  : \\n  polygon edge not found\\n```\\n\\nThe fix for this bug is currently unknown\\n\\n## Download Raw Data\\n\\n### Outline\\n\\nThe file `samples.tsv` is used to specify all steps for this workflow.\\nThis file must contain the columns: `accession`, `target`, `treatment` and `input`\\n\\n1. `accession` must be an SRA accession. Only single-end data is currently supported by this workflow\\n2. `target` defines the ChIP target. All files common to a target and treatment will be used to generate summarised coverage in bigWig Files\\n3. `treatment` defines the treatment group each file belongs to. If only one treatment exists, simply use the value 'control' or similar for every file\\n4. `input` should contain the accession for the relevant input sample. These will only be downloaded once. Valid input samples are *required* for this workflow\\n\\nAs some HPCs restrict internet access for submitted jobs, *it may be prudent to run the initial rules in an interactive session* if at all possible.\\nThis can be performed using the following (with 2 cores provided as an example)\\n\\n```bash\\nsnakemake --use-conda --until get_fastq --cores 2\\n```\\n\\n### Outputs\\n\\n- Downloaded files will be gzipped and written to `data/fastq/raw`.\\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/raw`\\n\\nBoth of these directories are able to be specified as relative paths in `config.yml`\\n\\n## Read Filtering\\n\\n### Outline\\n\\nRead trimming is performed using [AdapterRemoval](https://adapterremoval.readthedocs.io/en/stable/).\\nDefault settings are customisable using config.yml, with the defaults set to discard reads shorter than 50nt, and to trim using quality scores with a threshold of Q30.\\n\\n### Outputs\\n\\n- Trimmed fastq.gz files will be written to `data/fastq/trimmed`\\n- `FastQC` and `MultiQC` will also be run, with output in `docs/qc/trimmed`\\n- AdapterRemoval 'settings' files will be written to `output/adapterremoval`\\n\\n## Alignments\\n\\n### Outline\\n\\nAlignment is performed using [`bowtie2`](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml) and it is assumed that this index is available before running this workflow.\\nThe path and prefix must be provided using config.yml\\n\\nThis index will also be used to produce the file `chrom.sizes` which is essential for conversion of bedGraph files to the more efficient bigWig files.\\n\\n### Outputs\\n\\n- Alignments will be written to `data/aligned`\\n- `bowtie2` log files will be written to `output/bowtie2` (not the conenvtional log directory)\\n- The file `chrom.sizes` will be written to `output/annotations`\\n\\nBoth sorted and the original unsorted alignments will be returned.\\nHowever, the unsorted alignments are marked with `temp()` and can be deleted using \\n\\n```bash\\nsnakemake --delete-temp-output --cores 1\\n```\\n\\n## Deduplication\\n\\n### Outline\\n\\nDeduplication is performed using [MarkDuplicates](https://gatk.broadinstitute.org/hc/en-us/articles/360037052812-MarkDuplicates-Picard-) from the Picard set of tools.\\nBy default, deduplication will remove the duplicates from the set of alignments.\\nAll resultant bam files will be sorted and indexed.\\n\\n### Outputs\\n\\n- Deduplicated alignments are written to `data/deduplicated` and are indexed\\n- DuplicationMetrics files are written to `output/markDuplicates`\\n\\n## Peak Calling\\n\\n### Outline\\n\\nThis is performed using [`macs2 callpeak`](https://pypi.org/project/MACS2/).\\n\\n- Peak calling will be performed on:\\n    a. each sample individually, and \\n    b. merged samples for those sharing a common ChIP target and treatment group.\\n- Coverage bigWig files for each individual sample are produced using CPM values (i.e. Signal Per Million Reads, SPMR)\\n- For all combinations of target and treatment coverage bigWig files are also produced, along with fold-enrichment bigWig files\\n\\n### Outputs\\n\\n- Individual outputs are written to `output/macs2/{accession}`\\n\\t+ Peaks are written in `narrowPeak` format along with `summits.bed`\\n\\t+ bedGraph files are automatically converted to bigWig files, and the originals are marked with `temp()` for subsequent deletion\\n\\t+ callpeak log files are also added to this directory\\n- Merged outputs are written to `output/macs2/{target}/`\\n\\t+ bedGraph Files are also converted to bigWig and marked with `temp()`\\n\\t+ Fold-Enrichment bigWig files are also created with the original bedGraph files marked with `temp()`\\n\"\n",
      " 'Sample workflow template that combines simulations with data analytics. It is not a real workflow, but it mimics this type of workflows. It illustrates how COMPSs invokes binaries. It can be extended to invoke MPI applications. '\n",
      " '## ARA (Automated Record Analysis) : An automatic pipeline for exploration of SRA datasets with sequences as a query\\n\\n### Requirements\\n\\n- **Docker**\\n\\n  - Please checkout the [Docker installation](https://docs.docker.com/get-docker/) guide.\\n\\n    _or_\\n\\n- **Mamba package manager**\\n\\n  - Please checkout the [mamba or micromamba](https://mamba.readthedocs.io/en/latest/installation.html) official installation guide.\\n\\n  - We prefer `mamba` over [`conda`](https://docs.conda.io/en/latest/) since it is faster and uses `libsolv` to effectively resolve the dependencies.\\n\\n  - `conda` can still be used to install the pipeline using the same commands as described in the installation section.\\n\\n    > Note: **It is important to include the \\'bioconda\\' channel in addition to the other channels as indicated in the [official manual](https://bioconda.github.io/#usage \"Bioconda - Usage\")**. Use the following commands in the given order to configure the channels (one-time setup).\\n    >\\n    > ```bash\\n    > conda config --add channels defaults\\n    > conda config --add channels bioconda\\n    > conda config --add channels conda-forge\\n    > conda config --set channel_priority strict\\n    > ```\\n\\n---\\n\\n### Installation\\n\\nThe user can install the pipeline by using either Docker or Mamba using the steps mentioned below.\\n\\nFirst, click the green \"Code\" button, then select \"Download Zip\" to begin downloading the contents of this repository. Once the download is complete, extract the zip file by into the desired location before starting the setup. Please use the commands shown below to begin installing the pipeline.\\n\\nAlternatively, the github repo can also be cloned through the options shown after clicking the \"Code\" button. Navigate inside the folder after by using the `cd ARA/` command before starting the setup.\\n\\n> _Warning: Before starting any analysis with the pipeline, please make sure that the system has enough disk space available for the data you wish to retrieve and process from the SRA repository._\\n\\n- **Using Docker**\\n\\n  ```bash\\n  cd ARA-main/\\n  docker build -t ara_img .\\n  ```\\n\\n_or_\\n\\n- **Using Mamba**\\n\\n  ```bash\\n  cd ARA-main/\\n  mamba env create --file requirements.yaml\\n  mamba activate ara_env\\n  perl setup.pl\\n  ```\\n\\n  > _Note: After installation, the virtual environment consumes approximately 1.5 GB of disk space. The installation was tested on \"Ubuntu 20.04.4 LTS\", \"Ubuntu 22.04.1 LTS\" and \"Fedora 37\" using the procedure mentioned above._\\n\\nPlease be patient because downloading and configuring the tools/modules may take several minutes. The warning messages that appear during the installation of certain Perl modules can be ignored by users.\\n\\nOptional: The user can also add the current directory to PATH for ease of use. Use the `chmod +x ara.pl` followed by `export PATH=\"$(pwd):$PATH\"` command. Alternatively, the user is free to create symbolic, copy the executable to `/bin/`, or use any other method depending on their operating system.\\n\\nRefer the \\'Troubleshooting\\' section in case of any installation related issues.\\n\\n---\\n\\n### Example usage\\n\\n- **Docker**\\n\\n  `docker run -it ara_img /home/ARA-main/ara.pl --input /home/ARA-main/example/SraRunInfo.csv --sequences /home/ARA-main/example/Arabidopsis_thaliana.TAIR10.ncrna.fa`\\n\\n- **Mamba environment**\\n\\n  `perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`\\n\\nTo get full usage info: `perl ara.pl --help`\\n\\n> _Note_: The user can delete the contents of `results/` directory after testing the tool using the example mentioned above.\\n\\n### Configuration file\\n\\nThe configuration file `conf.txt` is automatically generated during the installation by setup script. It contains certain default parameters as well as the location to the executable binaries of the tools incorporated in the pipeline.\\n\\nThe user can modify the default parameters in `conf.txt` and pass it to the pipeline as an input. For example, the `data_perc` option in the configuration refers to the default value of 5% of the dataset selected for analysis. However, the user has the flexibility to provide any integer value between 1 and 100 to specify the desired percentage of the dataset to be used.\\n\\nSimilarly, the user can choose between _blastn_ or _bowtie2_ by changing the \\'execute flag\\' to either 0 or 1 in the configuration file while leaving the rest of the parameters to default values. By default, both the tools are enabled _ie_. `execute = 1`.\\n\\nThe `read_drop_perc_cutoff` in `conf.txt` config file denotes the cutoff to discard a sample if the total reads left after executing the trimmomatic are higher than the threshold (by default, if the more than 70% of reads are dropped as per the trimmomatic log, then the sample will fail the quality criteria and will not be processed downstream). Please refer the documentation of [Trimmomatic ](https://github.com/usadellab/Trimmomatic) for more details about the parameters present in the config file.\\n\\nSimilarly, the criteria to check the minimal alignment rate are indicated by the `alignment perc cutoff` parameter under blastn and bowtie2 in the `conf.txt` configuration file (if the total alignment percentage is less than the threshold then the pipeline will report that the sample failed the quality criteria). More details about the parameters used in the `conf.txt` file can be found in the respective documentations of [Blastn](https://www.ncbi.nlm.nih.gov/books/NBK279690/) and [Bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml).\\n\\nBy default, the pipeline uses a pre-built Kraken2 viral genomic database ([release: 9/8/2022](https://genome-idx.s3.amazonaws.com/kraken/k2_viral_20220908.tar.gz)) from <https://benlangmead.github.io/aws-indexes/k2>. Users can provide their own database by changing the `kraken2_db_path` parameter in the `conf.txt` file.\\n\\n> _Note:_ If the user wishes to use a different installation than Bioconda, the user can manually install the required tools and specify the absolute path of the executable binaries in the configuration.\\n\\n---\\n\\n### Pipeline parameters\\n\\n- **`--input`** (mandatory) The user can provide input in either of the following ways:\\n\\n  - A single SRA run accession. eg: **`perl ara.pl --input SRR12548227 --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\\n\\n  - A list of run accessions in a text file (1 run accession per line). eg: **`perl ara.pl --input example/list.txt --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\\n\\n  - The SRA runInfo exported directly from the NCBI-SRA web portal. Goto the [SRA homepage](https://www.ncbi.nlm.nih.gov/sra \"Home - NCBI - SRA\") and search for the desired keyword. Export the `SraRunInfo.csv` by clicking \\'Send to\\' =\\\\> File =\\\\> RunInfo). eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa`**\\n\\n- **`--sequences`** (mandatory) The user should provide a fasta file containing the query sequences.\\n\\n- **`--output`** (optional) The output directory to store the results. By default, the output will be stored into the **`results/`** directory of the package. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/`**\\n\\n- **`--mode`** (optional) Choose one of the three modes to run the pipeline.\\n\\n  - The **`screen`** is the default mode which will only download a fraction of the data-set per SRA-run accession and analyse the file as per the given configuration.\\n\\n  - The **`full`** mode will execute the pipeline by downloading the complete fastq file per SRA-run accession.\\n\\n  - The **`both`** option searches for samples using a fraction of the data that meet the minimum alignment cutoff from either \\'bowtie2\\' or \\'blastn\\', and then automatically performs alignment by downloading the entire fastq file. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/ --mode screen`**\\n\\n    > _Note:_ There is a supporting **`summary`** mode, that will generate a unified alignment summary by examining the output files created by either screen-mode or full-mode. The summary mode should only be used when the user needs to recreate the summary stats from the pre-existing results. The user must enter **`–mode summary`** along with the previously used command parameters to re-generate the summary.\\n\\n  - **`--config`** (optional) Pipeline configuration. By default it will use the **`conf.txt`** generated by the setup script. eg: **`perl ara.pl --input example/SraRunInfo.csv --sequences example/Arabidopsis_thaliana.TAIR10.ncrna.fa --output /src/main/test/ --mode screen --config conf.txt`**\\n\\n---\\n\\n### Output structure\\n\\nThe pipeline will create folders per SRA run accession and generate results using the run accession as the prefix. The analysis related to the screening a fraction of data will be stored in `screening_results` directory whereas the analysis conducted on the whole dataset will be stored in `full_analyis_results` directory.\\n\\nAn outline of directory structure containing the results is shown below-\\n\\n    results/\\n    `-- test/ (name derived from the input fasta sequence file)\\n        |-- test.screening.analysis.stats.sorted.by.alignment.txt (combined metadata and analysis report generated after processing all the SRA run accessions, sorted in decreasing order of total alignment percentage)\\n        |-- metadata/\\n        |   |-- test.metadata.txt (Combined metadata downloaded from SRA)\\n        |   |-- test.metadata.screened.txt (List of SRA accessions which qualify the filter criteria specified in the config.)\\n        |   |-- SRA_RUN.run.metadata.txt (unprocessed metadata on a single SRA accession as retrieved from NCBI)\\n        |-- reference/\\n        |   |-- blastn_db/ (folder containing the blast database created from the input fasta sequence)\\n        |   |-- bowtie2_index/ (folder containing the bowtie index created from the input fasta sequence)\\n        |   |-- bowtie2_index.stdout.txt (stdout captured from bowtie2 index creation)\\n        |   `-- makeblastdb.stdout.txt (stdout captured from blastn database creation)\\n        `-- screening_results/ (similar structure for screeing or full mode)\\n            |-- SRA_RUN/ (each SRA run accession will be processed into a seperate folder)\\n            |   |-- blastn/\\n            |   |   |-- SRA_RUN.blast.results.txt (output from NCBI Blastn)\\n            |   |   `-- blast.stats.txt (blastn overall alignment stats)\\n            |   |-- bowtie2/\\n            |   |   |-- SRA_RUN.bam (output from bowtie2)\\n            |   |   |-- alignment.stats.txt (bowtie2 stdout)\\n            |   |   `-- alignment.txt (bowtie2 overall alignment summary)\\n            |   |-- fastQC/\\n            |   |   |-- <Raw data FastQC report>\\n            |   |   |-- <Adapter trimmed FastQC report>\\n            |   |-- kraken2/\\n            |   |   |-- SRA_RUN.kraken (kraken2 standard classification table)\\n            |   |   |-- SRA_RUN.report (kraken2 classification report)\\n            |   |   `-- SRA_RUN.stdout.txt (kraken2 stdout)\\n            |   |-- raw_fastq/\\n            |   |   |-- <Downloaded single end or paired end fastq file(s)>\\n            |   |   |-- fastq_dump.stdout.txt\\n            |   |   |-- sra/\\n            |   |   `-- wget.full.sra.stdout.txt\\n            |   `-- trimmed_data/\\n            |       |-- <Adapter trimmed single end or paired end fastq file(s)>\\n            |       `-- SRA_RUN_trim_stdout_log.txt (trimmomatic stdout)\\n            `-- runlog.SRA_RUN.txt (Complete run log of the pipeline per SRA run accession)\\n\\nFor a thorough understanding of the results of the third-party tools, take a look at the following documentations:\\n\\n- [Blastn](https://www.ncbi.nlm.nih.gov/books/NBK279690/)\\n- [Bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/manual.shtml)\\n- [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\\n- [Kraken2](https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown)\\n- [Trimmomatic](https://github.com/usadellab/Trimmomatic)\\n\\n---\\n\\n### Disk usage using the input from the example\\n\\nThe table below provides a summary of the disk usage for different analyses conducted on varying dataset sizes. It demonstrates how disk usage can increase depending on the choice of the fraction of the dataset the user wishes to analyze.\\n\\n| RUN ACCESSION | 100% of dataset | 5% of dataset | 10% of dataset |\\n| ------------- | --------------- | ------------- | -------------- |\\n| SRR8392720    | 1.3G            | 85M           | 156M           |\\n| SRR7289585    | 1.4G            | 150M          | 288M           |\\n| SRR12548227   | 15M             | 9.0M          | 9.1M           |\\n\\nThis summary highlights how the disk usage (in megabytes or gigabytes) can vary depending on the chosen fraction of the dataset for analysis.\\n\\n---\\n\\n### Troubleshooting\\n\\n- Errors related to mamba/conda environment:\\n\\n  Since `mamba` is a drop-in replacement and uses the same commands and configuration options as **conda**, it\\'s possible to swap almost all commands between **conda** & **mamba**.\\n\\n  Use **`conda list`** command to verify whether the packages mentioned in the `requirements.yaml` are successfully installed into your environment.\\n\\n  > _Note:_ The `requirements.yaml` provided in this package was exported from `mamba 0.25.0` installation running on `Ubuntu 20.04.4 LTS`.\\n\\n  In case of any missing tool/ conflicting dependencies in the environment, the user can try using **`conda search <tool name>`** or `mamba repoquery search <tool name>` command to find the supported version of the tool and then manually install it by typing **`conda install <tool name>`** or `mamba install <tool name>` inside the environment. Please refer the official [troubleshooting guide](https://conda.io/projects/conda/en/latest/user-guide/troubleshooting.html \"User guide » Troubleshooting\") for further help.\\n\\n  > _Note:_ On macOS and Linux, the supported tools and their dependencies aren\\'t always the same. Even when all of the requirements are completely aligned, the set of available versions isn\\'t necessarily the same. User may try setting up the environment using any of the supplementary `requirements-*.txt` provided in the `src/main/resources/` directory.\\n\\n- Error installing Perl modules:\\n\\n  Users must ensure that they have write permission to the `/Users/\\\\*/.cpan/` or similar directory, and the CPAN is properly configured.\\n\\n  You might need to define the PERLLIB/PERL5LIB environment variable if you see an error similar to the following:\\n\\n  ```bash\\n      Cant locate My/Module.pm in @INC (@INC contains:\\n      ...\\n      ...\\n      .).\\n      BEGIN failed--compilation aborted.\\n  ```\\n\\n  > _Note about MAKE_: \\'make\\' is an essential tool for building Perl modules. Please make sure that you have \\'make\\' installed in your system. The setup script provided in this package utilizes \\'cpan\\' to build the required Perl modules automatically.\\n\\n  If the automatic setup provided in the package fails to install the required dependencies, you may need to install them manually by using the command `cpan install <module name>` or searching the package on [Metacpan](https://metacpan.org/).\\n\\n  Additionally, some Perl modules can also be installed through `mamba` (eg. the compatible version of Perl module `Config::Simple` can be searched on mamba by `mamba repoquery search perl-config-simple`)\\n\\n---\\n\\n### List of Perl modules and tools incorporated in the pipeline\\n\\n- Perl modules:\\n\\n  - Config::Simple\\n  - Parallel::ForkManager\\n  - Log::Log4perl\\n  - Getopt::Long\\n  - Text::CSV\\n  - Text::Unidecode\\n\\n- Tools:\\n\\n  - [NCBI EDirect utilities \\\\>=16.2](https://www.ncbi.nlm.nih.gov/books/NBK179288/)\\n  - [NCBI SRA Toolkit \\\\>=2.10.7](https://www.ncbi.nlm.nih.gov/home/tools/)\\n  - [FastQC \\\\>=0.11.9](https://www.bioinformatics.babraham.ac.uk/projects/download.html#fastqc)\\n  - [Trimmomatic \\\\>=0.39](http://www.usadellab.org/cms/?page=trimmomatic)\\n  - [FASTX-Toolkit \\\\>=0.0.14](http://hannonlab.cshl.edu/fastx_toolkit/)\\n  - [NCBI Blast \\\\>=2.10.1](https://blast.ncbi.nlm.nih.gov/Blast.cgi?PAGE_TYPE=BlastDocs&DOC_TYPE=Download)\\n  - [Bowtie2 \\\\>=2.4.5](http://bowtie-bio.sourceforge.net/bowtie2/index.shtml)\\n  - [Samtools \\\\>=1.15.1](http://www.htslib.org/download/)\\n  - [Kraken2 \\\\>=2.1.2](https://ccb.jhu.edu/software/kraken2/)\\n\\n---\\n'\n",
      " '# GERONIMO\\n\\n## Introduction\\nGERONIMO is a bioinformatics pipeline designed to conduct high-throughput homology searches of structural genes using covariance models. These models are based on the alignment of sequences and the consensus of secondary structures. The pipeline is built using Snakemake, a workflow management tool that allows for the reproducible execution of analyses on various computational platforms.  \\n\\nThe idea for developing GERONIMO emerged from a comprehensive search for [telomerase RNA in lower plants] and was subsequently refined through an [expanded search of telomerase RNA across Insecta]. GERONIMO can test hundreds of genomes and ensures the stability and reproducibility of the analyses performed.\\n\\n\\n[telomerase RNA in lower plants]: https://doi.org/10.1093/nar/gkab545\\n[expanded search of telomerase RNA across Insecta]: https://doi.org/10.1093/nar/gkac1202\\n\\n## Scope\\nThe GERONIMO tool utilises covariance models (CMs) to conduct homology searches of RNA sequences across a wide range of gene families in a broad evolutionary context. Specifically, it can be utilised to:\\n\\n* Detect RNA sequences that share a common evolutionary ancestor\\n* Identify and align orthologous RNA sequences among closely related species, as well as paralogous sequences within a single species\\n* Identify conserved non-coding RNAs in a genome, and extract upstream genomic regions to characterise potential promoter regions.  \\nIt is important to note that GERONIMO is a computational tool, and as such, it is intended to be run on a computer with a small amount of data. Appropriate computational infrastructure is necessary for analysing hundreds of genomes.\\n\\nAlthough GERONIMO was primarily designed for Telomerase RNA identification, its functionality extends to include the detection and alignment of other RNA gene families, including **rRNA**, **tRNA**, **snRNA**, **miRNA**, and **lncRNA**. This can aid in identifying paralogs and orthologs across different species that may carry specific functions, making it useful for phylogenetic analyses.  \\n\\nIt is crucial to remember that some gene families may exhibit similar characteristics but different functions. Therefore, analysing the data and functional annotation after conducting the search is essential to characterise the sequences properly.\\n\\n## Pipeline overview\\n\\n\\nBy default, the GERONIMO pipeline conducts high-throughput searches of homology sequences in downloaded genomes utilizing covariance models. If a significant similarity is detected between the model and genome sequence, the pipeline extracts the upstream region, making it convenient to identify the promoter of the discovered gene. In brief, the pipeline:\\n- Compiles a list of genomes using the NCBI\\'s [Entrez] database based on a specified query, *e.g. \"Rhodophyta\"[Organism]*\\n- Downloads and decompresses the requested genomes using *rsync* and *gunzip*, respectively\\n- *Optionally*, generates a covariance model based on a provided alignment using [Infernal]\\n- Conducts searches among the genomes using the covariance model [Infernal]\\n- Supplements genome information with taxonomy data using [rentrez]\\n- Expands the significant hits sequence by extracting upstream genomic regions using [*blastcmd*]\\n- Compiles the results, organizes them into a tabular format, and generates a visual summary of the performed analysis.\\n\\n[Entrez]: https://www.ncbi.nlm.nih.gov/books/NBK179288/\\n[Infernal]: http://eddylab.org/infernal/\\n[rentrez]: https://github.com/ropensci/rentrez\\n[*blastcmd*]: https://www.ncbi.nlm.nih.gov/books/NBK569853/\\n\\n## Quick start\\nThe GERONIMO is available as a `snakemake pipeline` running on Linux and Windows operating systems.\\n\\n### Windows 10\\nInstal Linux on Windows 10 (WSL) according to [instructions], which bottling down to opening PowerShell or Windows Command Prompt in *administrator mode* and pasting the following:\\n```shell\\nwsl --install\\nwsl.exe --install UBUNTU\\n```\\nThen restart the machine and follow the instructions for setting up the Linux environment.\\n\\n[instructions]: https://learn.microsoft.com/en-us/windows/wsl/install\\n\\n### Linux:\\n#### Check whether the conda is installed:\\n```shell\\nconda -V\\n```\\n> GERONIMO was tested on conda 23.3.1\\n#### 1) If you do not have installed `conda`, please install `miniconda`\\nPlease follow the instructions for installing [miniconda]\\n\\n[miniconda]: https://conda.io/projects/conda/en/stable/user-guide/install/linux.html\\n\\n#### 2) Continue with installing `mamba` (recommended but optional)\\n```shell\\nconda install -n base -c conda-forge mamba\\n```\\n#### 3) Install `snakemake`\\n```shell\\nconda activate base\\nmamba create -p env_snakemake -c conda-forge -c bioconda snakemake\\nmamba activate env_snakemake\\nsnakemake --help\\n```\\nIn case of complications, please check the section `Questions & Answers` below or follow the [official documentation] for troubleshooting.\\n\\n[official documentation]: https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\\n\\n### Clone the GERONIMO repository\\nGo to the path in which you want to run the analysis and clone the repository:\\n```shell\\ncd <PATH>\\ngit clone https://github.com/amkilar/GERONIMO.git\\n```\\n\\n### Run sample analysis to ensure GERONIMO installation was successful\\nAll files are prepared for the sample analysis as a default. Please execute the line below:\\n```shell\\nsnakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx\\n```\\n\\nThis will prompt GERONIMO to quickly scan all modules, verifying the correct setup of the pipeline without executing any analysis.\\nYou should see the message `Building DAG of jobs...`, followed by `Nothing to be done (all requested files are present and up to date).`, when successfully completed.\\n\\nIf you want to run the sample analysis fully, please remove the folder `results` from the GERONIMO directory and execute GERONIMO again with:\\n\\n`snakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx`\\n\\n> You might consider allowing more cores to speed up the analysis, which might take up to several hours.\\n\\n#### You might want to clean `GERONIMO/` directory from the files produced by the example analysis. You can safely remove the following:\\n- `GERONIMO/results`\\n- `GERONIMO/database`\\n- `GERONIMO/taxonomy`\\n- `GERONIMO/temp`\\n- `.create_genome_list.touch`\\n- `list_of_genomes.txt`\\n\\n## Setup the inputs\\n\\n### 1) Prepare the `covariance models`:\\n\\n#### Browse the collection of available `covariance models` at [Rfam] (*You can find the covariance model in the tab `Curation`.*)  \\nPaste the covariance model to the folder `GERONIMO/models` and ensure its name follows the convention: `cov_model_<NAME>`\\n\\n[Rfam]: https://rfam.org/\\n\\n#### **OR**\\n\\n#### Prepare your own `covariance model` using [LocARNA]\\n1. Paste or upload your sequences to the web server and download the `.stk` file with the alignment result.  \\n  \\n    > *Please note that the `.stk` file format is crucial for the analysis, containing sequence alignment and secondary structure consensus.*\\n    \\n    > The LocARNA web service allows you to align 30 sequences at once - if you need to align more sequences, please use the standalone version available [here]  \\n    > After installation run: \\n    ```shell\\n    mlocarna my_fasta_sequences.fasta\\n    ```\\n  \\n2. Paste the `.stk` alignment file to the folder `GERONIMO/model_to_build` and ensure its name follows the convention: `<NAME>.stk`\\n\\n   > Please check the example `heterotrichea.stk` format in `GERONIMO/models_to_built` for reference\\n   \\n\\n[LocARNA]: http://rna.informatik.uni-freiburg.de/LocARNA/Input.jsp\\n[here]: http://www.bioinf.uni-freiburg.de/Software/LocARNA/\\n\\n\\n### 2) Adjust the `config.yaml` file\\nPlease adjust the analysis specifications, as in the following example:\\n\\n> - database: \\'<DATABASE_QUERY> [Organism]\\' (in case of difficulties with defining the database query, please follow the instructions below)\\n> - extract_genomic_region-length:  <number> (here you can determine how long the upstream genomic region should be extracted; tested for 200)\\n> - models: [\"<NAME>\", \"<NAME>\"] (here specify the names of models that should be used to perform analysis)\\n>   \\n>   *Here you can also insert the name of the covariance model you want to build with GERONIMO - just be sure you placed `<NAME>.stk` file in `GERONIMO/models_to_build` before starting analysis*\\n> - CPU_for_model_building: <number> (specify the number of available CPUs devoted to the process of building model (cannot exceed the CPU number allowed to snakemake with `--cores`)\\n>\\n>   *You might ignore this parameter when you do not need to create a new covariance model*\\n\\n\\nKeep in mind that the covariance models and alignments must be present in the respective GERONIMO folders.\\n \\n### 3) Remove folder `results`, which contains example analysis output\\n### 4) **Please ensure you have enough storage capacity to download all the requested genomes (in the `GERONIMO/` directory)**\\n\\n## Run GERONIMO\\n```shell\\nmamba activate env_snakemake\\ncd ~/GERONIMO\\nsnakemake -s GERONIMO.sm --cores <declare number of CPUs> --use-conda results/summary_table.xlsx\\n```\\n  \\n## Example results\\n\\n### Outputs characterisation\\n\\n#### A) Summary table\\nThe Excel table contains the results arranged by taxonomy information and hit significance. The specific columns include:\\n* family, organism_name, class, order, phylum (taxonomy context)\\n* GCA_id - corresponds to the genome assembly in the *NCBI database*\\n* model - describes which covariance model identified the result\\n* label - follows the *Infernal* convention of categorizing hits\\n* number - the counter of the result\\n* e_value - indicates the significance level of the hit\\n* HIT_sequence - the exact HIT sequence found by *Infernal*, which corresponds to the covariance model\\n* HIT_ID - describes in which part of the genome assembly the hit was found, which may help publish novel sequences\\n* extended_genomic_region - upstream sequence, which may contain a possible promoter sequence\\n* secondary_structure - the secondary structure consensus of the covariance model\\n\\n\\n#### B) Significant Hits Distribution Across Taxonomy Families\\nThe plot provides an overview of the number of genomes in which at least one significant hit was identified, grouped by family. The bold black line corresponds to the number of genomes present in each family, helping to minimize bias regarding unequal data representation across the taxonomy.\\n\\n\\n#### C) Hits Distribution in Genomes Across Families\\nThe heatmap provides information about the most significant hits from the genome, identified by a specific covariance model. Genomes are grouped by families (on the right). Hits are classified into three categories based on their e-values. Generally, these categories correspond to hit classifications (\"HIT,\" \"MAYBE,\" \"NO HIT\"). The \"HIT\" category is further divided to distinguish between highly significant hits and moderately significant ones.\\n\\n\\n\\n### GERONIMO directory structure\\n\\nThe GERONIMO directory structure is designed to produce files in a highly structured manner, ensuring clear insight and facilitating the analysis of results. During a successful run, GERONIMO produces the following folders:\\n* `/database` - which contains genome assemblies that were downloaded from the *NCBI database* and grouped in subfolders\\n* `/taxonomy` - where taxonomy information is gathered and stored in the form of tables\\n* `/results` - the main folder containing all produced results:\\n  * `/infernal_raw` - contains the raw results produced by *Infernal*\\n  * `/infernal` - contains restructured results of *Infernal* in table format\\n  * `/cmdBLAST` - contains results of *cmdblast*, which extracts the extended genomic region\\n  * `/summary` - contains summary files that join results from *Infernal*, *cmdblast*, and attach taxonomy context\\n  * `/plots` - contains two types of summary plots\\n* `/temp` - folder contains the information necessary to download genome assemblies from *NCBI database*\\n\\n* `/env` - stores instructions for dependency installation\\n* `/models` - where calibrated covariance models can be pasted, *for example, from the Rfam database*\\n* `/modes_to_built` - where multiple alignments in *.stk* format can be pasted\\n* `/scripts` - contains developed scripts that perform results structurization\\n\\n#### The example GERONIMO directory structure:\\n\\n```shell\\nGERONIMO\\n├── database\\n│\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 └── ...\\n├── env\\n├── models\\n├── model_to_build\\n├── results\\n│\\xa0\\xa0 ├── cmdBLAST\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── MRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── ...\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── SRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── extended\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── filtered\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 └── ...\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── ...\\n│\\xa0\\xa0 ├── infernal\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── MRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── ...\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── SRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── ...\\n│\\xa0\\xa0 ├── plots\\n│\\xa0\\xa0 ├── raw_infernal\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── MRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── ...\\n│\\xa0\\xa0 │\\xa0\\xa0 ├── SRP\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0 │\\xa0\\xa0 │\\xa0\\xa0 ├── ...\\n│\\xa0\\xa0 └── summary\\n│\\xa0\\xa0     ├── GCA_000091205.1_ASM9120v1_genomic\\n│\\xa0\\xa0     ├── GCA_000341285.1_ASM34128v1_genomic\\n│\\xa0\\xa0     ├── GCA_000350225.2_ASM35022v2_genomic\\n│\\xa0\\xa0     ├── ...\\n├── scripts\\n├── taxonomy\\n└── temp\\n```\\n\\n## GERONIMO applicability\\n\\n### Expanding the evolutionary context\\nTo add new genomes or database queries to an existing analysis, please follow the instructions:\\n1) Rename the `list_of_genomes.txt` file to `previous_list_of_genomes.txt` or any other preferred name.\\n2) Modify the `config.yaml` file by replacing the previous database query with the new one.\\n3) Delete:\\n   - `summary_table.xlsx`, `part_summary_table.csv`, `summary_table_models.xlsx` files located in the `GERONIMO\\\\results` directory\\n   - `.create_genome_list.touch` file\\n5) Run GERONIMO to calculate new results using the command:\\n     ```shell\\n     snakemake -s GERONIMO.sm --cores <declare number of CPUs> --use-conda results/summary_table.xlsx\\n     ```\\n7) Once the new results are generated, reviewing them before merging them with the original results is recommended.\\n8) Copy the contents of the `previous_list_of_genomes.txt` file and paste them into the current `list_of_genomes.txt`.\\n9) Delete:\\n   - `summary_table.xlsx` located in the `GERONIMO\\\\results` directory\\n   - `.create_genome_list.touch` file\\n10) Run GERONIMO to merge the results from both analyses using the command:\\n    ```shell\\n      snakemake -s GERONIMO.sm --cores 1 --use-conda results/summary_table.xlsx\\n    ```\\n\\n### Incorporating new covariance models into existing analysis\\n1) Copy the new covariance model to `GERONIMO/models`\\n2) Modify the `config.yaml` file by adding the name of the new model to the line `models: [...]`\\n3) Run GERONIMO to see the updated analysis outcome\\n\\n### Building a new covariance model\\nWith GERONIMO, building a new covariance model from multiple sequence alignment in the `.stk` format is possible. \\n\\nTo do so, simply paste `<NAME>.stk` file to `GERONIMO/models_to_build` and paste the name of the new covariance  model to `config.yaml` file to the line `models: [\"<NAME>\"]`\\n\\nand run GERONIMO.\\n\\n\\n## Questions & Answers\\n\\n### How to specify the database query?\\n- Visit the [NCBI Assemblies] website.  \\n- Follow the instruction on the graphic below:\\n\\n[NCBI Assemblies]: https://www.ncbi.nlm.nih.gov/assembly/?term=\\n\\n### WSL: problem with creating `snakemake_env`\\nIn the case of an error similar to the one below:\\n> CondaError: Unable to create prefix directory \\'/mnt/c/Windows/system32/env_snakemake\\'.\\n> Check that you have sufficient permissions.  \\n  \\nYou might try to delete the cache with: `rm -r ~/.cache/` and try again.\\n\\n### When `snakemake` does not seem to be installed properly\\nIn the case of the following error:\\n> Command \\'snakemake\\' not found ...\\n\\nCheck whether the `env_snakemake` is activated.\\n> It should result in a change from (base) to (env_snakemake) before your login name in the command line window.\\n\\nIf you still see `(base)` before your login name, please try to activate the environment with conda:\\n`conda activate env_snakemake`\\n\\n\\nPlease note that you might need to specify the full path to the `env_snakemake`, like /home/your user name/env_snakemake\\n\\n### How to browse GERONIMO results obtained in WSL?\\nYou can easily access the results obtained on WSL from your Windows environment by opening `File Explorer` and pasting the following line into the search bar: `\\\\\\\\wsl.localhost\\\\Ubuntu\\\\home\\\\`. This will reveal a folder with your username, as specified during the configuration of your Ubuntu system. To locate the GERONIMO results, simply navigate to the folder with your username and then to the `home` folder. (`\\\\\\\\wsl.localhost\\\\Ubuntu\\\\home\\\\<user>\\\\home\\\\GERONIMO`)\\n\\n### GERONIMO occupies a lot of storage space\\nThrough genome downloads, GERONIMO can potentially consume storage space, rapidly leading to a shortage. Currently, downloading genomes is an essential step for optimal GERONIMO performance.\\n\\nRegrettably, if the analysis is rerun without the `/database` folder, it will result in the need to redownload genomes, which is a highly time-consuming process.\\n\\nNevertheless, if you do not intend to repeat the analysis and have no requirement for additional genomes or models, you are welcome to retain your results tables and plots while removing the remaining files.\\n\\nIt is strongly advised against using local machines for extensive analyses. If you lack access to external storage space, it is recommended to divide the analysis into smaller segments, which can be later merged, as explained in the section titled `Expanding the evolutionary context`.\\n\\nConsidering this limitation, I am currently working on implementing a solution that will help circumvent the need for redundant genome downloads without compromising GERONIMO performance in the future.\\n\\nYou might consider deleting the `.snakemake` folder to free up storage space. However, please note that deleting this folder will require the reinstallation of GERONIMO dependencies when the analysis is rerun.\\n\\n## License\\nCopyright (c) 2023 Agata M. Kilar\\n\\nPermission is hereby granted, free of charge, to any person obtaining a copy\\nof this software and associated documentation files (the \"Software\"), to deal\\nin the Software without restriction, including without limitation the rights\\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\\ncopies of the Software, and to permit persons to whom the Software is\\nfurnished to do so, subject to the following conditions:\\n\\nThe above copyright notice and this permission notice shall be included in all\\ncopies or substantial portions of the Software.\\n\\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\\nSOFTWARE.\\n\\n## Contact\\nmgr inż. Agata Magdalena Kilar, PhD (agata.kilar@ceitec.muni.cz)\\n\\n'\n",
      " '# Protein Conformational Transitions calculations tutorial using BioExcel Building Blocks (biobb) and GOdMD\\n\\nThis tutorial aims to illustrate the process of computing a conformational transition between two known structural conformations of a protein, step by step, using the BioExcel Building Blocks library (biobb).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Macromolecular Coarse-Grained Flexibility (FlexServ) tutorial using BioExcel Building Blocks (biobb)\\n\\nThis tutorial aims to illustrate the process of generating protein conformational ensembles from 3D structures and analysing its molecular flexibility, step by step, using the BioExcel Building Blocks library (biobb).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '[![Snakemake](https://img.shields.io/badge/snakemake-≥7.0.0-brightgreen.svg?style=flat)](https://snakemake.readthedocs.io)\\n\\n\\n# About SnakeMAGs\\nSnakeMAGs is a workflow to reconstruct prokaryotic genomes from metagenomes. The main purpose of SnakeMAGs is to process Illumina data from raw reads to metagenome-assembled genomes (MAGs).\\nSnakeMAGs is efficient, easy to handle and flexible to different projects. The workflow is CeCILL licensed, implemented in Snakemake (run on multiple cores) and available for Linux.\\nSnakeMAGs performed eight main steps:\\n- Quality filtering of the reads\\n- Adapter trimming\\n- Filtering of the host sequences (optional)\\n- Assembly\\n- Binning\\n- Evaluation of the quality of the bins\\n- Classification of the MAGs\\n- Estimation of the relative abundance of the MAGs\\n\\n\\n![scheme of workflow](SnakeMAGs_schema.jpg?raw=true)\\n\\n# How to use SnakeMAGs\\n## Install conda\\nThe easiest way to install and run SnakeMAGs is to use [conda](https://www.anaconda.com/products/distribution). These package managers will help you to easily install [Snakemake](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\\n\\n## Install and activate Snakemake environment\\nNote: The workflow was developed with Snakemake 7.0.0\\n```\\nconda activate\\n\\n# First, set up your channel priorities\\nconda config --add channels defaults\\nconda config --add channels bioconda\\nconda config --add channels conda-forge\\n\\n# Then, create a new environment for the Snakemake version you require\\nconda create -n snakemake_7.0.0 snakemake=7.0.0\\n\\n# And activate it\\nconda activate snakemake_7.0.0\\n```\\n\\nAlternatively, you can also install Snakemake via mamba:\\n```\\n# If you do not have mamba yet on your machine, you can install it with:\\nconda install -n base -c conda-forge mamba\\n\\n# Then you can install Snakemake\\nconda activate base\\nmamba create -c conda-forge -c bioconda -n snakemake snakemake\\n\\n# And activate it\\nconda activate snakemake\\n\\n```\\n\\n## SnakeMAGs executable\\nThe easiest way to procure SnakeMAGs and its related files is to clone the repository using git:\\n```\\ngit clone https://github.com/Nachida08/SnakeMAGs.git\\n```\\nAlternatively, you can download the relevant files:\\n```\\nwget https://github.com/Nachida08/SnakeMAGs/blob/main/SnakeMAGs.smk https://github.com/Nachida08/SnakeMAGs/blob/main/config.yaml\\n```\\n\\n## SnakeMAGs input files\\n- Illumina paired-end reads in FASTQ.\\n- Adapter sequence file ([adapter.fa](https://github.com/Nachida08/SnakeMAGs/blob/main/adapters.fa)).\\n- Host genome sequences in FASTA (if host_genome: \"yes\"), in case you work with host-associated metagenomes (e.g. human gut metagenome).\\n\\n## Download Genome Taxonomy Database (GTDB)\\nGTDB-Tk requires ~66G+ of external data (GTDB) that need to be downloaded and unarchived. Because this database is voluminous, we let you decide where you want to store it.\\nSnakeMAGs do not download automatically GTDB, you have to do it:\\n\\n```\\n#Download the latest release (tested with release207)\\n#Note: SnakeMAGs uses GTDBtk v2.1.0 and therefore require release 207 as minimum version. See https://ecogenomics.github.io/GTDBTk/installing/index.html#installing for details.\\nwget https://data.gtdb.ecogenomic.org/releases/latest/auxillary_files/gtdbtk_v2_data.tar.gz\\n#Decompress\\ntar -xzvf *tar.gz\\n#This will create a folder called release207_v2\\n```\\nAll you have to do now is to indicate the path to the database folder (in our example, the folder is called release207_v2) in the config file, Classification section.\\n\\n## Download the GUNC database (required if gunc: \"yes\")\\nGUNC accepts either a progenomes or GTDB based reference database. Both can be downloaded using the ```gunc download_db``` command. For our study we used the default proGenome-derived GUNC database. It requires less resources with similar performance.\\n\\n```\\nconda activate\\n# Install and activate GUNC environment\\nconda create --prefix /path/to/gunc_env\\nconda install -c bioconda metabat2 --prefix /path/to/gunc_env\\nsource activate /path/to/gunc_env\\n\\n#Download the proGenome-derived GUNC database (tested with gunc_db_progenomes2.1)\\n#Note: SnakeMAGs uses GUNC v1.0.5\\ngunc download_db -db progenomes /path/to/GUNC_DB\\n```\\nAll you have to do now is to indicate the path to the GUNC database file in the config file,  Bins quality section.\\n\\n## Edit config file\\nYou need to edit the config.yaml file. In particular, you need to set the correct paths: for the working directory, to specify where are your fastq files, where you want to place the conda environments (that will be created using the provided .yaml files available in [SnakeMAGs_conda_env directory](https://github.com/Nachida08/SnakeMAGs/tree/main/SnakeMAGs_conda_env)), where are the adapters, where is GTDB and optionally where is the GUNC database and where is your host genome reference.\\n\\nLastly, you need to allocate the proper computational resources (threads, memory) for each of the main steps. These can be optimized according to your hardware.\\n\\n\\n\\nHere is an example of a config file:\\n\\n```\\n#####################################################################################################\\n#####  _____    ___    _              _   _    ______   __    __              _______   _____   #####\\n##### /  ___|  |   \\\\  | |     /\\\\     | | / /  |  ____| |  \\\\  /  |     /\\\\     /  _____| /  ___|  #####\\n##### | (___   | |\\\\ \\\\ | |    /  \\\\    | |/ /   | |____  |   \\\\/   |    /  \\\\    | |   __  | (___   #####\\n#####  \\\\___ \\\\  | | \\\\ \\\\| |   / /\\\\ \\\\   | |\\\\ \\\\   |  ____| | |\\\\  /| |   / /\\\\ \\\\   | |  |_ |  \\\\___ \\\\  #####\\n#####  ____) | | |  \\\\   |  / /__\\\\ \\\\  | | \\\\ \\\\  | |____  | | \\\\/ | |  / /__\\\\ \\\\  | |____||  ____) | #####\\n##### |_____/  |_|   \\\\__| /_/    \\\\_\\\\ |_|  \\\\_\\\\ |______| |_|    |_| /_/    \\\\_\\\\  \\\\______/ |_____/  #####\\n#####                                                                                           #####\\n#####################################################################################################\\n\\n############################\\n### Execution parameters ###\\n############################\\n\\nworking_dir: /path/to/working/directory/                                 #The main directory for the project\\nraw_fastq: /path/to/raw_fastq/                                           #The directory that contains all the fastq files of all the samples (eg. sample1_R1.fastq & sample1_R2.fastq, sample2_R1.fastq & sample2_R2.fastq...)\\nsuffix_1: \"_R1.fastq\"                                                    #Main type of suffix for forward reads file (eg. _1.fastq or _R1.fastq or _r1.fastq or _1.fq or _R1.fq or _r1.fq )\\nsuffix_2: \"_R2.fastq\"                                                    #Main type of suffix for reverse reads file (eg. _2.fastq or _R2.fastq or _r2.fastq or _2.fq or _R2.fq or _r2.fq )\\n\\n###########################\\n### Conda environnemnts ###\\n###########################\\n\\nconda_env: \"/path/to/SnakeMAGs_conda_env/\"                               #Path to the provided SnakeMAGs_conda_env directory which contains the yaml file for each conda environment\\n\\n#########################\\n### Quality filtering ###\\n#########################\\nemail: name.surname@your-univ.com                                        #Your e-mail address\\nthreads_filter: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\\nresources_filter: 150                                                    #Memory according to tools need (in GB)\\n\\n########################\\n### Adapter trimming ###\\n########################\\nadapters: /path/to/working/directory/adapters.fa                         #A fasta file contanning a set of various Illumina adaptors (this file is provided and is also available on github)\\ntrim_params: \"2:40:15\"                                                   #For further details, see the Trimmomatic documentation\\nthreads_trim: 10                                                         #The number of threads to run this process. To be adjusted according to your hardware\\nresources_trim: 150                                                      #Memory according to tools need (in GB)\\n\\n######################\\n### Host filtering ###\\n######################\\nhost_genome: \"yes\"                                                      #yes or no. An optional step for host-associated samples (eg. termite, human, plant...)\\nthreads_bowtie2: 50                                                     #The number of threads to run this process. To be adjusted according to your hardware\\nhost_genomes_directory: /path/to/working/host_genomes/                  #the directory where the host genome is stored\\nhost_genomes: /path/to/working/host_genomes/host_genomes.fa             #A fasta file containing the DNA sequences of the host genome(s)\\nthreads_samtools: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\\nresources_host_filtering: 150                                           #Memory according to tools need (in GB)\\n\\n################\\n### Assembly ###\\n################\\nthreads_megahit: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\\nmin_contig_len: 1000                                                   #Minimum length (in bp) of the assembled contigs\\nk_list: \"21,31,41,51,61,71,81,91,99,109,119\"                           #Kmer size (for further details, see the megahit documentation)\\nresources_megahit: 250                                                 #Memory according to tools need (in GB)\\n\\n###############\\n### Binning ###\\n###############\\nthreads_bwa: 50                                                        #The number of threads to run this process. To be adjusted according to your hardware\\nresources_bwa: 150                                                     #Memory according to tools need (in GB)\\nthreads_samtools: 50                                                   #The number of threads to run this process. To be adjusted according to your hardware\\nresources_samtools: 150                                                #Memory according to tools need (in GB)\\nseed: 19860615                                                         #Seed number for reproducible results\\nthreads_metabat: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\\nminContig: 2500                                                        #Minimum length (in bp) of the contigs\\nresources_binning: 250                                                 #Memory according to tools need (in GB)\\n\\n####################\\n### Bins quality ###\\n####################\\n#checkM\\nthreads_checkm: 50                                                    #The number of threads to run this process. To be adjusted according to your hardware\\nresources_checkm: 250                                                 #Memory according to tools need (in GB)\\n#bins_quality_filtering\\ncompletion: 50                                                        #The minimum completion rate of bins\\ncontamination: 10                                                     #The maximum contamination rate of bins\\nparks_quality_score: \"yes\"                                            #yes or no. If yes bins are filtered according to the Parks quality score (completion-5*contamination >= 50)\\n#GUNC\\ngunc: \"yes\"                                                           #yes or no. An optional step to detect and discard chimeric and contaminated genomes using the GUNC tool\\nthreads_gunc: 50                                                      #The number of threads to run this process. To be adjusted according to your hardware\\nresources_gunc: 250                                                   #Memory according to tools need (in GB)\\nGUNC_db: /path/to/GUNC_DB/gunc_db_progenomes2.1.dmnd                  #Path to the downloaded GUNC database (see the readme file)\\n\\n######################\\n### Classification ###\\n######################\\nGTDB_data_ref: /path/to/downloaded/GTDB                                #Path to uncompressed GTDB-Tk reference data (GTDB)\\nthreads_gtdb: 10                                                       #The number of threads to run this process. To be adjusted according to your hardware\\nresources_gtdb: 250                                                    #Memory according to tools need (in GB)\\n\\n##################\\n### Abundances ###\\n##################\\nthreads_coverM: 10                                                     #The number of threads to run this process. To be adjusted according to your hardware\\nresources_coverM: 150                                                  #Memory according to tools need (in GB)\\n```\\n# Run SnakeMAGs\\nIf you are using a workstation with Ubuntu (tested on Ubuntu 22.04):\\n```{bash}\\nsnakemake --cores 30 --snakefile SnakeMAGs.smk --use-conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --configfile /path/to/config.yaml --keep-going --latency-wait 180\\n```\\n\\nIf you are working on a cluster with Slurm (tested with version 18.08.7):\\n```{bash}\\nsnakemake --snakefile SnakeMAGs.smk --cluster \\'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" \\' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\\n```\\n\\nIf you are working on a cluster with SGE (tested with version 8.1.9):\\n```{bash}\\nsnakemake --snakefile SnakeMAGs.smk --cluster \"qsub -cwd -V -q <short.q/long.q> -pe thread {threads} -e cluster_logs/{rule}.e{jobid} -o cluster_logs/{rule}.o{jobid}\" --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\\n```\\n\\n\\n# Test\\nWe provide you a small data set in the [test](https://github.com/Nachida08/SnakeMAGs/tree/main/test) directory which will allow you to validate your instalation and take your first steps with SnakeMAGs. This data set is a subset from [ZymoBiomics Mock Community](https://www.zymoresearch.com/blogs/blog/zymobiomics-microbial-standards-optimize-your-microbiomics-workflow) (250K reads) used in this tutoriel [metagenomics_tutorial](https://github.com/pjtorres/metagenomics_tutorial).\\n\\n1. Before getting started make sure you have cloned the SnakeMAGs repository or you have downloaded all the necessary files (SnakeMAGs.smk, config.yaml, chr19.fa.gz, insub732_2_R1.fastq.gz, insub732_2_R2.fastq.gz). See the [SnakeMAGs executable](#snakemags-executable) section.\\n2. Unzip the fastq files and the host sequences file.\\n```\\ngunzip fastqs/insub732_2_R1.fastq.gz fastqs/insub732_2_R2.fastq.gz host_genomes/chr19.fa.gz\\n```\\n3. For better organisation put all the read files in the same directory (eg. fastqs) and the host sequences file in a separate directory (eg. host_genomes)\\n4. Edit the config file (see [Edit config file](#edit-config-file) section)\\n5. Run the test (see [Run SnakeMAGs](#run-snakemags) section)\\n\\nNote: the analysis of these files took 1159.32 secondes to complete on a Ubuntu 22.04 LTS with an Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz x 40 processor, 96GB of RAM.\\n\\n# Genome reference for host reads filtering\\nFor host-associated samples, one can remove host sequences from the metagenomic reads by mapping these reads against a reference genome. In the case of termite gut metagenomes, we are providing [here](https://zenodo.org/record/6908287#.YuAdFXZBx8M) the relevant files (fasta and index files) from termite genomes.\\n\\nUpon request, we can help you to generate these files for your own reference genome and make them available to the community.\\n\\nNB. These steps of mapping generate voluminous files such as .bam and .sam. Depending on your disk space, you might want to delete these files after use.\\n\\n\\n# Use case\\nDuring the test phase of the development of SnakeMAGs, we used this workflow to process 10 publicly available termite gut metagenomes generated by Illumina sequencing, to ultimately reconstruct prokaryotic MAGs. These metagenomes were retrieved from the NCBI database using the following accession numbers: SRR10402454; SRR14739927; SRR8296321; SRR8296327; SRR8296329; SRR8296337; SRR8296343; DRR097505; SRR7466794; SRR7466795. They come from five different studies: Waidele et al, 2019; Tokuda et al, 2018; Romero Victorica et al, 2020; Moreira et al, 2021; and Calusinska et al, 2020.\\n\\n## Download the Illumina pair-end reads\\nWe use fasterq-dump tool to extract data in FASTQ-format from SRA-accessions. It is a commandline-tool which offers a faster solution for downloading those large files.\\n\\n```\\n# Install and activate sra-tools environment\\n## Note: For this study we used sra-tools 2.11.0\\n\\nconda activate\\nconda install -c bioconda sra-tools\\nconda activate sra-tools\\n\\n# Download fastqs in a single directory\\nmkdir raw_fastq\\ncd raw_fastq\\nfasterq-dump <SRA-accession> --threads <threads_nbr> --skip-technical --split-3\\n```\\n\\n## Download Genome reference for host reads filtering\\n```\\nmkdir host_genomes\\ncd host_genomes\\nwget https://zenodo.org/record/6908287/files/termite_genomes.fasta.gz\\ngunzip termite_genomes.fasta.gz\\n```\\n\\n## Edit the config file\\nSee [Edit config file](#edit-config-file) section.\\n\\n## Run SnakeMAGs\\n```\\nconda activate snakemake_7.0.0\\nmkdir cluster_logs\\nsnakemake --snakefile SnakeMAGs.smk --cluster \\'sbatch -p <cluster_partition> --mem <memory> -c <cores> -o \"cluster_logs/{wildcards}.{rule}.{jobid}.out\" -e \"cluster_logs/{wildcards}.{rule}.{jobid}.err\" \\' --jobs <nbr_of_parallel_jobs> --use-conda --conda-frontend conda --conda-prefix /path/to/SnakeMAGs_conda_env/ --jobname \"{rule}.{wildcards}.{jobid}\" --latency-wait 180 --configfile /path/to/config.yaml --keep-going\\n```\\n\\n## Study results\\nThe MAGs reconstructed from each metagenome and their taxonomic classification are available in this [repository](https://doi.org/10.5281/zenodo.7661004).\\n\\n# Citations\\n\\nIf you use SnakeMAGs, please cite:\\n> Tadrent N, Dedeine F and Hervé V. SnakeMAGs: a simple, efficient, flexible and scalable workflow to reconstruct prokaryotic genomes from metagenomes [version 2; peer review: 2 approved]. F1000Research 2023, 11:1522 (https://doi.org/10.12688/f1000research.128091.2)\\n\\n\\nPlease also cite the dependencies:\\n- [Snakemake](https://doi.org/10.12688/f1000research.29032.2) : Mölder, F., Jablonski, K. P., Letcher, B., Hall, M. B., Tomkins-tinch, C. H., Sochat, V., Forster, J., Lee, S., Twardziok, S. O., Kanitz, A., Wilm, A., Holtgrewe, M., Rahmann, S., Nahnsen, S., & Köster, J. (2021) Sustainable data analysis with Snakemake [version 2; peer review: 2 approved]. *F1000Research* 2021, 10:33.\\n- [illumina-utils](https://doi.org/10.1371/journal.pone.0066643) : Murat Eren, A., Vineis, J. H., Morrison, H. G., & Sogin, M. L. (2013). A Filtering Method to Generate High Quality Short Reads Using Illumina Paired-End Technology. *PloS ONE*, 8(6), e66643.\\n- [Trimmomatic](https://doi.org/10.1093/bioinformatics/btu170) : Bolger, A. M., Lohse, M., & Usadel, B. (2014). Genome analysis Trimmomatic: a flexible trimmer for Illumina sequence data. *Bioinformatics*, 30(15), 2114-2120.\\n- [Bowtie2](https://doi.org/10.1038/nmeth.1923) : Langmead, B., & Salzberg, S. L. (2012). Fast gapped-read alignment with Bowtie 2. *Nature Methods*, 9(4), 357–359.\\n- [SAMtools](https://doi.org/10.1093/bioinformatics/btp352) : Li, H., Handsaker, B., Wysoker, A., Fennell, T., Ruan, J., Homer, N., Marth, G., Abecasis, G., & Durbin, R. (2009). The Sequence Alignment/Map format and SAMtools. *Bioinformatics*, 25(16), 2078–2079.\\n- [BEDtools](https://doi.org/10.1093/bioinformatics/btq033) : Quinlan, A. R., & Hall, I. M. (2010). BEDTools: A flexible suite of utilities for comparing genomic features. *Bioinformatics*, 26(6), 841–842.\\n- [MEGAHIT](https://doi.org/10.1093/bioinformatics/btv033) : Li, D., Liu, C. M., Luo, R., Sadakane, K., & Lam, T. W. (2015). MEGAHIT: An ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. *Bioinformatics*, 31(10), 1674–1676.\\n- [bwa](https://doi.org/10.1093/bioinformatics/btp324) : Li, H., & Durbin, R. (2009). Fast and accurate short read alignment with Burrows-Wheeler transform. *Bioinformatics*, 25(14), 1754–1760.\\n- [MetaBAT2](https://doi.org/10.7717/peerj.7359) : Kang, D. D., Li, F., Kirton, E., Thomas, A., Egan, R., An, H., & Wang, Z. (2019). MetaBAT 2: An adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. *PeerJ*, 2019(7), 1–13.\\n- [CheckM](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043–1055.\\n- [GTDB-Tk](https://doi.org/10.1093/BIOINFORMATICS/BTAC672) : Chaumeil, P.-A., Mussig, A. J., Hugenholtz, P., Parks, D. H. (2022). GTDB-Tk v2: memory friendly classification with the genome taxonomy database. *Bioinformatics*.\\n- [CoverM](https://github.com/wwood/CoverM)\\n- [Waidele et al, 2019](https://doi.org/10.1101/526038) : Waidele, L., Korb, J., Voolstra, C. R., Dedeine, F., & Staubach, F. (2019). Ecological specificity of the metagenome in a set of lower termite species supports contribution of the microbiome to adaptation of the host. *Animal Microbiome*, 1(1), 1–13.\\n- [Tokuda et al, 2018](https://doi.org/10.1073/pnas.1810550115) : Tokuda, G., Mikaelyan, A., Fukui, C., Matsuura, Y., Watanabe, H., Fujishima, M., & Brune, A. (2018). Fiber-associated spirochetes are major agents of hemicellulose degradation in the hindgut of wood-feeding higher termites. *Proceedings of the National Academy of Sciences of the United States of America*, 115(51), E11996–E12004.\\n- [Romero Victorica et al, 2020](https://doi.org/10.1038/s41598-020-60850-5) : Romero Victorica, M., Soria, M. A., Batista-García, R. A., Ceja-Navarro, J. A., Vikram, S., Ortiz, M., Ontañon, O., Ghio, S., Martínez-Ávila, L., Quintero García, O. J., Etcheverry, C., Campos, E., Cowan, D., Arneodo, J., & Talia, P. M. (2020). Neotropical termite microbiomes as sources of novel plant cell wall degrading enzymes. *Scientific Reports*, 10(1), 1–14.\\n- [Moreira et al, 2021](https://doi.org/10.3389/fevo.2021.632590) : Moreira, E. A., Persinoti, G. F., Menezes, L. R., Paixão, D. A. A., Alvarez, T. M., Cairo, J. P. L. F., Squina, F. M., Costa-Leonardo, A. M., Rodrigues, A., Sillam-Dussès, D., & Arab, A. (2021). Complementary contribution of Fungi and Bacteria to lignocellulose digestion in the food stored by a neotropical higher termite. *Frontiers in Ecology and Evolution*, 9(April), 1–12.\\n- [Calusinska et al, 2020](https://doi.org/10.1038/s42003-020-1004-3) : Calusinska, M., Marynowska, M., Bertucci, M., Untereiner, B., Klimek, D., Goux, X., Sillam-Dussès, D., Gawron, P., Halder, R., Wilmes, P., Ferrer, P., Gerin, P., Roisin, Y., & Delfosse, P. (2020). Integrative omics analysis of the termite gut system adaptation to Miscanthus diet identifies lignocellulose degradation enzymes. *Communications Biology*, 3(1), 1–12.\\n- [Orakov et al, 2021](https://doi.org/10.1186/s13059-021-02393-0) : Orakov, A., Fullam, A., Coelho, L. P., Khedkar, S., Szklarczyk, D., Mende, D. R., Schmidt, T. S. B., & Bork, P. (2021). GUNC: detection of chimerism and contamination in prokaryotic genomes. *Genome Biology*, 22(1).\\n- [Parks et al, 2015](https://doi.org/10.1101/gr.186072.114) : Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P., & Tyson, G. W. (2015). CheckM: Assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Research*, 25(7), 1043–1055.\\n# License\\nThis project is licensed under the CeCILL License - see the [LICENSE](https://github.com/Nachida08/SnakeMAGs/blob/main/LICENCE) file for details.\\n\\nDeveloped by Nachida Tadrent at the Insect Biology Research Institute ([IRBI](https://irbi.univ-tours.fr/)), under the supervision of Franck Dedeine and Vincent Hervé.\\n'\n",
      " '# The Polygenic Score Catalog Calculator (`pgsc_calc`)\\n\\n[![Documentation Status](https://readthedocs.org/projects/pgsc-calc/badge/?version=latest)](https://pgsc-calc.readthedocs.io/en/latest/?badge=latest)\\n[![pgscatalog/pgsc_calc CI](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml/badge.svg)](https://github.com/PGScatalog/pgsc_calc/actions/workflows/ci.yml)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5970794.svg)](https://doi.org/10.5281/zenodo.5970794)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-≥22.10.0-23aa62.svg?labelColor=000000)](https://www.nextflow.io/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n\\n## Introduction\\n\\n`pgsc_calc` is a bioinformatics best-practice analysis pipeline for calculating\\npolygenic [risk] scores on samples with imputed genotypes using existing scoring\\nfiles from the [Polygenic Score (PGS) Catalog](https://www.pgscatalog.org/)\\nand/or user-defined PGS/PRS.\\n\\n## Pipeline summary\\n\\n<p align=\"center\">\\n  <img width=\"80%\" src=\"https://github.com/PGScatalog/pgsc_calc/assets/11425618/f766b28c-0f75-4344-abf3-3463946e36cc\">\\n</p>\\n\\nThe workflow performs the following steps:\\n\\n* Downloading scoring files using the PGS Catalog API in a specified genome build (GRCh37 and GRCh38).\\n* Reading custom scoring files (and performing a liftover if genotyping data is in a different build).\\n* Automatically combines and creates scoring files for efficient parallel computation of multiple PGS\\n    - Matching variants in the scoring files against variants in the target dataset (in plink bfile/pfile or VCF format)\\n* Calculates PGS for all samples (linear sum of weights and dosages)\\n* Creates a summary report to visualize score distributions and pipeline metadata (variant matching QC)\\n\\nAnd optionally:\\n\\n- Genetic Ancestry: calculate similarity of target samples to populations in a\\n  reference dataset ([1000 Genomes (1000G)](http://www.nature.com/nature/journal/v526/n7571/full/nature15393.html)), using principal components analysis (PCA)\\n- PGS Normalization: Using reference population data and/or PCA projections to report\\n  individual-level PGS predictions (e.g. percentiles, z-scores) that account for genetic ancestry\\n\\nSee documentation for a list of planned [features under development](https://pgsc-calc.readthedocs.io/en/latest/index.html#Features-under-development).\\n\\n## Quick start\\n\\n1. Install\\n[`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation)\\n(`>=22.10.0`)\\n\\n2. Install [`Docker`](https://docs.docker.com/engine/installation/) or\\n[`Singularity (v3.8.3 minimum)`](https://www.sylabs.io/guides/3.0/user-guide/)\\n(please only use [`Conda`](https://conda.io/miniconda.html) as a last resort)\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n    ```console\\n    nextflow run pgscatalog/pgsc_calc -profile test,<docker/singularity/conda>\\n    ```\\n\\n4. Start running your own analysis!\\n\\n    ```console\\n    nextflow run pgscatalog/pgsc_calc -profile <docker/singularity/conda> --input samplesheet.csv --pgs_id PGS001229\\n    ```\\n\\nSee [getting\\nstarted](https://pgsc-calc.readthedocs.io/en/latest/getting-started.html) for more\\ndetails.\\n\\n## Documentation\\n\\n[Full documentation is available on Read the Docs](https://pgsc-calc.readthedocs.io/)\\n\\n## Credits\\n\\npgscatalog/pgsc_calc is developed as part of the PGS Catalog project, a\\ncollaboration between the University of Cambridge’s Department of Public Health\\nand Primary Care (Michael Inouye, Samuel Lambert) and the European\\nBioinformatics Institute (Helen Parkinson, Laura Harris).\\n\\nThe pipeline seeks to provide a standardized workflow for PGS calculation and\\nancestry inference implemented in nextflow derived from an existing set of\\ntools/scripts developed by Inouye lab (Rodrigo Canovas, Scott Ritchie, Jingqin\\nWu) and PGS Catalog teams (Samuel Lambert, Laurent Gil).\\n\\nThe adaptation of the codebase, nextflow implementation, and PGS Catalog features\\nare written by Benjamin Wingfield, Samuel Lambert, Laurent Gil with additional input\\nfrom Aoife McMahon (EBI). Development of new features, testing, and code review\\nis ongoing including Inouye lab members (Rodrigo Canovas, Scott Ritchie) and others. A\\nmanuscript describing the tool is *in preparation*. In the meantime if you use the\\ntool we ask you to cite the repo and the paper describing the PGS Catalog\\nresource:\\n\\n- >PGS Catalog Calculator _(in preparation)_. PGS Catalog\\n  Team. [https://github.com/PGScatalog/pgsc_calc](https://github.com/PGScatalog/pgsc_calc)\\n- >Lambert _et al._ (2021) The Polygenic Score Catalog as an open database for\\nreproducibility and systematic evaluation.  Nature Genetics. 53:420–425\\ndoi:[10.1038/s41588-021-00783-5](https://doi.org/10.1038/s41588-021-00783-5).\\n\\nThis pipeline is distrubuted under an [Apache License](LICENSE) amd uses code and \\ninfrastructure developed and maintained by the [nf-core](https://nf-co.re) community \\n(Ewels *et al. Nature Biotech* (2020) doi:[10.1038/s41587-020-0439-x](https://doi.org/10.1038/s41587-020-0439-x)), \\nreused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\nAdditional references of open-source tools and data used in this pipeline are described in\\n[`CITATIONS.md`](CITATIONS.md).\\n\\nThis work has received funding from EMBL-EBI core funds, the Baker Institute,\\nthe University of Cambridge, Health Data Research UK (HDRUK), and the European\\nUnion’s Horizon 2020 research and innovation programme under grant agreement No\\n101016775 INTERVENE.\\n'\n",
      " 'We present an R script that describes the workflow for analysing honey bee (_Apis mellifera_) wing shape. It is based on a dataset of wing images and landmark coordinates available at Zenodo: https://doi.org/10.5281/zenodo.8128010. \\nThe dataset can be used as a reference for the identification of local bees from southern Kazakhstan, which most probably belong to the subspecies _Apis mellifera pomonella_. It was compared with data from Nawrocka et al. (2018), available at Zenodo: https://doi.org/10.5281/zenodo.7567336. '\n",
      " '# HiFi *de novo* genome assembly workflow\\n\\nHiFi-assembly-workflow is a bioinformatics pipeline that can be used to analyse Pacbio CCS reads for *de novo* genome assembly using PacBio Circular Consensus Sequencing (CCS) reads. This workflow is implemented in Nextflow and has 3 major sections. \\n \\nPlease refer to the following documentation for detailed description of each workflow section:\\n \\n- [Adapter filtration and pre-assembly quality control (QC)](https://australianbiocommons.github.io/hifi-assembly-workflow/recommendations#stage-1-adapter-filtration-and-pre-assembly-quality-control)\\n- [Assembly](https://australianbiocommons.github.io/hifi-assembly-workflow/recommendations#stage-2-assembly)\\n- [Post-assembly QC](https://australianbiocommons.github.io/hifi-assembly-workflow/recommendations#stage-3-post-assembly-quality-control)\\n\\n\\n## General recommendations \\n\\nA more detailed module and workflow description as well as execution examples on Gadi and Setonix are [available here](https://australianbiocommons.github.io/hifi-assembly-workflow/workflows).\\n\\n\\n## Attributions\\n\\nThis work was developed at AGRF and supported by the Australian BioCommons via Bioplatforms Australia funding, the Australian Research Data Commons (https://doi.org/10.47486/PL105) and the Queensland Government RICF programme. Bioplatforms Australia and the Australian Research Data Commons are enabled by the National Collaborative Research Infrastructure Strategy (NCRIS).\\n\\nThe documentation in this repository is based on Australian BioCommons guidelines. \\n'\n",
      " 'This workflow takes as input SR BAM from ChIP-seq. It calls peaks on each replicate and intersect them. In parallel, each BAM is subsetted to smallest number of reads. Peaks are called using both subsets combined. Only peaks called using a combination of both subsets which have summits intersecting the intersection of both replicates will be kept.'\n",
      " 'Racon polish with long reads, x4'\n",
      " '# CLAWS (CNAG\\'s Long-read Assembly Workflow in Snakemake)\\n Snakemake Pipeline used for de novo genome assembly @CNAG. It has been developed for Snakemake v6.0.5.\\n\\nIt accepts Oxford Nanopore Technologies (ONT) reads, PacBio HFi reads, illumina paired-end data, illumina 10X data and Hi-C reads. It does the preprocessing of the reads, assembly, polishing, purge_dups, scaffodling and different evaluation steps. By default it will preprocess the reads, run Flye + Hypo + purge_dups + yahs and evaluate the resulting assemblies with BUSCO, MERQURY, Nseries and assembly_stats. It needs a config file and a spec file (json file with instructions on which resources should slurm use for each of the jobs). Both files are created by the script \"create_config_assembly.py\" that is located in the bin directory. To check all the options accepted by the script, do:\\n\\n```\\nbin/create_config_assembly.py -h\\n```\\n\\nOnce the 2 config files are produced, the pipeline can be launched using snakemake like this:\\n\\n``snakemake --notemp -j 999 --snakefile assembly_pipeline.smk --configfile assembly.config --is --cluster-conf assembly.spec --use-conda --use-envmodules``\\n\\nIf you are using an HPC cluster, please check how should you run snakemake to launch the jobs to the cluster. \\n\\nMost of the tools used will be installed via conda using the environments of the \"envs\" directory after providing the \"--use-conda\" option to snakemake. However, a few tools cannot be installed via conda and will have to be available in your PATH, or as a module in the cluster. Those tools are:\\n\\n- NextDenovo/2.5.0\\n- NextPolish/1.4.1\\n\\n# How to provide input data:\\n\\nThere are several ways of providing the reads.\\n\\n### 1- ONT reads\\n\\n1.1 Using the option ``--ont-dir {DIR}`` in create_config_assembly.py.\\n\\nIf you do so, it will look for all the files in the directory that end in \\'.fastq.gz\\' and will add the basenames to \"ONT_wildcards\". These wildcards will be processed by the pipeline that will:\\n\\n- Concatenate all the files into a single file\\n\\n- Run filtlong with the default or specified parameters. \\n\\n- Use the resulting file for assembly, polishing and/or purging.\\n\\nYou can also specify the basenames of the files that you want to use with the ``--ont-list `` option. In this case, the pipeline will use the wildcards that you\\'re providing instead of merging all the files in the directory.\\n\\n1.2 Using the option ```--ont-reads {FILE}``` in create_config_assembly.py.\\n\\nIf you do so, it will consider that you already have all the reads in one file and will:  \\n\\n- Run filtlong with the default or specified parameters.\\n\\n- Use the resulting file for assembly, polishing and/or purging.\\n\\n1.3 Using the option ```--ont-filt {FILE}```. It will use this file as the output from filtlong. Hence, it will skip the preprocessing steps and directly use it for assembly, polishing and/or purging. \\n\\n\\n\\n### 2-Illumina 10X-linked data\\n\\n2.1 Using the  ```--raw-10X {DIR:list}``` option. \\n\\nDictionary with 10X raw read directories, it has to be the mkfastq dir. You must specify as well the sampleIDs from this run. Example: \\'{\"mkfastq-                        dir\":\"sample1,sample2,sample3\"}\\'...\\n\\nIt will take each basename in the list to get the fastqs from the corresponding directory and run longranger on each sample. Afterwards, it will build meryldbs for each \"barcoded\" file. Finally, it will concatenate all the meryldbs and \"barcoded\" files. Resulting \"barcoded\" file will be used for polishing. \\n\\n2.2 Using the ``--processed-10X {DIR}`` parameter. \\n\\nThis directory can already be there or be produced by the pipeline as described in step 2.1. Once all the \"barcoded\" fastq files are there, meryldbs will be built for each \"barcoded\" file.  Finally, it will concatenate all the meryldbs and \"barcoded\" files. Resulting \"barcoded\" file will be used for polishing. \\n\\n2.3 Using the ``--10X`` option. \\n\\nThe argument to this is the path to the concatenated \".barcoded\" file that needs to be used for polishing. If the pre-concatenated files are not given, meryldbs will be directly generated with this file, but it may run out of memory. \\n\\n### 3- Illumina short-read data\\n\\n3.1 Using the ``--illumina-dir {DIR}`` option, that will look for all the files in the directory that end in \\'.1.fastq.gz\\' and will add the basenames to \"illumina_wildcards\". These wildcards will be processed by the pipeline that will: \\n\\n- Trim adaptors with Trimgalore\\n\\n- Concatenate all the trimmed *.1.fastq.gz and the *2.fastq.gz in one file per pair. \\n\\n- The resulting reads will be used for building meryldbs and polishing. \\n\\n3.2 Using the ``--processed-illumina`` option. If the directory exists and contains files, the pipeline will look for all the files in the directory that end in \\'.1.fastq.gz\\' and will add the basenames to \"illumina_wildcards\". These wildcards will be processed by the pipeline that will:\\n\\n- Concatenate all the trimmed *.1.fastq.gz and the *2.fastq.gz in one file per pair. \\n\\n- The resulting reads will be used for building meryldbs and polishing. \\n\\n3.3 Using the ``--pe1 {FILE} and --pe2 {FILE}`` options. That will consider that these are the paired files containing all the illumina reads ready to be used and will build meryldbs and polish with them.\\n\\n### 4- Input assemblies\\n\\nIf you want to polish an already assembled assembly, you can give it to the pipeline by using the option ``--assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]\\n                        Dictionary with assemblies that need to be polished but not assembled and directory where they should\\n                        be polished. Example: \\'{\"assembly1\":\"polishing_dir1\"}\\' \\'{\"assembly2\"=\"polishing_dir2\"}\\' ...``\\n\\t\\t\\t\\nIf you want to start the pipeline after polishing on an already existing assembly, you can give it to the pipeline by using the option ``--postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]\\n                        Dictionary with assemblies for which postpolishing steps need to be run but that are not assembled and\\n                        base step for the directory where the first postpolishing step should be run. Example:\\n                        \\'{\"assembly1\":\"s04.1_p03.1\"}\\' \\'{\"assembly2\"=\"s04.2_p03.2\"}\\' ...``\\n\\nTo evaluate and produce the final pretext file on a curated assembly, use ``--curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]\\n                        Dictionary with assemblies that have already been curated. Evaluations and read alignment will be perforder. Example:\\n                        \\'{\"assembly1\":\"s04.1_p03.1\"}\\' \\'{\"assembly2\":\"s04.2_p03.2\"}\\' ...``\\n\\n\\n\\n# Description of implemented rules\\n\\n1- Preprocessing:\\n\\t\\n- **Read concatenation:**\\n\\n``zcat {input.fastqs} | pigz -p {threads} -c  > {output.final_fastq}``\\n\\t\\n- **Longranger for 10X reads**: it uses the Longranger version installed in the path specified in the configfile\\n\\n``longranger basic --id={params.sample} --sample={params.sample} --fastqs={input.mkfastq_dir} --localcores={threads}``\\n\\n- **Trimgalore:** By default it gives the ``--max_n 0 --gzip -q 20 --paired --retain_unpaired`` options, but it can be changed with the ``--trim-galore-opts `` argument. \\n\\n``trim_galore -j {threads} {params.opts} {input.read1} {input.read2}``\\n\\n- **Filtlong:** it uses the Filtlong version installed in the path specified in the configfile. By default it gives the min_length and min_mean_q parameters, but extra parameters can be added with the ``--filtlong-opts`` option.\\n\\n``filtlong --min_length {params.minlen} --min_mean_q {params.min_mean_q} {params.opts} {input.reads} | pigz -p {threads} -c > {output.outreads}``\\n\\t\\n- **Build meryldb**: it uses the merqury conda environment specified in the configfile. It takes as argument the `--mery-k` value that needs to be estimated first for the genome size. It can run either on the illumina reads, the ont reads or both, default behaviour is both. \\n\\n``meryl k={params.kmer} count output {output.out_dir} {input.fastq}``\\n\\t\\n- Concat meryldbs: with the merqury conda environment specified in the configfile\\n\\n``meryl union-sum output {output.meryl_all} {input.input_run}``\\n\\t\\n- **Align ONT (Minimap2):** it aligns the reads using minimap2 and outputs the alignment either in bam or in paf.gz formats. It uses the minimap2 conda environment specified in the configfile\\n\\n``minimap2 -{params.align_opts} -t {threads} {input.genome} {input.reads} ``\\n\\n- **Align Illumina (BWA-MEM):** it aligns the reads with BWA-mem and outputs a bam file\\n\\n``bwa mem -Y {params.options} -t {threads} {input.genome} {input.reads} | samtools view -Sb - | samtools sort -@ {threads} -o {output.mapping} -``\\n\\n2- Assembly\\n\\n- **Flye (default)**. It is run by default, if you don\\'t want the pipeline to run it, you can give `--no-flye` option when creating the config. It uses the conda environment specified in the config. By default it is set to 2 polishing iterations and gives the genome-size estimate that has been given when creating the config. Extra options can be provided with the `--flye-opts`.\\n\\n``flye --{params.readtype} {input.reads} -o {params.outdir}out -t {threads} -i {params.pol_iterations} {params.other_flye_opts} ``\\n\\t\\n- **Nextdenovo (if ``run-nextdenovo``):** It uses the cluster module specified in the config. If nextdenovo option is turned on, the create_config script will also create the nextdenovo config file. Check the create_config help to see which options can be modified on it. \\n\\n``nextDenovo {input.config}``\\n\\n3- Polishing\\n\\n- **Hypo (default):** It is the polisher that the pipeline uses by default, it can be turned off specifying ``--no-hypo`` when creating the config. If selected, the reads will be aligned in previous rules and then hypo will be run, it requires illumina data. It uses the conda environment specified in the config. \\n\\n``hypo -r @short_reads.list.txt -d {input.genome} -b {input.sr_bam} -c {coverage} -s {params.genome_size} -B {input.lr_bam} -t {threads} -o {output.polished} -p {params.proc} {params.opts} ``\\n\\t\\n- **Nextpolish ont (if turned on):** to run nextpolish with ONT reads, specify ``--nextpolish-ont-rounds`` and the number of rounds you want to run of it. \\n\\n``\"python /apps/NEXTPOLISH/1.3.1/lib/nextpolish2.py -g {input.genome} -p {threads} -l lgs.fofn -r {params.lrtype} > {output.polished}``\\n\\t\\n- **Nextpolish illumina (if turned on):** to run nextpolish with ONT reads, specify ``--nextpolish-ill-rounds`` and the number of rounds you want to run of it. \\n\\n``\"python /apps/NEXTPOLISH/1.3.1/lib/nextpolish1.py -g {input.genome}  -p {threads} -s {input.bam} -t {params.task} > {output.polished}``\\n\\n4- Post-assembly\\n\\n- **Purge_dups (by default):** select ``--no-purgedups`` if you don\\'t want to run it. If no manual cutoffs are given, it\\'ll run purgedups with automatic cutoffs and then will rerun it selecting the mean cutoff as 0.75\\\\*cov. It uses the version installed in the cluster module specified in the config. \\n\\n5- Evaluations\\n\\t\\n- **Merqury:** It runs on each \\'terminal\\' assembly. This is, the base assembly and the resulting assembly from each branch of the pipeline. \\n\\t\\n- **Busco:** It can be run only in the terminal assemblies or on all the assemblies produced by the pipeline. It uses the conda environment specified in the config as well as the parameters specified. \\n\\t\\n- **Nseries:** This is run during the *finalize* on all the assemblies that are evaluated. After it, that rule combines the statistics produced by all the evaluation rules. \\n\\n# Description of all options\\n```\\n bin/create_config_assembly.py -h\\nusage: create_configuration_file [-h] [--configFile configFile] [--specFile specFile] [--ndconfFile ndconfFile] [--concat-cores concat_cores]\\n                                 [--genome-size genome_size] [--lr-type lr_type] [--basename base_name] [--species species] [--keep-intermediate]\\n                                 [--preprocess-lr-step PREPROCESS_ONT_STEP] [--preprocess-10X-step PREPROCESS_10X_STEP]\\n                                 [--preprocess-illumina-step PREPROCESS_ILLUMINA_STEP] [--preprocess-hic-step PREPROCESS_HIC_STEP]\\n                                 [--flye-step FLYE_STEP] [--no-flye] [--nextdenovo-step NEXTDENOVO_STEP] [--run-nextdenovo]\\n                                 [--nextpolish-cores nextpolish_cores] [--minimap2-cores minimap2_cores] [--bwa-cores bwa_cores]\\n                                 [--hypo-cores hypo_cores] [--pairtools-cores pairtools_cores] [--busco-cores busco_cores]\\n                                 [--nextpolish-ont-rounds nextpolish_ont_rounds] [--nextpolish-ill-rounds nextpolish_ill_rounds]\\n                                 [--hypo-rounds hypo_rounds] [--longranger-cores longranger_cores] [--longranger-path longranger_path]\\n                                 [--genomescope-opts genomescope_additional] [--no-purgedups] [--ploidy ploidy] [--run-tigmint] [--run-kraken2]\\n                                 [--no-yahs] [--scripts-dir SCRIPTS_DIR] [--ont-reads ONT_READS] [--ont-dir ONT_DIR] [--ont-filt ONT_FILTERED]\\n                                 [--pe1 PE1] [--pe2 PE2] [--processed-illumina PROCESSED_ILLUMINA] [--raw-10X RAW_10X [RAW_10X ...]]\\n                                 [--processed-10X PROCESSED_10X] [--10X R10X] [--illumina-dir ILLUMINA_DIR]\\n                                 [--assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]]\\n                                 [--postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]]\\n                                 [--curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]] [--hic-dir HIC_DIR]\\n                                 [--pipeline-workdir PIPELINE_WORKDIR] [--filtlong-dir FILTLONG_DIR] [--concat-hic-dir CONCAT_HIC_DIR]\\n                                 [--flye-dir FLYE_DIR] [--nextdenovo-dir NEXTDENOVO_DIR] [--flye-polishing-dir POLISH_FLYE_DIR]\\n                                 [--nextdenovo-polishing-dir POLISH_NEXTDENOVO_DIR] [--eval-dir eval_dir] [--stats-out stats_out]\\n                                 [--hic-qc-dir hic_qc_dir] [--filtlong-minlen filtlong_minlen] [--filtlong-min-mean-q filtlong_min_mean_q]\\n                                 [--filtlong-opts filtlong_opts] [--kraken2-db kraken2_db] [--kraken2-kmer kraken2_kmers]\\n                                 [--kraken2-opts additional_kraken2_opts] [--kraken2-cores kraken2_threads] [--trim-galore-opts trim_galore_opts]\\n                                 [--trim-Illumina-cores Trim_Illumina_cores] [--flye-cores flye_cores] [--flye-polishing-iterations flye_pol_it]\\n                                 [--other-flye-opts other_flye_opts] [--nextdenovo-cores nextdenovo_cores] [--nextdenovo-jobtype nextdenovo_type]\\n                                 [--nextdenovo-task nextdenovo_task] [--nextdenovo-rewrite nextdenovo_rewrite]\\n                                 [--nextdenovo-parallel_jobs nextdenovo_parallel_jobs] [--nextdenovo-minreadlen nextdenovo_minreadlen]\\n                                 [--nextdenovo-seeddepth nextdenovo_seeddepth] [--nextdenovo-seedcutoff nextdenovo_seedcutoff]\\n                                 [--nextdenovo-blocksize nextdenovo_blocksize] [--nextdenovo-pa-correction  nextdenovo_pa_correction]\\n                                 [--nextdenovo-minimap_raw nextdenovo_minimap_raw] [--nextdenovo-minimap_cns nextdenovo_minimap_cns]\\n                                 [--nextdenovo-minimap_map nextdenovo_minimap_map] [--nextdenovo-sort nextdenovo_sort]\\n                                 [--nextdenovo-correction_opts nextdenovo_correction_opts] [--nextdenovo-nextgraph_opt nextdenovo_nextgraph_opt]\\n                                 [--sr-cov ill_cov] [--hypo-proc hypo_processes] [--hypo-no-lr] [--hypo-opts hypo_opts]\\n                                 [--purgedups-cores purgedups_cores] [--purgedups-calcuts-opts calcuts_opts] [--tigmint-cores tigmint_cores]\\n                                 [--tigmint-opts tigmint_opts] [--hic-qc] [--no-pretext] [--assembly-qc assembly_qc] [--yahs-cores yahs_cores]\\n                                 [--yahs-mq yahs_mq] [--yahs-opts yahs_opts] [--hic-map-opts hic_map_opts] [--mq mq [mq ...]]\\n                                 [--hic-qc-assemblylen hic_qc_assemblylen] [--blast-cores blast_cores] [--hic-blastdb blastdb]\\n                                 [--hic-readsblast hic_readsblast] [--no-final-evals] [--busco-lin busco_lineage] [--merqury-db merqury_db]\\n                                 [--merqury-plot-opts merqury_plot_opts] [--meryl-k meryl_k] [--meryl-threads meryl_threads]\\n                                 [--meryl-reads meryl_reads [meryl_reads ...]] [--ont-list ONT_wildcards] [--illumina-list illumina_wildcards]\\n                                 [--r10X-list r10X_wildcards] [--hic-list hic_wildcards]\\n\\nCreate a configuration json file for the assembly pipeline.\\n\\noptions:\\n  -h, --help            show this help message and exit\\n\\nGeneral Parameters:\\n  --configFile configFile\\n                        Configuration JSON to be generated. Default assembly.config\\n  --specFile specFile   Cluster specifications JSON fileto be generated. Default assembly.spec\\n  --ndconfFile ndconfFile\\n                        Name pf the nextdenovo config file. Default nextdenovo.config\\n  --concat-cores concat_cores\\n                        Number of threads to concatenate reads and to run filtlong. Default 4\\n  --genome-size genome_size\\n                        Approximate genome size. Example: 615m or 2.6g. Default None\\n  --lr-type lr_type     Type of long reads (options are flye read-type options). Default nano-hq\\n  --basename base_name  Base name for the project. Default None\\n  --species species     Name of the species to be assembled. Default None\\n  --keep-intermediate   Set this to True if you do not want intermediate files to be removed. Default False\\n  --preprocess-lr-step PREPROCESS_ONT_STEP\\n                        Step for preprocessing long-reads. Default 02.1\\n  --preprocess-10X-step PREPROCESS_10X_STEP\\n                        Step for preprocessing 10X reads. Default 02.2\\n  --preprocess-illumina-step PREPROCESS_ILLUMINA_STEP\\n                        Step for preprocessing illumina reads. Default 02.2\\n  --preprocess-hic-step PREPROCESS_HIC_STEP\\n                        Step for preprocessing hic reads. Default 02.3\\n  --flye-step FLYE_STEP\\n                        Step for running flye. Default 03.1\\n  --no-flye             Give this option if you do not want to run Flye.\\n  --nextdenovo-step NEXTDENOVO_STEP\\n                        Step for running nextdenovo. Default 03.2\\n  --run-nextdenovo      Give this option if you do want to run Nextdenovo.\\n  --nextpolish-cores nextpolish_cores\\n                        Number of threads to run the nextpolish step. Default 24\\n  --minimap2-cores minimap2_cores\\n                        Number of threads to run the alignment with minimap2. Default 32\\n  --bwa-cores bwa_cores\\n                        Number of threads to run the alignments with BWA-Mem2. Default 16\\n  --hypo-cores hypo_cores\\n                        Number of threads to run the hypo step. Default 24\\n  --pairtools-cores pairtools_cores\\n                        Number of threads to run the pairtools step. Default 100\\n  --busco-cores busco_cores\\n                        Number of threads to run BUSCO. Default 32\\n  --nextpolish-ont-rounds nextpolish_ont_rounds\\n                        Number of rounds to run the Nextpolish with ONT step. Default 0\\n  --nextpolish-ill-rounds nextpolish_ill_rounds\\n                        Number of rounds to run the Nextpolish with illumina step. Default 0\\n  --hypo-rounds hypo_rounds\\n                        Number of rounds to run the Hypostep. Default 1\\n  --longranger-cores longranger_cores\\n                        Number of threads to run longranger. Default 16\\n  --longranger-path longranger_path\\n                        Path to longranger executable. Default /scratch/project/devel/aateam/src/10X/longranger-2.2.2\\n  --genomescope-opts genomescope_additional\\n                        Additional options to run Genomescope2 with. Default -m 10000\\n  --no-purgedups        Give this option if you do not want to run Purgedups.\\n  --ploidy ploidy       Expected ploidy. Default 2\\n  --run-tigmint         Give this option if you want to run the scaffolding with 10X reads step.\\n  --run-kraken2         Give this option if you want to run Kraken2 on the input reads.\\n  --no-yahs             Give this option if you do not want to run yahs.\\n\\nInputs:\\n  --scripts-dir SCRIPTS_DIR\\n                        Directory with the different scripts for the pipeline. Default\\n                        /software/assembly/pipelines/Assembly_pipeline/CLAWSv2.2/bin/../scripts/\\n  --ont-reads ONT_READS\\n                        File with all the ONT reads. Default None\\n  --ont-dir ONT_DIR     Directory where the ONT fastqs are stored. Default None\\n  --ont-filt ONT_FILTERED\\n                        File with the ONT reads after running filtlong on them. Default None\\n  --pe1 PE1             File with the illumina paired-end fastqs, already trimmed, pair 1.\\n  --pe2 PE2             File with the illumina paired-end fastqs, already trimmed, pair 2.\\n  --processed-illumina PROCESSED_ILLUMINA\\n                        Directory to Processed illumina reads. Already there or to be produced by the pipeline.\\n  --raw-10X RAW_10X [RAW_10X ...]\\n                        Dictionary with 10X raw read directories, it has to be the mkfastq dir. You must specify as well the sampleIDs from this run.\\n                        Example: \\'{\"mkfastq-dir\":\"sample1,sample2,sample3\"}\\'...\\n  --processed-10X PROCESSED_10X\\n                        Directory to Processed 10X reads. Already there or to be produced by the pipeline.\\n  --10X R10X            File with barcoded 10X reads in fastq.gz format, concatenated.\\n  --illumina-dir ILLUMINA_DIR\\n                        Directory where the raw illumina fastqs are stored. Default None\\n  --assembly-in ASSEMBLY_IN [ASSEMBLY_IN ...]\\n                        Dictionary with assemblies that need to be polished but not assembled and directory where they should be polished. Example:\\n                        \\'{\"assembly1\":\"polishing_dir1\"}\\' \\'{\"assembly2\"=\"polishing_dir2\"}\\' ...\\n  --postpolish-assemblies POSTPOLISH_ASSEMBLIES [POSTPOLISH_ASSEMBLIES ...]\\n                        Dictionary with assemblies for whic postpolishing steps need to be run but that are not assembled and base step for the\\n                        directory where the first postpolishing step should be run. Example: \\'{\"assembly1\":\"s04.1_p03.1\"}\\'\\n                        \\'{\"assembly2\":\"s04.2_p03.2\"}\\' ...\\n  --curated-assemblies CURATED_ASSEMBLIES [CURATED_ASSEMBLIES ...]\\n                        Dictionary with assemblies that have already been curated. Evaluations and read alignment will be perforder. Example:\\n                        \\'{\"assembly1\":\"s04.1_p03.1\"}\\' \\'{\"assembly2\":\"s04.2_p03.2\"}\\' ...\\n  --hic-dir HIC_DIR     Directory where the HiC fastqs are stored. Default None\\n\\nOutputs:\\n  --pipeline-workdir PIPELINE_WORKDIR\\n                        Base directory for the pipeline run. Default /scratch_isilon/groups/assembly/jgomez/test_CLAWSv2/ilErePala/assembly/\\n  --filtlong-dir FILTLONG_DIR\\n                        Directory to process the ONT reads with filtlong. Default s02.1_p01.1_Filtlong\\n  --concat-hic-dir CONCAT_HIC_DIR\\n                        Directory to concatenate the HiC reads. Default s02.3_p01.1_Concat_HiC\\n  --flye-dir FLYE_DIR   Directory to run flye. Default s03.1_p02.1_flye/\\n  --nextdenovo-dir NEXTDENOVO_DIR\\n                        Directory to run nextdenovo. Default s03.2_p02.1_nextdenovo/\\n  --flye-polishing-dir POLISH_FLYE_DIR\\n                        Directory to polish the flye assembly. Default s04.1_p03.1_polishing/\\n  --nextdenovo-polishing-dir POLISH_NEXTDENOVO_DIR\\n                        Directory to run nextdenovo. Default s04.2_p03.2_polishing/\\n  --eval-dir eval_dir   Base directory for the evaluations. Default evaluations/\\n  --stats-out stats_out\\n                        Path to the file with the final statistics.\\n  --hic-qc-dir hic_qc_dir\\n                        Directory to run the hic_qc. Default hic_qc/\\n\\nFiltlong:\\n  --filtlong-minlen filtlong_minlen\\n                        Minimum read length to use with Filtlong. Default 1000\\n  --filtlong-min-mean-q filtlong_min_mean_q\\n                        Minimum mean quality to use with Filtlong. Default 80\\n  --filtlong-opts filtlong_opts\\n                        Extra options to run Filtlong (eg. -t 4000000000)\\n\\nKraken2:\\n  --kraken2-db kraken2_db\\n                        Database to be used for running Kraken2. Default None\\n  --kraken2-kmer kraken2_kmers\\n                        Database to be used for running Kraken2. Default None\\n  --kraken2-opts additional_kraken2_opts\\n                        Optional parameters for the rule Kraken2. Default\\n  --kraken2-cores kraken2_threads\\n                        Number of threads to run the Kraken2 step. Default 16\\n\\nTrim_Galore:\\n  --trim-galore-opts trim_galore_opts\\n                        Optional parameters for the rule trim_galore. Default --max_n 0 --gzip -q 20 --paired --retain_unpaired\\n  --trim-Illumina-cores Trim_Illumina_cores\\n                        Number of threads to run the Illumina trimming step. Default 8\\n\\nFlye:\\n  --flye-cores flye_cores\\n                        Number of threads to run FLYE. Default 128\\n  --flye-polishing-iterations flye_pol_it\\n                        Number of polishing iterations to use with FLYE. Default 2\\n  --other-flye-opts other_flye_opts\\n                        Additional options to run Flye. Default --scaffold\\n\\nNextdenovo:\\n  --nextdenovo-cores nextdenovo_cores\\n                        Number of threads to run nextdenovo. Default 2\\n  --nextdenovo-jobtype nextdenovo_type\\n                        Job_type for nextdenovo. Default slurm\\n  --nextdenovo-task nextdenovo_task\\n                        Task need to run. Default all\\n  --nextdenovo-rewrite nextdenovo_rewrite\\n                        Overwrite existing directory. Default yes\\n  --nextdenovo-parallel_jobs nextdenovo_parallel_jobs\\n                        Number of tasks used to run in parallel. Default 50\\n  --nextdenovo-minreadlen nextdenovo_minreadlen\\n                        Filter reads with length < minreadlen. Default 1k\\n  --nextdenovo-seeddepth nextdenovo_seeddepth\\n                        Expected seed depth, used to calculate seed_cutoff, co-use with genome_size, you can try to set it 30-45 to get a better\\n                        assembly result. Default 45\\n  --nextdenovo-seedcutoff nextdenovo_seedcutoff\\n                        Minimum seed length, <=0 means calculate it automatically using bin/seq_stat. Default 0\\n  --nextdenovo-blocksize nextdenovo_blocksize\\n                        Block size for parallel running, split non-seed reads into small files, the maximum size of each file is blocksize. Default 1g\\n  --nextdenovo-pa-correction  nextdenovo_pa_correction\\n                        number of corrected tasks used to run in parallel, each corrected task requires ~TOTAL_INPUT_BASES/4 bytes of memory usage,\\n                        overwrite parallel_jobs only for this step. Default 100\\n  --nextdenovo-minimap_raw nextdenovo_minimap_raw\\n                        minimap2 options, used to find overlaps between raw reads, see minimap2-nd for details. Default -t 30\\n  --nextdenovo-minimap_cns nextdenovo_minimap_cns\\n                        minimap2 options, used to find overlaps between corrected reads. Default -t 30\\n  --nextdenovo-minimap_map nextdenovo_minimap_map\\n                        minimap2 options, used to map reads back to the assembly. Default -t 30 --no-kalloc\\n  --nextdenovo-sort nextdenovo_sort\\n                        sort options, see ovl_sort for details. Default -m 400g -t 20\\n  --nextdenovo-correction_opts nextdenovo_correction_opts\\n                        Correction options. Default -p 30 -dbuf\\n  --nextdenovo-nextgraph_opt nextdenovo_nextgraph_opt\\n                        nextgraph options, see nextgraph for details. Default -a 1\\n\\nHypo:\\n  --sr-cov ill_cov      Approximate short read coverage for hypo Default 0\\n  --hypo-proc hypo_processes\\n                        Number of contigs to be processed in parallel by HyPo. Default 6\\n  --hypo-no-lr          Set this to false if you don¡t want to run hypo with long reads. Default True\\n  --hypo-opts hypo_opts\\n                        Additional options to run Hypo. Default None\\n\\nPurge_dups:\\n  --purgedups-cores purgedups_cores\\n                        Number of threads to run purgedups. Default 8\\n  --purgedups-calcuts-opts calcuts_opts\\n                        Adjusted values to run calcuts for purgedups. Default None\\n\\nScaffold_with_10X:\\n  --tigmint-cores tigmint_cores\\n                        Number of threads to run the 10X scaffolding step. Default 12\\n  --tigmint-opts tigmint_opts\\n                        Adjusted values to run the scaffolding with 10X reads. Default None\\n\\nHiC:\\n  --hic-qc              Give this option if only QC of the HiC data needs to be done.\\n  --no-pretext          Give this option if you do not want to generate the pretext file\\n  --assembly-qc assembly_qc\\n                        Path to the assembly to be used perfom the QC of the HiC reads.\\n  --yahs-cores yahs_cores\\n                        Number of threads to run YAHS. Default 48\\n  --yahs-mq yahs_mq     Mapping quality to use when running yahs.Default 40\\n  --yahs-opts yahs_opts\\n                        Additional options to give to YAHS.Default\\n  --hic-map-opts hic_map_opts\\n                        Options to use with bwa mem when aligning the HiC reads. Deafault -5SP -T0\\n  --mq mq [mq ...]      Mapping qualities to use for processing the hic mappings. Default [0, 40]\\n  --hic-qc-assemblylen hic_qc_assemblylen\\n                        Lentgh of the assembly to be used for HiC QC\\n  --blast-cores blast_cores\\n                        Number of threads to run blast with the HiC unmapped reads.Default 8\\n  --hic-blastdb blastdb\\n                        BLAST Database to use to classify the hic unmapped reads. Default /scratch_isilon/groups/assembly/data/blastdbs\\n  --hic-readsblast hic_readsblast\\n                        Number of unmapped hic reads to classify with blast. Default 100\\n\\nFinalize:\\n  --no-final-evals      If specified, do not run evaluations on final assemblies. Default True\\n  --busco-lin busco_lineage\\n                        Path to the lineage directory to run Busco with. Default None\\n  --merqury-db merqury_db\\n                        Meryl database. Default None\\n  --merqury-plot-opts merqury_plot_opts\\n                        Meryl database. Default None\\n  --meryl-k meryl_k     Merqury plot additional options, for example \" -m 200 -n 6000|\". Default None\\n  --meryl-threads meryl_threads\\n                        Number of threads to run meryl and merqury. Default 4\\n  --meryl-reads meryl_reads [meryl_reads ...]\\n                        Type of reads to be used to build the meryldb. Default ont illumina\\n\\nWildcards:\\n  --ont-list ONT_wildcards\\n                        List with basename of the ONT fastqs that will be used. Default None\\n  --illumina-list illumina_wildcards\\n                        List with basename of the illumina fastqs. Default None\\n  --r10X-list r10X_wildcards\\n                        List with basename of the raw 10X fastqs. Default None\\n  --hic-list hic_wildcards\\n                        List with basename of the raw hic fastqs. Default None\\n```\\n# Changes made to v2.2: \\n\\n1. General: \\n\\n\\tNow default read_type is nano-hq \\n\\n2. Rule trim_galore: \\n\\n\\t\"--max_n 0\" has been added to the default behaviour of \"--trim-galore-opts\" \\n\\n3. Meryl: \\n\\n\\tNew option \"--meryl-reads\" has been added to the config. Default is \"Illumina ont\" to build the meryl database using both type of reads, it can be changed to one or the other \\n\\n4. Merqury: \\n\\n\\tOption \"--merqury-plot-opts\" has been added to config file. It can be used to modify the x and y axis maximum values (eg. --merqury-plot-opts \" -m 200 -n 6000\") \\n\\n5. Genomescope: \\n\\n\\t\"-m 10000\" is now part of the default behavior of \"--genomescope-opts\" \\n\\n6. Hic_statistics: \\n\\n\\tThis is now running for each assembly and mq for which a pretext file is generated \\n\\n7. Assembly inputs for different steps: \\n\\n\\ta. \"--assembly-in\" to start after assembly step (eg. Evaluation, polishing, purging and scaffolding) \\n\\n\\tb. \"--postpolish-assemblies\" to start after polishing step (eg. Evaluation, purging and scaffolding) \\n\\n\\tc. \"--curated-assemblies\" to start after scaffolding step (eg. Evaluation and pretext generation) \\n'\n",
      " \"# ERGA Protein-coding gene annotation workflow.\\nAdapted from the work of Sagane Joye:\\n\\nhttps://github.com/sdind/genome_annotation_workflow\\n\\n## Prerequisites\\n\\nThe following programs are required to run the workflow and the listed version were tested. It should be noted that older versions of snakemake are not compatible with newer versions of singularity as is noted here: [https://github.com/nextflow-io/nextflow/issues/1659](https://github.com/nextflow-io/nextflow/issues/1659).\\n\\n`conda v 23.7.3`\\n\\n`singularity v 3.7.3`\\n\\n`snakemake v 7.32.3` \\n\\nYou will also need to acquire a licence key for Genemark and place this in your home directory with name `~/.gm_key` The key file can be obtained from the following location, where the licence should be read and agreed to: http://topaz.gatech.edu/GeneMark/license_download.cgi\\n\\n## Workflow\\n\\nThe pipeline is based on braker3 and was tested on the following dataset from Drosophila melanogaster: [https://doi.org/10.5281/zenodo.8013373](https://doi.org/10.5281/zenodo.8013373)\\n\\n### Input data\\n\\n- Reference genome in fasta format\\n\\n- RNAseq data in paired-end zipped fastq format\\n\\n- uniprot fasta sequences in zipped fasta format\\n\\n### Pipeline steps\\n\\n- **Repeat Model and Mask** Run RepeatModeler using the genome as input, filter any repeats also annotated as protein sequences in the uniprot database and use this filtered libray to mask the genome with RepeatMasker\\n\\n- **Map RNAseq data** Trim any remaining adapter sequences and map the trimmed reads to the input genome\\n\\n- **Run gene prediction software** Use the mapped RNAseq reads and the uniprot sequences to create hints for gene prediction using Braker3 on the masked genome\\n\\n- **Evaluate annotation** Run BUSCO to evaluate the completeness of the annotation produced\\n\\n### Output data\\n\\n- FastQC reports for input RNAseq data before and after adapter trimming\\n\\n- RepeatMasker report containing quantity of masked sequence and distribution among TE families\\n\\n- Protein-coding gene annotation file in gff3 format\\n\\n- BUSCO summary of annotated sequences\\n\\n## Setup\\n\\nYour data should be placed in the `data` folder, with the reference genome in the folder `data/ref` and the transcript data in the foler `data/rnaseq`.\\n\\nThe config file requires the following to be given:\\n\\n```\\nasm: 'absolute path to reference fasta'\\nsnakemake_dir_path: 'path to snakemake working directory'\\nname: 'name for project, e.g. mHomSap1'\\nRNA_dir: 'absolute path to rnaseq directory'\\nbusco_phylum: 'busco database to use for evaluation e.g. mammalia_odb10'\\n```\\n\"\n",
      " 'This workflow represents the Default ML Pipeline for AutoML feature from MLme. Machine Learning Made Easy (MLme) is a novel tool that simplifies machine learning (ML) for researchers. By integrating four essential functionalities, namely data exploration, AutoML, CustomML, and visualization, MLme fulfills the diverse requirements of researchers while eliminating the need for extensive coding efforts. MLme serves as a valuable resource that empowers researchers of all technical levels to leverage ML for insightful data analysis and enhance research outcomes. By simplifying and automating various stages of the ML workflow, it enables researchers to allocate more time to their core research tasks, thereby enhancing efficiency and productivity.\\n\\n'\n",
      " '# RepeatMasking Workflow\\n\\nThis workflow uses RepeatModeler and RepeatMasker for genome analysis.\\n\\n- RepeatModeler is a software package for identifying and modeling de novo families of transposable elements (TEs). At the heart of RepeatModeler are three de novo repeat search programs (RECON, RepeatScout and LtrHarvest/Ltr_retriever) which use complementary computational methods to identify repeat element boundaries and family relationships from sequence data.\\n\\n- RepeatMasker is a program that analyzes DNA sequences for *interleaved repeats* and *low-complexity* DNA sequences. The result of the program is a detailed annotation of the repeats present in the query sequence, as well as a modified version of the query sequence in which all annotated repeats are present.\\n\\n## Input dataset for RepeatModeler\\n- RepeatModeler requires a single input file, a genome in fasta format.\\n\\n\\n## Outputs dataset for RepeatModeler\\n- Two output files are generated:\\n    - summary file (.tbl)\\n    - fasta file containing alignments in order of appearance in the query sequence\\n\\n\\n## Input dataset for RepeatMasker\\n- ReapatMasker requires the fasta file generated by RepeatModeler\\n\\n## Outputs datasets for RepeatMasker\\n- Five output files are generated:\\n    - a fasta file\\n    - .gff3 file\\n    - a table summarizing the repeated content of the sequence analyzed\\n    - a file with statistics related to the repeated content of the sequence analyzed\\n    - a summary of the mutation sites found and the order of grouping\\n    \\n'\n",
      " 'This repository contains the python code to reproduce the experiments in Dłotko, Gurnari \"Euler Characteristic Curves and Profiles: a stable shape invariant for big data problems\"'\n",
      " 'We assume the identifiers of the input list are like:\\nsample_name_replicateID.\\nThe identifiers of the output list will be:\\nsample_name'\n",
      " 'The simplest workflow among a collection of workflows intended to solve tasks up to CTF estimation.'\n",
      " 'The second-level complexity workflow is one among a collection of workflows designed to address tasks up to CTF estimation. In addition to the functionalities provided by the layer 0 workflow, this workflow aims to enhance the quality of acquisition images using quality protocols.\\n\\n**Quality control protocols**\\n\\n* **Movie max shift**: automatic reject those movies whose frames move more than a given threshold.\\xa0\\n\\n* **Tilt analysis**: quality score based in the Power Spectrum Density (astigmatism and tilt)\\xa0\\n\\n* **CTF consensus**: acts as a filter discarding micrographs based on their CTF (limit resolution, defocus, astigmatism, etc.).\\n\\n**Advantages:**\\xa0\\n\\n* More control of the acquisition quality\\n\\n* Reduce unnecessary processing time and storage'\n",
      " 'The ultimate-level complexity workflow is one among a collection of workflows designed to address tasks up to CTF estimation. In addition to the functionalities provided by layer 0 and 1 workflows, this workflow aims to enhance the quality of both **acquisition images** and **processing**.\\n\\n**Quality control protocols**\\n\\n…\\n\\n**Combination of methods**\\n* **CTF consensus**\\n        * New methods to compare ctf estimations\\n        * CTF xmipp criteria (richer parameters i.e. ice detection)\\n\\n**Advantages**:\\xa0\\n* Control of the acquisition quality\\n* Robust estimations to continue with the processing'\n",
      " 'The workflow takes a paired-reads collection (like illumina WGS or HiC), runs FastQC and SeqKit, trims with Fastp, and creates a MultiQC report. The main outputs are a paired collection of trimmed reads, a report with raw and trimmed reads stats, and a table with raw reads stats.'\n",
      " 'The workflow takes a HiFi reads collection, runs FastQC and SeqKit, filters with Cutadapt, and creates a MultiQC report. The main outputs are a collection of filtred reads, a report with raw and filtered reads stats, and a table with raw reads stats.'\n",
      " 'The workflow takes a trimmed HiFi reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy. The main results are K-mer database and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 21 and 2, respectively. '\n",
      " 'The workflow takes a trimmed HiFi reads collection, Forward/Reverse HiC reads, and the max coverage depth (calculated from WF1) to run Hifiasm in HiC phasing mode. It produces both Pri/Alt and Hap1/Hap2 assemblies, and runs all the QC analysis (gfastats, BUSCO, and Merqury). The default Hifiasm purge level is Light (l1).'\n",
      " 'The workflow takes a trimmed HiFi reads collection, Hap1/Hap2 contigs, and the values for transition parameter and max coverage depth (calculated from WF1) to run Purge_Dups. It produces purged Hap1 and Hap2 contigs assemblies, and runs all the QC analysis (gfastats, BUSCO, and Merqury).'\n",
      " 'This Galaxy workflow takes a list of tumor/normal sample pair variants in VCF format and\\n1. annotates them using the ENSEMBL Variant Effect Predictor and custom annotation data\\n2. turns the annotated VCF into a MAF file for import into cBioPortal\\n3. generates human-readable variant- and gene-centric reports\\n\\nThe input VCF is expected to encode somatic status, somatic p-value and germline p-value of each variant in varscan somatic format, i.e., via SS, SPV and GPV INFO keys, respectively.'\n",
      " 'The workflow takes trimmed HiC forward and reverse reads, and Hap1/Hap2 assemblies to produce Hap1 and Hap2 scaffolded assemblies using YaHS. It also runs all the QC analyses (gfastats, BUSCO, Merqury and Pretext).'\n",
      " \"## Summary\\nHPPIDiscovery is a scientific workflow to augment, predict and perform an insilico curation of host-pathogen Protein-Protein Interactions (PPIs) using graph theory to build new candidate ppis and machine learning to predict and evaluate them by combining multiple PPI detection methods of proteins according to three categories: structural,  based on primary aminoacid sequence and functional annotations.<br>\\n\\nHPPIDiscovery contains three main steps: (i) acquirement of pathogen and host proteins information from seed ppis provided by HPIDB search methods, (ii) Model training and generation of new candidate ppis from HPIDB seed proteins' partners, and (iii) Evaluation of new candidate ppis and results exportation.\\n\\n(i) The first step acquires the identification of the taxonomy ids of the host and pathogen organisms in the result files. Then it proceeds parsing and cleaning the HPIDB results and downloading the protein interactions of the found organisms from the STRING database. The string protein identifiers are also mapped using the id mapping tool of uniprot API and we retrieve the uniprot entry ids along with the functional annotations, sequence, domain and kegg enzymes.\\n\\n(ii) The second step builds the training dataset using the non redundant hpidb validated interactions of each genome as positive set and random string low confidence ppis from each genome as negative set. Then, PredPrin tool is executed in the training mode to obtain the model that will evaluate the new candidate PPIs. The new ppis are then generated by performing a pairwise combination of string partners of host and pathogen hpidb proteins. \\n\\nFinally, (iii) in the third step, the predprin tool is used in the test mode to evaluate the new ppis and generate the reports and list of positively predicted ppis.\\n\\nThe figure below illustrates the steps of this workflow.\\n\\n## Requirements:\\n* Edit the configuration file (config.yaml) according to your own data, filling out the following fields:\\n\\t- base_data: location of the organism folders directory, example: /home/user/data/genomes \\n\\t- parameters_file: Since this workflow may perform parallel processing of multiple organisms at the same time, you must prepate a tabulated file containng the genome folder names located in base data, where the hpidb files are located. Example: /home/user/data/params.tsv. It must have the following columns: genome (folder name), hpidb_seed_network (the result exported by one of the search methods available in hpidb database), hpidb_search_method (the type of search used to generate the results) and target_taxon (the target taxon id). The column hpidb_source may have two values: keyword or homology. In the keyword mode, you provide a taxonomy, protein name, publication id or detection method and you save all results (mitab.zip) in the genome folder. Finally, in the homology mode allows the user to search for host pathogen ppis giving as input fasta sequences of a set of proteins of the target pathgen for enrichment (so you have to select the search for a pathogen set) and you save the zip folder results (interaction data) in the genome folder. This option is extremely useful when you are not sure that your organism has validated protein interactions, then it finds validated interactions from the closest proteins in the database. In case of using the homology mode, the identifiers of the pathogens' query fasta sequences must be a Uniprot ID. All the query protein IDs must belong to the same target organism (taxon id).\\n\\t- model_file: path of a previously trained model in joblib format (if you want to train from the known validated PPIs given as seeds, just put a 'None' value)\\n\\n## Usage Instructions\\nThe steps below consider the creation of a sqlite database file with all he tasks events which can be used after to retrieve the execution time taken by the tasks. It is possible run locally too (see luigi's documentation to change the running command). <br ><br>\\n* Preparation:\\n\\t1. ````git clone https://github.com/YasCoMa/hppidiscovery.git````\\n\\t2. ````cd hppidiscovery````\\n\\t3. ````mkdir luigi_log```` \\n\\t4. ````luigid --background --logdir luigi_log```` (start luigi server)\\n\\t5. conda env create -f hp_ppi_augmentation.yml\\n\\t6. conda activate hp_ppi_augmentation\\n\\t6.1. (execute ````pip3 install wget```` (it is not installed in the environment))\\n\\t7. run ````pwd```` command and get the full path\\n\\t8. Substitute <path> in config_example.yaml with the full path obtained in the previous step\\n\\t9. Download SPRINT pre-computed similarities in https://www.csd.uwo.ca/~ilie/SPRINT/precomputed_similarities.zip and unzip it inside workflow_hpAugmentation/predprin/core/sprint/HSP/\\n\\t10. ````cd workflow_hpAugmentation/predprin/````\\n\\t11. Uncompress annotation_data.zip\\n\\t12. Uncompress sequence_data.zip\\n\\t13. ````cd ../../````\\n\\t14. ````cd workflow_hpAugmentation````\\n\\t15. snake -n (check the plan of jobs, it should return no errors and exceptions)\\n\\t16. snakemake -j 4 (change this number according the number of genomes to analyse and the amount of cores available in your machine)\"\n",
      " '## Contiging Solo:\\n\\nGenerate assembly based on PacBio Hifi Reads.\\n\\n\\n### Inputs\\n\\n\\n1. Hifi long reads [fastq]\\n2. K-mer database [meryldb]\\n3. Genome profile summary generated by Genomescope [txt]\\n4. Homozygous Read Coverage. Optional, use if you think the estimation from Genomescope is inacurate. \\n5. Genomescope Model Parameters generated by Genomescope [tabular]\\n6. Database for busco lineage (recommended: latest)\\n7. Busco lineage (recommended: vertebrata)\\n8. Name of first assembly\\n9. Name of second assembly\\n\\n\\n### Outputs\\n\\n1. Primary assembly\\n2. Alternate assembly\\n3. QC: Bandage image for the raw unitigs\\n4. QC: BUSCO report for both assemblies\\n5. QC: Merqury report for both assemblies\\n6. QC: Assembly statistics for both assemblies\\n7. QC: Nx plot for both assemblies\\n8. QC: Size plot for both assemblie\\n'\n",
      " 'Automated inference of stable isotope incorporation rates in proteins for functional metaproteomics'\n",
      " 'A demonstration workflow for Reduced Order Modeling (ROM) within the eFlows4HPC project, implemented using Kratos Multiphysics, EZyRB, COMPSs, and dislib.'\n",
      " 'Run baredSC in 1 dimension in logNorm for 1 to N gaussians and combine models.'\n",
      " \"## Summary\\nPredPrIn is a scientific workflow to predict Protein-Protein Interactions (PPIs) using machine learning to combine multiple PPI detection methods of proteins according to three categories: structural,  based on primary aminoacid sequence and functional annotations.<br>\\n\\nPredPrIn contains three main steps: (i) acquirement and treatment of protein information, (ii) feature generation, and (iii) classification and analysis.\\n\\n(i) The first step builds a knowledge base with the available annotations of proteins and reuses this base for other prediction experiments, saving time and becoming more efficient. \\n\\n(ii) The feature generation step involves several evidence from different classes, such as: Gene Ontology (GO) information, domain interaction, metabolic pathway participation and sequence-based interaction. For the GO branches, we made a study to evaluate the best method to calculate semantic similarity to enhance the workflow performance. This step can be easily modified by adding new metrics, making PredPrIn flexible for future improvements. \\n\\nFinally, (iii) in the third step, the adaboost classifier is responsible for predicting the final scores from the numerical features dataset, exporting results of performance evaluation metrics.\\n\\n## Requirements:\\n* Python packages needed:\\n    - pip3 install luigi\\n        - pip3 install sqlalchemy\\n        - pip3 install rdflib\\n        - pip3 install sklearn\\n        - pip3 install matplotlib\\n        - pip3 install numpy\\n\\n* Other instalation:\\n        - sqlite (to be able to see the documentation generated by luigi about the tasks after execution)\\n\\n## Usage Instructions\\nThe steps below consider the creation of a sqlite database file with all he tasks events which can be used after to retrieve the execution time taken by the tasks. It is possible run locally too (see luigi's documentation to change the running command). <br ><br>\\n* Preparation:\\n        1. ````git clone https://github.com/YasCoMa/predprin.git````\\n        2. ````cd PredPrIn````\\n        3. `pip3 install -r requirements.txt`\\n        4. Download annotation_data.zip (https://drive.google.com/file/d/1bWPSyULaooj7GTrDf6QBY3ZyeyH5MRpm/view?usp=share_link)\\n        5. Download rdf_data.zip (https://drive.google.com/file/d/1Cp511ioXiw2PiOHdkxa4XsZnxOeM3Pan/view?usp=share_link)\\n        6. Download sequence_data.zip (https://drive.google.com/file/d/1uEKh5EF9X_6fgZ9cTTp0jW3XaL48stxA/view?usp=share_link)\\n        7. Unzip annotation_data.zip\\n        8. Unzip rdf_data.zip\\n        9. Unzip sequence_data.zip\\n        10. Download SPRINT pre-computed similarities in https://www.csd.uwo.ca/~ilie/SPRINT/precomputed_similarities.zip and unzip it inside core/sprint/HSP/\\n        11. Certify that there is a file named client.cfg (to configure the history log and feed the sqlite database). It must have the following data:\\n        ````\\n        [core]\\n        default-scheduler-host=localhost\\n        default-scheduler-port=8082\\n        rpc-connect-timeout=60.0 \\n        rpc-retry-attempts=10    \\n        rpc-retry-wait=60        \\n\\n        [scheduler]\\n        record_task_history = True\\n\\n        [task_history]\\n        db_connection = sqlite:///luigi-task-hist.db\\n        ````\\n* Parameters:\\n        1. parameters-file -> json file with all the information to process the prediction experiment (example: params.json)\\n        2. mode -> it can have two values: train (executes cross validation and save the model as a .joblib file) or test (uses a model obtained in train mode to test in some dataset listed in the parameters file)\\n        3. model -> it is the model file full path saved in train mode as .joblib\\n        \\n* Running:\\n        1. ````mkdir luigi_log```` (or other name for the log folder of your choice)\\n        2. ````luigid --background --logdir luigi_log```` (start luigi server)\\n        3. ````nohup python3.5 -m luigi --module main RunPPIExperiment --parameters-file params.json --mode 'train' --model none.joblib --workers 3 &```` <br >\\n           ````nohup python3.5 -m luigi --module main RunPPIExperiment --parameters-file params.json --mode 'test' --model model.jolib --workers 3 &```` <br >\\n                - Replace python3.5 by the command python of your environment <br>\\n                - Replace the data given as example in params.json using your own data <br > \\n                - Adapt the number of workers to use as you need and the capacity of your computational resource available\\n\\n        You can monitor the prediction experiment execution in localhost:8082\\n\\n## Reference\\nMartins YC, Ziviani A, Nicolás MF, de Vasconcelos AT. Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy. Frontiers in Bioinformatics. 2021:38.\\nhttps://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full\\n\\n## Bug Report\\nPlease, use the [Issues](https://github.com/YasCoMa/PredPrIn/issues) tab to report any bug.\"\n",
      " '## Summary\\n\\nThe validation process proposed has two pipelines for filtering PPIs predicted by some _IN SILICO_  detection method, both pipelines can be executed separately. The first pipeline (i) filter according to association rules of cellular locations extracted from HINT database. The second pipeline (ii) filter according to scientific papers where both proteins in the PPIs appear in interaction context in the sentences.\\n\\nThe pipeline (i) starts extracting cellular component annotations from HINT PPIs building a dataset and then the Apriori algorithm is applied in this dataset in an iterative process that repeat the application of this algorithm till the rules cover 15 main locations in the cell. This process generate a database with association rules with two main columns: antecedent and consequent, meaning that a location that occurs in antecedent also occurs with the location in consequent. The filtering task evaluate the PPI checking if some location annotated for the first protein is in the antecedent column and if some location of the second protein is also in the same rule but in the consequent column. If so, the PPI passes according to the criteria.\\n\\nThe pipeline (ii) starts getting all papers that mention both proteins in the PPIs and extrating their content using the NCBI [API](https://www.ncbi.nlm.nih.gov/home/develop/api/). These XML files are cleaned removing hypertext markup and references to figures, tables and supplementary materials. The paragraphs of the remaining articles content are processed by Natural language processing steps to extract sentences, tokens, stopwords removal to remove words extremely common in english language and do not help to identify the context of interest, prioritizing tokens using part-of-speech tagging to keep just nouns and verbs. Then the sentences filtered goes to the task that identifies the proteins of the PPI in evaluation among the tokens and also tries to identify tokens or set of tokens that mention experimental methods. The sentences that have the proteins of interest are filtered if the nouns and verbs have some of the items of the list of words indicating interaction relation (recruit, bind, interact, signaling, etc). Finally, a report is made by pair with the article identifiers, the sentences, the proteins and interacting words found.\\n\\nThe figure below illustrates all the tasks of these pipelines.\\n\\n<div style=\"text-align: center\">\\n\\t<img src=\"pipeline.png\" alt=\"pipeline\"\\n\\ttitle=\"PPI validation process\" width=\"600px\" />\\n</div>\\n\\n## Requirements:\\n* Python packages needed:\\n\\t- pip3 install pandas\\n\\t- pip3 install rdflib\\n\\t- pip3 install mlxtend\\n\\t- pip3 install inflect\\n\\t- pip3 install nltk\\n\\t- pip3 install biopython\\n\\t- pip3 install lxml\\n\\t- pip3 install bs4 (beautiful soup)\\n\\n## Usage Instructions\\n### Preparation:\\n1. ````git clone https://github.com/YasCoMa/ppi_validation_process.git````\\n2. `pip3 install -r requirements.txt`\\n3. ````cd ppi_validation_process/pipe_location_assocRules/````\\n4. ````unzip pygosemsim.zip````\\n5. ````cd ../````\\n\\n### Filtering by association rules of cellular locations (first filtering part) - File ````pipe_location_assocRules/find_pattern.py```` :\\n* Pipeline parameters:\\n\\t- __-fo__ or __--folder__ <br>\\n\\t\\tFolder to store the files (use the folder where the other required file can be found)\\n\\t- __-if__ or __--interactome_file__ <br>\\n\\t\\tFile with the pairs (two columns with uniprot identifiers in tsv format)<br>\\n\\n\\t\\tExample of this file: pipe_location_assocRules/running_example/all_pairs.tsv\\n\\n\\n* Running modes examples:\\n\\t1. Go to the first filtering part folder: <br>\\n\\t````cd pipe_location_assocRules/````\\n\\n\\t2. Uncompress annotation_data.zip\\n\\t\\n\\t3. Run: <br>\\n\\t````python3 find_pattern.py -fo running_example/ -if all_pairs.tsv````\\n\\n\\n### Filtering by text mining on scientific papers (second filtering part) - File ````ppi_pubminer/pubmed_pmc_literature_pipeline.py````:\\n\\n* Pipeline parameters:\\n\\t- __-em__ or __--execution_mode__ <br>\\n\\t\\tUse to indicate the execution mode desired: <br>\\n\\t\\t1 - Mode using a list of protein pairs as bait <br>\\n\\t\\t2 - Mode that tries to find sentences of PPI context for any protein pairs given a list of articles\\n\\t\\n\\t- __-fo__ or __--folder__ <br>\\n\\t\\tFolder to store the files (use the folder where the other required file can be found)\\n\\n\\t- __-rtm1__ or __--running_type_mode_1__ <br>\\n\\t\\tUse to indicate which execution step you want to run for mode 1 (it is desirable following the order showed): <br>\\n\\t\\t0 (default) - Run all steps <br>\\n\\t\\t1 - Run step 1 (Get mentions of both proteins in PMC articles) <br>\\n\\t\\t2 - Run step 2 (Get the PMC or Pubmed files, clean and store them) <br>\\n\\t\\t3 - Run step 3 (Get the exact sentences where the proteins were found on interacting context)\\n\\n\\t- __-rtm2__ or __--running_type_mode_2__ <br>\\n\\t\\tUse to indicate which execution step you want to run for mode 2 (it is desirable following the order showed): <br>\\n\\t\\t0 (default) - Run all steps <br>\\n\\t\\t1 - Run step 1 (Get the PMC or Pubmed files from the given list, clean and store them) <br>\\n\\t\\t2 - Run step 2 (Get the exact sentences where the proteins were found on an interacting context)\\n\\n\\t- __-fp__ or __--file_pairs__ <br>\\n\\t\\t(For mode 1) File with the pairs (two columns with uniprot identifiers in tsv format)<br>\\n\\t\\t\\n\\t\\tExample of this file: ppipubminer/running_example/mode_1/all_pairs.tsv\\n\\n\\t- __-fe__ or __--file_evaluation__ <br>\\n\\t\\t(For mode 1) File exported after step 1 execution in tsv format<br>\\n\\n\\t- __-fa__ or __--file_articles__ <br>\\n\\t\\t(For mode 2) File with the articles (First column indicating if it is from pmc or pubmed and the second one is the article id) in tsv format)<br>\\n\\t\\t\\n\\t\\tExample of this file: ppipubminer/running_example/mode_2/articles_info.tsv\\n\\n* Running modes examples:\\n\\t- Go to the second filtering part folder: <br>\\n\\t````cd ppipubminer/````\\n\\n\\t- Mode 1 - From protein pairs (PPIs) to sentences in articles\\n\\t\\t1. Running all three steps of mode 1: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 0 -fo running_example/mode_1/ -fp all_pairs.tsv````\\n\\n\\t\\t2. Running only step 1 of mode 1: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 1 -fo running_example/mode_1/ -fp all_pairs.tsv````\\n\\n\\t\\t3. Running only step 2 of mode 1: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 2 -fo running_example/mode_1/ -fp all_pairs.tsv -fe literature_evaluation_pairs.tsv````\\n\\n\\t\\t4. Running only step 3 of mode 1: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 1 -rtm1 3 -fo running_example/mode_1/ -fp all_pairs.tsv -fe literature_evaluation_pairs.tsv````\\n\\n\\t- Mode 2 - From articles to report of sentences with any protein pairs (PPIs)\\n\\t\\t1. Running all three steps of mode 2: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 0 -fo running_example/mode_2/ -fa articles_info.tsv````\\n\\n\\t\\t2. Running only step 1 of mode 2: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 1 -fo running_example/mode_2/ -fa articles_info.tsv````\\n\\n\\t\\t3. Running only step 2 of mode 2: <br>\\n\\t\\t````python3 pubmed_pmc_literature_pipeline.py -em 2 -rtm1 2 -fo running_example/mode_2/ -fa articles_info.tsv ````\\n\\n## Reference\\nMartins YC, Ziviani A, Nicolás MF, de Vasconcelos AT. Large-Scale Protein Interactions Prediction by Multiple Evidence Analysis Associated With an In-Silico Curation Strategy. Frontiers in Bioinformatics. 2021:38.\\nhttps://www.frontiersin.org/articles/10.3389/fbinf.2021.731345/full\\n\\n## Bug Report\\nPlease, use the [Issues](https://github.com/YasCoMa/ppi_validation_process/issues) tab to report any bug.'\n",
      " \"## Summary\\n\\nThis pipeline has as major goal provide a tool for protein interactions (PPI) prediction data formalization and standardization using the [OntoPPI](https://link.springer.com/chapter/10.1007/978-3-030-36599-8_23) ontology. This pipeline is splitted in two parts: (i) a part to prepare data from three main sources of PPI data ([HINT](http://hint.yulab.org/), [STRING](https://string-db.org/) and [PredPrin](https://github.com/YasCoMa/PredPrin.git)) and create the standard files to be processed by the next part; (ii) the second part uses the data prepared before to semantically describe using ontologies related to the concepts of this domain. It describes the provenance information of PPI prediction experiments, datasets characteristics, functional annotations of proteins involved in the PPIs, description of the PPI detection methods (also named as evidence) used in the experiment,  and the prediction score obtained by each PPI detection method for the PPIs. This pipeline also execute data fusion to map the same protein pairs from different data sources and, finally, it creates a database of all these information in the [alegro](https://allegrograph.com/) graph triplestore.\\n\\n## Requirements:\\n* Python packages needed:\\n\\t- pip3 install numpy\\n\\t- pip3 install rdflib\\n\\t- pip3 install uuid\\n\\t- pip3 install SPARQLWrapper\\n\\t- alegro graph tools (pip3 install agraph-python) <br > \\n\\t\\tGo to this [site](https://franz.com/agraph/support/documentation/current/python/install.html) for the installation tutorial\\n\\n## Usage Instructions\\n### Preparation:\\n1. ````git clone https://github.com/YasCoMa/ppintegrator.git````\\n2. ````cd ppintegrator````\\n3. `pip3 install -r requirements.txt`\\n**Allegrograph is a triple store, which is a database to maintain semantic descriptions. This database's server provides a web application with a user interface to run, edit and manage queries, visualize results and manipulate the data without writing codes other than SPARQL query language. The use of the Allegregraph option is not mandatory, but if you want to export and use it, you have to install the server and the client.**\\n4. if you want to use the Allegrograph server option (this triple store has free license up to 5,000,000 triples), install allegrograph server in your machine (configure a user and password): Server - https://franz.com/agraph/support/documentation/current/server-installation.html; Client - https://franz.com/agraph/support/documentation/current/python/install.html\\n5. Export the following environment variables to configure Allegrograph server\\n\\n````\\nexport AGRAPH_HOST=127.0.0.1\\nexport AGRAPH_PORT=10035\\nexport AGRAPH_USER=chosen_user\\nexport AGRAPH_PASSWORD=chosen_password\\n````\\n5. Start allegrograph: ````path/to/allegrograph/bin/agraph-control --config path/to/allegrograph/lib/agraph.cfg start````\\n6. Read the file data_requirements.txt to understand which files are needed for the process\\n\\n### Data preparation (first part) - File ````prepare_data_triplification.py```` :\\n* Pipeline parameters:\\n\\t- __-rt__ or __--running_type__ <br>\\n\\t\\tUse to indicate from which source you want to prepare PPI data, as follows: <br>\\n\\t\\t1 - Prepare data for PredPrin <br>\\n\\t\\t2 - Prepare data for String <br>\\n\\t\\t3 - Prepare data for HINT\\n\\t- __-fec__ or __--file_experiment_config__ <br>\\n\\t\\tFile with the experiment configuration in json format<br>\\n\\t\\t\\n\\t\\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\\n\\n\\t- __-org__ or __--organism__ <br>\\n\\t\\tPrepare data only for one organism of interest (example: homo_sapiens) <br >\\n\\n\\t\\tThis parameter is optional. If you do not specify, it will automatically use the organisms described in the experiment configuration file above\\n\\n\\n* Running modes examples:\\n\\t1. Running for PPI data generated by PredPrin: <br>\\n\\t````python3 prepare_data_triplification.py -rt 1 -fec params_predrep_5k.json````\\n\\n\\t2. Running for HINT database: <br>\\n\\t````python3 prepare_data_triplification.py -rt 3 -fec params_hint.json````\\n\\n\\t3. Running for STRING database: <br>\\n\\t````python3 prepare_data_triplification.py -rt 2 -fec params_string.json````\\n\\n\\tIn the file ````auxiliar_data_preparation.py```` you can run it for all the examples provided automatically, as follows: <br>\\n\\t````python3 auxiliar_data_preparation.py````\\n\\n\\n### PPI data triplification (second part) - File ````triplification_ppi_data.py````:\\n\\n* Pipeline parameters:\\n\\t- __-rt__ or __--running_type__ <br>\\n\\t\\tUse to indicate which execution step you want to run (it is desirable following the order showed): <br>\\n\\t\\t0 - Generate the descriptions for all the protein interaction steps of an experiment  (run steps 1, 2 and 3) <br >\\n\\t\\t1 - Generate triples just about data provenance <br >\\n\\t\\t2 - Generate triples just for protein functional annotations<br >\\n\\t\\t3 - Generate triples just for the score results of each evidence<br >\\n\\t\\t4 - Execute data fusion<br >\\n\\t\\t5 - Generate descriptions and execute data fusion (run steps 1, 2, 3 and 4)<br >\\n\\t\\t6 - Export to allegrograph server\\n\\n\\t- __-fec__ or __--file_experiment_config__ <br>\\n\\t\\tFile with the experiment configuration in json format<br>\\n\\t\\t\\n\\t\\tExamples are in these files (all the metadata are required): params_hint.json, params_predrep_5k.json e params_string.json\\n\\n\\t- __-fev__ or __--file_evidence_info__ <br>\\n\\t\\tFile with the PPI detection methods information in json format<br>\\n\\t\\t\\n\\t\\tExamples are in these files (all the metadata are required): evidences_information.json, evidences_information_hint.json e evidences_information_string.json\\n\\n\\t- __-fcv__ or __--file_config_evidence__ <br>\\n\\t\\tFile with the experiment and evidence methods files addresses in tsv format<br>\\n\\t\\t\\n\\t\\tExample of this file: config_evidence_file.tsv\\n\\n* Running modes examples:\\n\\t1. Running to generate all semantic descriptions for PredPrin: <br>\\n\\t````python3 triplification_ppi_data.py -rt 0 -fec params_predrep_5k.json -fev evidences_information.json````\\n\\n\\t2. Running to generate only triples of data provenance: <br>\\n\\t````python3 triplification_ppi_data.py -rt 1 -fec params_hint.json -fev evidences_information_hint.json````\\n\\n\\t3. Running to generate only triples of PPI scores for each evidence: <br>\\n\\t````python3 triplification_ppi_data.py -rt 3 -fec params_hint.json -fev evidences_information_hint.json````\\n\\n\\t4. Running to generate only triples of protein functional annotations (only PredPrin exports these annotations): <br>\\n\\t````python3 triplification_ppi_data.py -rt 2 -fec params_predrep_5k.json -fev evidences_information.json````\\n\\n\\t5. Running to generate all semantic descrptions for STRING: <br>\\n\\t````python3 triplification_ppi_data.py -rt 0 -fec params_string.json -fev evidences_information_string.json````\\n    \\n    **For the next options (4, 5 and 6), it is mandatory running at least mode 1 and 3 for HINT, STRING and PredPrin**\\n    \\n\\t6. Running to execute data fusion of different sources: <br>\\n\\t````python3 triplification_ppi_data.py -rt 4 -fcv config_evidence_file.tsv````\\n\\n\\t7. Running to generate all semantic descriptions and execute data fusion of different sources (combines mode 0 and 4): <br>\\n\\t````python3 triplification_ppi_data.py -rt 5 -fcv config_evidence_file.tsv````\\n\\n\\t8.  Export semantic data to allegrograph server: <br>\\n\\t````python3 triplification_ppi_data.py -rt 6 -fcv config_evidence_file.tsv````\\n\\n## Query Scenarios for analysis\\nSupposing you ran all the steps showed in the section above, you can run the following options to analyse the data stored alegro graph triple store. <br>\\nFile to use for this section: ````query_analysis_ppitriplificator.py```` <br>\\n\\n* Parameter:\\n\\t- __-q__ or __--query_option__ <br>\\n\\t\\tUse to indicate which query you want to perform: <br>\\n\\t\\t1 - Get all the different organisms whose interactions are stored in the database<br >\\n\\t\\t2 - Get the interactions that have scientific papers associated and the list of these papers<br >\\n\\t\\t3 - Get a list of the most frequent biological processes annotated for the interactions of Escherichia coli bacteria<br >\\n\\t\\t4 - Get only the interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\\n\\t\\t5 - Get the scores of interactions belonging to a specific biological process (regulation of transcription, DNA-templated) in Escherichia coli bacteria<br >\\n\\t\\t6 - Get a list of the most frequent biological processes annotated for the interactions of human organism<br >\\n\\t\\t7 - Get only the interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism<br >\\n\\t\\t8 - Get the scores of interactions belonging to a specific biological process (positive regulation of transcription by RNA polymerase II) in human organism\\n\\n* Running modes examples:\\n\\t1. Running queries: <br>\\n\\t````python3 query_analysis_ppitriplificator.py -q 1 ```` <br>\\n\\t\\tChange number 1 to the respective number of the query you want to perform\\n\\n## Reference\\nMartins, Y. C., Ziviani, A., Cerqueira e Costa, M. D. O., Cavalcanti, M. C. R., Nicolás, M. F., & de Vasconcelos, A. T. R. (2023). PPIntegrator: semantic integrative system for protein–protein interaction and application for host–pathogen datasets. Bioinformatics Advances, 3(1), vbad067.\\n\\n## Bug Report\\nPlease, use the [Issues](https://github.com/YasCoMa/ppintegrator/issues) tab to report any bug.\"\n",
      " '## Summary\\n\\nThe PPI information aggregation pipeline starts getting all the datasets in [GEO](https://www.ncbi.nlm.nih.gov/geo/) database whose material was generated using expression profiling by high throughput sequencing. From each database identifiers, it extracts the supplementary files that had the counts table. Once finishing the download step, it identifies those that were normalized or had the raw counts to normalize.  It also identify and map the gene ids to uniprot (the ids found usually were from HGNC and Ensembl). For each normalized counts table belonging to some experiment, il filters those which have the proteins (already mapped from HGNC to Uniprot identifiers) in the pairs in evaluation. Then, it calculates the correlation matrix based on Pearson method in the tables and saves the respective pairs correlation value for each table. Finally, a repor is made for each pair in descending order of correlation value with the experiment identifiers.\\n\\n## Requirements:\\n* Python packages needed:\\n\\t- os\\n\\t- scipy\\n\\t- pandas\\n\\t- sklearn\\n\\t- Bio python\\n\\t- numpy\\n\\n## Usage Instructions\\n* Preparation:\\n\\t1. ````git clone https://github.com/YasCoMa/PipeAggregationInfo.git````\\n\\t2. ````cd PipeAggregationInfo````\\n\\t3. ````pip3 install -r requirements.txt````\\n\\n### Preprocessing pipeline\\n* Go to the ncbi [GDS database webpage](https://www.ncbi.nlm.nih.gov/gds), use the key words to filter your gds datasets of interest and save the results as file (\"Send to\" option), and choose \"Summary (text)\"\\n* Alternatively, we already saved the results concerning protein interactions, you may use them to run preprocessing in order to obtain the necessary files for the main pipeline\\n* Running preprocessing:\\n    - ````cd preprocessing````\\n    - ````python3 data_preprocessing.py ./workdir_preprocessing filter_files````\\n    - ````cd ../````\\n    - Copy the generated output folder \"data_matrices_count\" into the workflow folder: ````cp -R preprocessing/workdir_preprocessing/data_matrices_count .````\\n\\n### Main pipeline\\n\\n* Pipeline parameters:\\n\\t- __-rt__ or __--running_type__ <br>\\n\\t\\tUse to indicate the step you want to execute (it is desirable following the order): <br>\\n\\t\\t1 - Make the process of finding the experiments and ranking them by correlation <br>\\n\\t\\t2 - Select pairs that were already processed and ranked making a separated folder of interest\\n\\n\\t- __-fo__ or __--folder__ <br>\\n\\t\\tFolder to store the files (use the folder where the other required file can be found)\\n\\t\\n\\t- __-if__ or __--interactome_file__ <br>\\n\\t\\tFile with the pairs (two columns with uniprot identifiers in tsv format)<br>\\n\\t\\t\\n\\t\\tExample of this file: running_example/all_pairs.tsv\\n\\n\\t- __-spf__ or __--selected_pairs_file__ <br>\\n\\t\\tFile with PPIs of interest (two columns with uniprot identifiers in tsv format)<br>\\n\\t\\t\\n\\t\\tExample of this file: running_example/selected_pairs.tsv\\n\\n* Running modes examples:\\n\\t1. Run step 1: <br>\\n\\t````python3 pipeline_expression_pattern.py -rt 1 -fo running_example/ -if all_pairs.tsv ````\\n\\n\\t2. Run step 2: <br>\\n\\t````python3 pipeline_expression_pattern.py -rt 2 -fo running_example/ -spf selected_pairs.tsv ````\\n\\n## Bug Report\\nPlease, use the [Issue](https://github.com/YasCoMa/PipeAggregationInfo/issues) tab to report any bug.'\n",
      " '## Summary\\n\\nThis pipeline contains the following functions: \\n(1) Data processing to handle the tansformations needed to obtain the original pathway scores of the samples according to single sample analysis GSEA\\n(2) Model training based on the disease and healthy sample pathway scores, to classify them\\n(3) Scoring matrix weights optimization according to a gold standard list of drugs (those that went on clinical trials or are approved for the disease).It tests the weights in a range of 0 to 30 (you may change as you want). The evaluation function tests and try to maximize the number of approved drugs whose modified pathway scores for disease samples is changed from disease to healthy sample classification, according to the trained model.\\n(4) Computation of the calibrated disease samples pathwa scores according to the interaction among drug and targets found in the sample pathways & Drug ranking based on the disease samples whose calibrated matrix were responsible to change the trained model decision from disease to healthy state.\\n(5) Drug combination ranking evaluated the same way as in option (4) but adding the effects of multiple drugs in each sample while calculating the calibrated scoring matrix\\n            \\n## Input configuration file:\\n* The pipeline only needs a configuration file and the step number you want to run.\\n- Configuration file keys (see also the example in config.json):\\n    - **identifier**: project identifier to be used in the result files\\n    - **type_normalization**: normalization type (possible values: tpm, fpkm, tmm, cpm or fpkm_uq)\\n    - **genome_assembly**: the supported assemblies are the 37 and 38 (values may be: g37 or g38)\\n    - **pathway_geneset**: pathway-based gene sets, choose one identifier from the list in [genesets_available.txt](https://github.com/YasCoMa/caliscoma_pipeline/blob/master/genesets_available.txt)\\n    - **folder**: working directory\\n    - **expression_file**: compressed gene expression file for the desired icgc project, it must be separated by tabulation. The following columns are mandatory: submitted_file_id (sample names), raw_read_count (the read counts without normalization) and gene_id (genes in ensembl or hgnc symbol). File expected to be in {folder}.\\n    - **labels_file** (optional for function 1): file with two columns, one named \\'sample\\' corresponding to the unique values of submitted_sample_id; the second named \\'label\\' corresponding to a disease (or confirmed tumour) (1) or a healthy (0) case. File expected to be in {folder}.\\n    - **trained_model** (optional for function 1): file with the trained model to separate healthy and disease cases. Full path is expected.\\n    - **means_table_file** (optional for function 1): file with the means table calculated when the model is trained by the function 3. Full path is expected.\\n    - **samples_pathway_scores** (optional for function 1): file with the original model calculated pathway scores by function 1, in order to check the number of features expected by the original model. Full path is expected.\\n    - **optimized_weights_file**: tab separated table file with two columns representing the weights (w1, w2, w3) and their respective values.\\n    - **drug_list_file** (only mandatory for function 3): file with the gold standard drug list (one drugbank id per line), this file is expected to be in the in the experiment item folder results ({folder}/{identifier})\\n    - **drug_combination_file** (only mandatory for function 5): file with the drug combination candidates list (drugbank ids concatenated with comma in each line). Full path is expected.\\n\\n- Observation:    \\n    * The \"labels_file\" parameter is mandatory for the weights optimization, scoring matrix calculation, model traning and drug (or drug combination) ranking \\n    * In case of transfer learning, \"labels_file\" may be ignored only if both \"trained_model\", \"means_table_file\" and \"samples_pathway_scores\" are present. This is only possible for the functions 2, 4 and 5. For weights optimization, only labels file is accepted.\\n    * If type_normalization and/or genome_assembly are missing or empty, it will switch to the default fpkm_uq\\n    * If pathway_geneset is missing or empty, it will switch to the default KEGG_2021_HUMAN\\n    * If optimized_weights_file is missing or empty, it will switch to the default values (w1: 20, w2: 5, w3: 10)\\n    \\n## Usage Instructions\\n### Preparation:\\n1. ````git clone https://github.com/YasCoMa/caliscoma_pipeline.git````\\n2. ````cd caliscoma_pipeline````\\n3. Create conda environment to handle dependencies: ````conda env create -f drugresponse_env.yml````\\n4. ````conda activate drugresponse_env````\\n5. Setup an environment variable named \"path_workflow\" with the full path to this workflow folder\\n\\n### Getting data for the running example in the LICA-FR and LIRI-JP projects from ICGC\\n1. Download the [expression file for LICA-FR](https://dcc.icgc.org/api/v1/download?fn=/current/Projects/LICA-FR/exp_seq.LICA-FR.tsv.gz) and put it in data_icgc folder\\n2. Download the [expression file for LIRI-JP](https://dcc.icgc.org/api/v1/download?fn=/current/Projects/LIRI-JP/exp_seq.LIRI-JP.tsv.gz) and put it in data_icgc folder\\n3. For the liri-jp project, the labels file is already processed, to given an example of a project that run all steps proposed by this workflow\\n\\n### Run analysis\\n- Run all steps: ````python3 main.py -rt 0 -cf config.json````\\n- Run all steps: ````python3 main.py -rt 0 -cf config_transfer_options.json````\\n\\n- Run only data processing: ````python3 main.py -rt 1 -cf config.json````\\n- Run only data processing: ````python3 main.py -rt 1 -cf config_transfer_options.json````\\n\\n- Run only model training & modified pathway score matrix: ````python3 main.py -rt 2 -cf config.json````\\n- Run only model training & modified pathway score matrix: ````python3 main.py -rt 2 -cf config_transfer_options.json````\\n\\n- Run only weights optimization: ````python3 main.py -rt 3 -cf config.json````\\n\\n- Run only drug ranking: ````python3 main.py -rt 4 -cf config.json````\\n- Run only drug ranking: ````python3 main.py -rt 4 -cf config_transfer_options.json````\\n\\n- Run only drug combination evaluation: ````python3 main.py -rt 5 -cf config.json````\\n- Run only drug combination evaluation: ````python3 main.py -rt 5 -cf config_transfer_options.json````\\n\\n## Reference\\nMartins, Y. C. (2023). Multi-task analysis of gene expression data on cancer public datasets. medRxiv, 2023-09.\\n\\n## Bug Report\\nPlease, use the [Issues](https://github.com/YasCoMa/caliscoma_pipeline/issues) tab to report any bug.'\n",
      " '## Summary\\n\\nThe data preparation pipeline contains tasks for two distinct scenarios: [leukaemia](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE425) that contains microarray data for 119 patients and [ovarian](https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE140082) cancer that contains next generation sequencing data for 380 patients.\\n\\nThe disease outcome prediction pipeline offers two strategies for this task:\\n\\n**Graph kernel method**: It starts generating personalized networks for each patient using the interactome file provided and generate the patient network checking if each PPI of the interactome has both proteins up regulated or down regulated according to the gene expression table provided. The first step generate a set of graphs for the patients that are evaluated with 4 distinct kernels for graph classification, which are: Linear kernel between edge histograms, Linear kernel between vertex histograms and the Weisfeiler lehman. These kernels functions calculate a similarity matrix for the graphs and then this matrix is used by the support vector machine classifier. Then the predictions are delivered to the last task that exports a report with the accuracy reached by each kernel. It allows some customizations about the network parameters to be used, such as the DEG cutoff to determine up and down regulated based on the log2 fold change, which will determine the topology and the labels distribution in the specific sample graphs. It is also possible customize the type of node/edge attributes passed to the kernel function, which may be only label, only weight or both.\\n\\n**GSEA based pathway scores method**: This method is faster and do not rely on tensor inputs such as the previous method. It uses geneset enrichment analysis on the pathways from KEGG 2021 of Human, and uses the scores of the pathways found enriched for the samples to build the numerical features matrix, that is then delivered to the AdaBoost classifier. The user may choose balance the dataset using oversampling strategy provided by SMOTE.\\n\\n## Usage Instructions\\n### Preparation:\\n1. ````git clone https://github.com/YasCoMa/screendop.git````\\n2. ````cd screendop````\\n3. Decompress screening_ovarian/raw_expression_table.tsv.tar.xz\\n4. Create conda environment to handle dependencies: ````conda env create -f drugresponse_env.yml````\\n5. ````conda activate drugresponse_env````\\n6. Setup an environment variable named \"path_workflow_screendop\" with the full path to this workflow folder\\n\\n### Data preparation - File ````data_preparation_for_pipeline.py```` :\\n\\n#### Files decompression\\n\\n- Decompress data_preparation/lekaemia.tar.xz\\n- Decompress data_preparation/ovarian/GSE140082_data.tar.xz\\n    - Put the decompressed file GSE140082_series_matrix.txt in data_preparation/ovarian/\\n    \\n#### Pipeline parameters\\n\\n- __-rt__ or __--running_type__ <br>\\n\\tUse to prepare data for the desired scenario: <br>\\n\\t1 - Run with Leukaemia data <br>\\n\\t2 - Run with Ovarian cancer data\\n\\n#### Running modes examples\\n\\n1. Run for Leukaemia data: <br>\\n````python3 data_preparation_for_pipeline.py -rt 1 ```` \\n\\nIn this case, you must have [R](https://www.r-project.org/) installed and also the library [limma](https://bioconductor.org/packages/release/bioc/html/limma.html), it is used to determine DEGs from microarray data. For this dataset, the files are already prepared in the folder.\\n\\n2. Run for Ovarian cancer data: <br>\\n````python3 data_preparation_for_pipeline.py -rt 2 ```` \\n\\nIn this case, you must have [R](https://www.r-project.org/) installed and also the library [DESeq](https://bioconductor.org/packages/release/bioc/html/DESeq.html), because this scenario treats next generation sequencing data\\n\\n### Disease outcome prediction execution - File ````main.py````:\\n\\n#### Pipeline parameters\\n\\n- __-rt__ or __--running_step__ <br>\\n\\tUse to prepare data for the desired scenario: <br>\\n\\t1 - Run graph kernel method <br>\\n\\t2 - Run gsea based pathway scores method\\n\\n- __-cf__ or __--configuration_file__ <br>\\n\\tFile with the expression values for the genes by sample/patient in tsv format<br>\\n\\t\\n\\tExample of this file: config.json\\n\\t\\t\\n#### Input configuration file\\n\\n- Configuration file keys (see also the example in config.json):\\n    - **folder** (mandatory for both methods): working directory\\n    - **identifier**: project identifier to be used in the result files\\n    - **mask_expression_table** (mandatory for both methods): Gene expression values file with the result of the fold change normalized value of a certain gene for each sample, already pruned by the significance (p-value). \\n    - **raw_expression_table** (mandatory for both methods): Raw gene expression values already normalized following the method pf preference of the user.\\n    - **labels_file** (mandatory for both methods): File with the prognosis label for each sample\\n    - **deg_cutoff_up**: Cutoff value to determine up regulated gene. Default value is 1.\\n    - **deg_cutoff_down**: Cutoff value to determine down regulated gene. Default value is -1.\\n    - **nodes_enrichment**: Node attributes to be used in the screening evaluation. It may be a list combining the options \"label\", \"weight\" or \"all\". Examples: [\"all\", \"weight\"], [\"label\"], [\"label\", \"weight\"]. Default value is [\"all\"].\\n    - **edges_enrichment**: Edge attributes to be used in the screening evaluation. It may be a list combining the options \"label\", \"weight\" or \"all\". Examples: [\"all\", \"weight\"], [\"label\"], [\"label\", \"weight\"]. Default value is [\"all\"].\\n    - **flag_balance**: Flag to indicate whether the user wants to balance the samples in each outcome class, by SMOTE oversampling. Values may be false or true. Default value is false.\\n\\n#### Running modes examples\\n1. Running disease outcome prediction by graph kernel method: <br>\\n\\t````python3 main.py -rt 1 -cf config.json````\\n\\n2. Running disease outcome prediction by gsea enriched network method: <br>\\n\\t````python3 main.py -rt 2 -cf config.json````\\n\\n## Reference\\nMartins, Y. C. (2023). Multi-task analysis of gene expression data on cancer public datasets. medRxiv, 2023-09.\\n\\n## Bug Report\\nPlease, use the [Issue](https://github.com/YasCoMa/screendop/issues) tab to report any bug.'\n",
      " 'This workflow takes as input a SRA_manifest from SRA Run Selector and will generate one fastq file or fastq pair of file for each experiment (concatenated multiple runs if necessary). Output will be relabelled to match the column specified by the user.'\n",
      " \"The aim of this workflow is to handle the routine part of shotgun metagenomics data processing on Galaxy Australia. \\n\\nThe workflow is using the tools MetaPhlAn2 for taxonomy classification and HUMAnN2 for functional profiling of the metagenomes. The workflow is based on the Galaxy Training tutorial 'Analyses of metagenomics data - The global picture' (Saskia Hiltemann, Bérénice Batut) https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/general-tutorial/tutorial.html#shotgun-metagenomics-data. \\n\\nThe how-to guide is available here: https://vmurigneu.github.io/shotgun_howto_ga_workflows/\\n\"\n",
      " 'Scaffolding using HiC data with YAHS.'\n",
      " '# MMV Im2Im Transformation\\n\\n[![Build Status](https://github.com/MMV-Lab/mmv_im2im/workflows/Build%20Main/badge.svg)](https://github.com/MMV-Lab/mmv_im2im/actions)\\n\\nA generic python package for deep learning based image-to-image transformation in biomedical applications\\n\\nThe main branch will be further developed in order to be able to use the latest state of the art techniques and methods in the future. To reproduce the results of our manuscript, we refer to the branch [paper_version](https://github.com/MMV-Lab/mmv_im2im/tree/paper_version).\\n(We are actively working on the documentation and tutorials. Submit a feature request if there is anything you need.)\\n\\n---\\n\\n## Overview\\n\\nThe overall package is designed with a generic image-to-image transformation framework, which could be directly used for semantic segmentation, instance segmentation, image restoration, image generation, labelfree prediction, staining transformation, etc.. The implementation takes advantage of the state-of-the-art ML engineering techniques for users to focus on researches without worrying about the engineering details. In our pre-print [arxiv link](https://arxiv.org/abs/2209.02498), we demonstrated the effectiveness of *MMV_Im2Im* in more than ten different biomedical problems/datasets. \\n\\n* For computational biomedical researchers (e.g., AI algorithm development or bioimage analysis workflow development), we hope this package could serve as the starting point for their specific problems, since the image-to-image \"boilerplates\" can be easily extended further development or adapted for users\\' specific problems.\\n* For experimental biomedical researchers, we hope this work provides a comprehensive view of the image-to-image transformation concept through diversified examples and use cases, so that deep learning based image-to-image transformation could be integrated into the assay development process and permit new biomedical studies that can hardly be done only with traditional experimental methods\\n\\n\\n## Installation\\n\\nBefore starting, we recommend to [create a new conda environment](https://docs.conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html#creating-an-environment-with-commands) or [a virtual environment](https://docs.python.org/3/library/venv.html) with Python 3.9+.\\n\\nPlease note that the proper setup of hardware is beyond the scope of this pacakge. This package was tested with GPU/CPU on Linux/Windows and CPU on MacOS. [Special note for MacOS users: Directly pip install in MacOS may need [additional setup of xcode](https://developer.apple.com/forums/thread/673827).]\\n\\n### Install MONAI\\n\\nTo reproduce our results, we need to install MONAI\\'s code version of a specific commit. To do this:\\n```\\ngit clone https://github.com/Project-MONAI/MONAI.git\\ncd ./MONAI\\ngit checkout 37b58fcec48f3ec1f84d7cabe9c7ad08a93882c0\\npip install .\\n```\\n\\nWe will remove this step for the main branch in the future to ensure a simplified installation of our tool.\\n\\n### Install MMV_Im2Im for basic usage:\\n\\n(For users only using this package, not planning to change any code or make any extension):\\n\\n**Option 1: core functionality only** `pip install mmv_im2im`<br>\\n**Option 2: advanced functionality (core + logger)** `pip install mmv_im2im[advance]`<br>\\n**Option 3: to reproduce paper:** `pip install mmv_im2im[paper]`<br>\\n**Option 4: install everything:** `pip install mmv_im2im[all]`<br>\\n\\nFor MacOS users, additional \\' \\' marks are need when using installation tags in zsh. For example, `pip install mmv_im2im[paper]` should be `pip install mmv_im2im\\'[paper]\\'` in MacOS.\\n\\n### Install MMV_Im2Im for customization or extension:\\n\\n\\n```\\ngit clone https://github.com/MMV-Lab/mmv_im2im.git\\ncd mmv_im2im\\npip install -e .[all]\\n```\\n\\nNote: The `-e` option is the so-called \"editable\" mode. This will allow code changes taking effect immediately. The installation tags, `advance`, `paper`, `all`, are be selected based on your needs.\\n\\n### (Optional) Install using Docker\\n\\nIt is also possible to use our package through [docker](https://www.docker.com/). The installation tutorial is [here](docker/tutorial.md).\\n\\n### (Optional) Use MMV_Im2Im with Google Colab\\n\\nWe provide a web-based demo, if cloud computing is preferred. you can [![Open a 2D labelfree DEMO in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MMV-Lab/mmv_im2im/blob/main/tutorials/colab/labelfree_2d.ipynb). The same demo can de adapted for different applications.\\n\\n## Quick start\\n\\nYou can try out on a simple example following [the quick start guide](tutorials/quick_start.md)\\n\\nBasically, you can specify your training configuration in a yaml file and run training with `run_im2im --config /path/to/train_config.yaml`. Then, you can specify the inference configuration in another yaml file and run inference with `run_im2im --config /path/to/inference_config.yaml`. You can also run the inference as a function with the provided API. This will be useful if you want to run the inference within another python script or workflow.  Here is an example:\\n\\n```\\nfrom pathlib import Path\\nfrom aicsimageio import AICSImage\\nfrom aicsimageio.writers import OmeTiffWriter\\nfrom mmv_im2im.configs.config_base import ProgramConfig, parse_adaptor, configuration_validation\\nfrom mmv_im2im import ProjectTester\\n\\n# load the inference configuration\\ncfg = parse_adaptor(config_class=ProgramConfig, config=\"./paper_configs/semantic_seg_2d_inference.yaml\")\\ncfg = configuration_validation(cfg)\\n\\n# define the executor for inference\\nexecutor = ProjectTester(cfg)\\nexecutor.setup_model()\\nexecutor.setup_data_processing()\\n\\n# get the data, run inference, and save the result\\nfn = Path(\"./data/img_00_IM.tiff\")\\nimg = AICSImage(fn).get_image_data(\"YX\", Z=0, C=0, T=0)\\n# or using delayed loading if the data is large\\n# img = AICSImage(fn).get_image_dask_data(\"YX\", Z=0, C=0, T=0)\\nseg = executor.process_one_image(img)\\nOmeTiffWriter.save(seg, \"output.tiff\", dim_orders=\"YX\")\\n```\\n\\n\\n## Tutorials, examples, demonstrations and documentations\\n\\nThe overall package aims to achieve both simplicty and flexibilty with the modularized image-to-image boilerplates. To help different users to best use this package, we provide documentations from four different aspects:\\n\\n* [Examples (i.e., scripts and config files)](tutorials/example_by_use_case.md) for reproducing all the experiments in our [pre-print](https://arxiv.org/abs/2209.02498)\\n* A bottom-up tutorials on [how to understand the modularized image-to-image boilerplates](tutorials/how_to_understand_boilerplates.md) (for extending or adapting the package) and [how to understand the configuration system in details](tutorials/how_to_understand_config.md) (for advance usage to make specific customization).\\n* A top-down tutorials as [FAQ](tutorials/FAQ.md), which will continuously grow as we receive more questions.\\n* All the models used in the manuscript and sample data can be found here: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10034416.svg)](https://doi.org/10.5281/zenodo.10034416)\\n\\n\\n### Contribute models to [BioImage Model Zoo](https://bioimage.io/#/)\\n\\nWe highly appreciate the BioImage Model Zoo\\'s initiative to provide a comprehensive collection of pre-trained models for a wide range of applications. To make MMV_Im2Im trained models available as well, the first step involves extracting the state_dict from the PyTorch Lightning checkpoint.\\nThis can be done via:\\n\\n```python\\nimport torch\\n\\nckpt_path = \"./lightning_logs/version_0/checkpoints/last.ckpt\"\\ncheckpoint = torch.load(ckpt_path, map_location=torch.device(\\'cpu\\'))\\nstate_dict = checkpoint[\\'state_dict\\']\\ntorch.save(state_dict, \"./state_dict.pt\")\\n```\\n\\nAll further steps to provide models can be found in the [official documentation](https://bioimage.io/docs/#/contribute_models/README).\\n\\n## Development\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for information related to developing the code.\\n\\n\\n**MIT license**\\n'\n",
      " '**Name:** Matrix multiplication with Objects  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/matmul_objects.py numberOfBlocks blockSize\\n```\\n\\nwhere:\\n* numberOfBlocks: Number of blocks inside each matrix\\n* blockSize: Size of each block\\n\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/matmul_objects.py 16 4\\nruncompss src/matmul_objects.py 16 4\\npython -m pycompss src/matmul_objects.py 16 4\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " 'Call somatic, germline and LoH event variants from PE Illumina sequencing data obtained from matched pairs of tumor and normal tissue samples.\\n\\nThis workflow can be used with whole-genome and whole-exome sequencing data as input. For WES data, parts of the analysis can be restricted to the exome capture kits target regions by providing the optional \"Regions of Interest\" bed dataset.\\n\\nThe current version uses bwa-mem for read mapping and varscan somatic for variant calling and somatic status classification.'\n",
      " 'A variation of the Cancer variant annotation (hg38 VEP-based) workflow at https://doi.org/10.48546/workflowhub.workflow.607.1.\\n\\nLike that other workflow it takes a list of tumor/normal sample pair variants in VCF format (see the other workflow for details about the expected format) and\\n\\n1. annotates them using the ENSEMBL Variant Effect Predictor and custom annotation data\\n2. turns the annotated VCF into a MAF file for import into cBioPortal\\n3. generates human-readable variant- and gene-centric reports\\n\\nIn addition, this worklfow exports the resulting MAF dataset to a WebDAV-enabled remote folder for subsequent import into cBioPortal.\\nWebDAV access details can be configured in the Galaxy user preferences.'\n",
      " '# VGP Workflow #1\\n\\nThis workflow produces a Meryl database and Genomescope outputs that will be used to determine parameters for following workflows, and assess the quality of genome assemblies. Specifically, it provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality.\\n\\n### Inputs\\n\\n-   A collection of Hifi long reads in FASTQ format\\n-   *k*-mer length\\n-   Ploidy\\n\\n### Outputs\\n\\n-   Meryl Database of kmer counts\\n-   GenomeScope\\n    -   Linear plot\\n    -   Log plot\\n    -   Transformed linear plot\\n    -   Transformed log plot\\n    -   Summary\\n    -   Model\\n    -   Model parameteres\\n\\n ![image](https://github.com/galaxyproject/iwc/assets/4291636/565238fc-f8a9-46ac-8b31-6276410fa436)\\n'\n",
      " 'Purge contigs marked as duplicates by purge_dups (could be haplotypic duplication or overlap duplication). This workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5)'\n",
      " '**Name:** Word Count  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nWordcount is an application that counts the number of words for a given set of files.\\n\\nTo allow parallelism every file is treated separately and merged afterwards.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/wordcount.py datasetPath\\n```\\n\\nwhere:\\n* datasetPath: Absolute path of the file to parse (e.g. /home/compss/tutorial_apps/python/wordcount/data/)\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/wordcount.py $(pwd)/data/\\nruncompss src/wordcount.py $(pwd)/data/\\npython -m pycompss src/wordcount.py $(pwd)/data/\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " '# ![sanger-tol/insdcdownload](docs/images/sanger-tol-insdcdownload_logo.png)\\n\\n[![GitHub Actions CI Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+CI%22)\\n\\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/insdcdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/insdcdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\\n\\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7155119-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7155119)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n\\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\\n\\n## Introduction\\n\\n**sanger-tol/insdcdownload** is a pipeline that downloads assemblies from INSDC into a Tree of Life directory structure.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\\n\\n## Pipeline summary\\n\\n## Overview\\n\\nThe pipeline takes an assembly accession number, as well as the assembly name, and downloads it. It also builds a set of common indices (such as `samtools faidx`), and extracts the repeat-masking performed by the NCBI.\\n\\nSteps involved:\\n\\n- Download from the NCBI the genomic sequence (Fasta) and the assembly\\n  stats and reports files.\\n- Turn the masked Fasta file into an unmasked one.\\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\\n  `samtools dict`.\\n- Generate the `.sizes` file usually required for conversion of data\\n  files to UCSC\\'s \"big\" formats, e.g. bigBed.\\n- Extract the coordinates of the masked regions into a BED file.\\n- Compress and index the BED file with `bgzip` and `tabix`.\\n\\n## Quick Start\\n\\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\\n\\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n   ```bash\\n   nextflow run sanger-tol/insdcdownload -profile test,YOURPROFILE --outdir <OUTDIR>\\n   ```\\n\\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\\n\\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\\n\\n4. Start running your own analysis!\\n\\n   ```console\\n   nextflow run sanger-tol/insdcdownload --assembly_accession GCA_927399515.1 --assembly_name gfLaeSulp1.1 --outdir results\\n   ```\\n\\n## Documentation\\n\\nThe sanger-tol/insdcdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\\n\\n## Credits\\n\\nsanger-tol/insdcdownload was mainly written by @muffato, with major borrowings from @priyanka-surana\\'s [read-mapping](https://github.com/sanger-tol/readmapping) pipeline, e.g. the script to remove the repeat-masking, and the overall structure and layout of the sub-workflows.\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don\\'t hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/insdcdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\\n\\n## Citations\\n\\nIf you use sanger-tol/insdcdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7155119](https://doi.org/10.5281/zenodo.7155119)\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n'\n",
      " '**Name:** TruncatedSVD (Randomized SVD)  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum4  \\n\\nTruncatedSVD (Randomized SVD) for computing just 456 singular values out of a (3.6M x 1200) size matrix.  \\nThe input matrix represents a CFD transient simulation of aire moving past a cylinder.  \\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " 'Autosubmit mHM test domains'\n",
      " '# Contiging Solo w/HiC:\\n\\nGenerate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for phasing.\\n\\n## Inputs\\n\\n1. Hifi long reads [fastq]\\n2. HiC forward reads (if multiple input files, concatenated in same order as reverse reads) [fastq]\\n3. HiC reverse reads (if multiple input files, concatenated in same order as forward reads) [fastq]\\n4. K-mer database [meryldb]\\n5. Genome profile summary generated by Genomescope [txt]\\n6. Name of first assembly\\n7. Name of second assembly\\n\\n## Outputs\\n\\n1. Haplotype 1 assembly ([fasta] and [gfa])\\n2. Haplotype 2 assembly ([fasta] and [gfa])\\n3. QC: BUSCO report for both assemblies\\n4. QC: Merqury report for both assemblies\\n5. QC: Assembly statistics for both assemblies\\n6. QC: Nx plot for both assemblies\\n7. QC: Size plot for both assemblies'\n",
      " '# Assembly with Hifi reads and Trio Data\\n\\nGenerate phased assembly based on PacBio Hifi Reads using parental Illumina data for phasing\\n\\n## Inputs\\n\\n1. Hifi long reads [fastq]\\n2. Concatenated Illumina reads : Paternal [fastq]\\n3. Concatenated Illumina reads : Maternal [fastq]\\n4. K-mer database [meryldb]\\n5. Paternal hapmer database [meryldb]\\n6. Maternal hapmer database [meryldb]\\n7. Genome profile summary generated by Genomescope [txt]\\n8. Genome model parameters generated by Genomescope [tabular]\\n9. Homozygous read coverage (Estimated from the Genomescope model if not provided)\\n10. Lineage of the species being assembled\\n11. Bloom Filter\\n12. Name of first haplotype\\n13. Name of second haplotype\\n\\n## Outputs\\n\\n1. Haplotype 1 assembly\\n2. Haplotype 2 assembly\\n3. QC: BUSCO report for both assemblies\\n4. Merqury report for both assemblies\\n5. Assembly statistics for both assemblies\\n6. Nx Plot for both assemblies\\n7. Size plot for both assemblies\\n\\n\\n'\n",
      " '# Scaffolding with Bionano\\n\\nScaffolding using Bionano optical map data\\n\\n## Inputs\\n\\n1. Bionano data [cmap]\\n2. Estimated genome size [txt]\\n3. Phased assembly generated by Hifiasm [gfa1]\\n\\n## Outputs\\n\\n1. Scaffolds\\n2. Non-scaffolded contigs\\n3. QC: Assembly statistics\\n4. QC: Nx plot\\n5. QC: Size plot'\n",
      " 'Takes fastqs and reference data, to produce a single cell counts matrix into and save in annData format - adding a column called sample with the sample name.  \\n'\n",
      " 'From the R1 and R2 fastq files of a single samples, make a scRNAseq counts matrix, and perform basic QC with scanpy. Then, do further processing by making a UMAP and clustering. Produces a processed AnnData \\nDepreciated: use individual workflows insead for multiple samples'\n",
      " \"16S Microbial Analysis with mothur (short)\\n\\nThe workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, Bérénice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \\n\\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\\n\"\n",
      " \"The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, Bérénice Batut, Dave Clements), adapted for piepline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \\n\\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\\n\"\n",
      " \"The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, Bérénice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \\n\\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\\n\"\n",
      " \"The workflows in this collection are from the '16S Microbial Analysis with mothur' tutorial for analysis of 16S data (Saskia Hiltemann, Bérénice Batut, Dave Clements), adapted for pipeline use on galaxy australia (Ahmed Mehdi). The workflows developed in galaxy use mothur software package developed by Schloss et al https://pubmed.ncbi.nlm.nih.gov/19801464/. \\n\\nPlease also refer to the 16S tutorials available at Galaxy https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop-short/tutorial.html and [https://training.galaxyproject.org/training-material/topics/metagenomics/tutorials/mothur-miseq-sop/tutorial.html\\n\\n\"\n",
      " '# ANNOTATO - Annotation workflow To Annotate Them Oll\\n\\n- [ANNOTATO - Annotation workflow To Annotate Them Oll](#annotato---annotation-workflow-to-annotate-them-oll)\\n  - [Overview of the workflow](#overview-of-the-workflow)\\n    - [Input data](#input-data)\\n    - [Pipeline steps](#pipeline-steps)\\n    - [Output data](#output-data)\\n  - [Prerequisites](#prerequisites)\\n  - [Installation](#installation)\\n  - [Running ANNOTATO](#running-annotato)\\n    - [Before running the pipeline (IMPORTANT)](#before-running-the-pipeline-important)\\n    - [Without RNASeq and protein data](#without-rnaseq-and-protein-data)\\n    - [Running ANNOTATO with RNASeq data](#running-annotato-with-rnaseq-data)\\n    - [Running ANNOTATO with protein data](#running-annotato-with-protein-data)\\n    - [Running ANNOTATO with both protein and RNASeq data](#running-annotato-with-both-protein-and-rnaseq-data)\\n    - [Running ANNOTATO with params.json](#running-annotato-with-paramsjson)\\n    - [Other parameters for running the analysis](#other-parameters-for-running-the-analysis)\\n  - [Evaluating output GFF to the exon level](#evaluating-output-gff-to-the-exon-level)\\n  - [Performance of the workflow on annotating difference eukaryote genomes](#performance-of-the-workflow-on-annotating-difference-eukaryote-genomes)\\n  - [Future work](#future-work)\\n\\n## Overview of the workflow\\n\\nThe pipeline is based on `Funannotate` or `BRAKER` and was initially developed and tested on the two datasets:\\n- Drosophila melanogaster: [https://doi.org/10.5281/zenodo.8013373](https://doi.org/10.5281/zenodo.8013373)\\n- *Pocillopora* cf. *effusa*: [https://www.ncbi.nlm.nih.gov/biosample/26809107](https://www.ncbi.nlm.nih.gov/biosample/26809107)\\n\\nThen, it was further tested on these species during the [BioHackathon 2023 - project 20](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20)\\n\\n- Helleia helle\\n- Homo sapiens chrom 19\\n- Melampus jaumei\\n- Phakellia ventilabrum\\n- Trifolium dubium\\n\\n### Input data\\n\\n- Reference genome `genome.[.fna, .fa, .fasta][.gz]`\\n- RNAseq data listed in a metadata csv file. Input type can be mixed between long and short reads, with the option of single-end read. The input file should follow the format below:\\n\\n```\\nsample_id,R1_path,R2_path,read_type\\nSAM1,/path/to/R1,,long             # For long reads\\nSAM2,/path/to/R1,/path/to/R2,short # For PE reads\\nSAM3,/path/to/R1,,short            # For SE reads\\n```\\n\\n- Protein sequence data in fasta format, could be gzip or not\\n\\n### Pipeline steps\\n\\n![Pipeline](./assets/images/annotato-workflow.drawio.svg)\\n\\nThe main pipeline is divided into five different subworkflows.\\n- `Preprocess RNA` is where the input RNASeq data are QC and trimmed.\\n- `Process RNA Minimap` is triggered when long reads FastQ are in the input CSV file.\\n- `Process RNA STAR` will run when short reads FastQ are in the input CSV.\\n- `Genome Masking` runs by default if not skipped. It assumes the input genome fasta is not masked and will run Denovo repeat masking with RepeatModeler and RepeatMasker.\\n- `Filter Repeat` whenever there is a Denovo masking step, this sub-workflow will be triggered to remove the repeat sequences that appeared in the Uniprot Swissprot protein data. \\n\\n### Output data\\n\\n- MultiQC report for the RNASeq data, before and after trimming, mapping rate of short reads, and the BUSCO results of predicted genes.\\n- RepeatMasker report containing quantity of masked sequence and distribution among TE families\\n- Protein-coding gene annotation file in gff3 format\\n- BUSCO summary of annotated sequences\\n\\n## Prerequisites\\n\\nThe following programs are required to run the workflow and the listed version were tested. \\n\\n`nextflow v23.04.0 or higher`\\n\\n`singularity`\\n\\n`conda` and `mamba` (currently, having problem with Funannotate and BRAKER installation)\\n\\n`docker` (have not been tested but in theory should work fine)\\n\\n## Installation\\n\\nSimply get the code from github or workflowhub and directly use it for the analysis with `nextflow`.\\n\\n```\\ngit clone https://github.com/ERGA-consortium/pipelines/tree/main/annotation/nextflow\\n```\\n\\n## Running ANNOTATO\\n\\n### Before running the pipeline (IMPORTANT)\\n\\nOne thing with Nextflow is that it is running off a Java Virtual Machine (JVM), and it will try to use all available memory for Nextflow even though it is unnecessary (for workflow management and job control). This will cause much trouble if you run a job on an HPC cluster. Thus, to minimize the effect of it, we need to limit the maximum memory the JVM can use.\\n\\n```\\nexport NFX_OPTS=\"-Xms=512m -Xmx=3g\"\\n```\\n\\n`-Xms` is the lower limit, which is set as 512 MB.\\n`-Xmx` is the upper limit, which in this case is set as 3 GB.\\nPlease modify this according to your situation.\\n\\n### Without RNASeq and protein data\\n\\nPerform the analysis with only the draft genome and busco database.\\n\\n```\\nnextflow run main.nf --genome /path/to/genome.fasta --species \"Abc def\" --buscodb \\'metazoa\\' \\n```\\n\\nThe workflow will run Denovo repeat masking on the draft genome, then softmask the repeat region and use the genome to run `funannotate`. Add `--run_braker` to run the genome prediction using `BRAKER` instead.\\n\\n### Running ANNOTATO with RNASeq data\\n\\nWhen you want to let the workflow run the mapping by itself, uses `input.csv` as input with the link to all `FASTQ` file.\\n\\n```\\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb \\'metazoa\\' \\n```\\n\\nBased on the content of the `input.csv` file to trigger different RNASeq processing workflows. The output `bam` file will then be used for genome prediction.\\n\\nWhen reads are mapped to the reference genome, the aligned `bam` file can be used as input to the pipeline instead of the raw `FASTQ`\\n\\n```\\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --short_rna_bam /path/to/shortreads.bam [--long_rna_bam /path/to/longreads.bam] --species \"Abc def\" --buscodb \\'metazoa\\' \\n```\\n\\n**ATTENTION**: One major drawback of the current workflow is that the input genome will be sorted and renamed by the `funannotate sort` function. This is because `AUGUSTUS` and `Funannotate` won\\'t work normally when the header of the input genome is too long and contains weird characters. Therefore, if you want to provide a `bam` file as input instead of the raw `FASTQ`, please run `funannotate sort` on the genome fasta first and then use it as the reference for running alignment. Or in case your genome headers are already shorter than 16 character, please add `--skip_rename` when running the pipeline.\\n\\n### Running ANNOTATO with protein data\\n\\n```\\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --species \"Abc def\" --buscodb \\'metazoa\\' \\n```\\n\\nWhen only protein data is provided, the workflow will run denovo masking then repeat filter with the additional protein data. The masked genome and protein fasta will then be used for gene prediction.\\n\\n### Running ANNOTATO with both protein and RNASeq data\\n\\nThe full pipeline is triggered when both RNASeq data and protein fasta is provided.\\n\\n```\\nnextflow run main.nf --genome /path/to/genome.fasta[.gz] --protein /path/to/protein.fasta[.gz] --rnaseq /path/to/input.csv --species \"Abc def\" --buscodb \\'metazoa\\' \\n```\\n\\n### Running ANNOTATO with params.json\\n\\nOne plus side with Nextflow is that it can use a parameter JSON file called `params.json` to start the analysis pipeline with all required parameters. Please modify the content of the `params.json` according to your need then run the following command.\\n\\n```\\nnextflow run main.nf -params-file params.json\\n```\\n\\n### Other parameters for running the analysis\\n\\n```\\nCompulsory input:\\n--genome                       Draft genome fasta file contain the assembled contigs/scaffolds\\n--species                      Species name for the annotation pipeline, e.g. \"Drosophila melanogaster\"\\n\\nOptional input:\\n--protein                      Fasta file containing known protein sequences used as an additional information for gene prediction pipeline.\\n                               Ideally this should come from the same species and/or closely related species. [default: null]\\n--rnaseq                       A CSV file following the pattern: sample_id,R1_path,R2_path,read_type.\\n                               This could be generated using gen_input.py. Run `python gen_input.py --help` for more information. \\n                               [default: null]\\n--long_rna_bam                 A BAM file for the alignment of long reads (if any) to the draft genome. Noted that the header of the draft\\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \\n                               [default: null]\\n--short_rna_bam                A BAM file for the alignment of short reads (if any) to the draft genome. Noted that the header of the draft \\n                               genome need to be renamed first before alignment otherwise it will causes trouble for AUGUSTUS and funannotate. \\n                               [default: null]\\n--knownrepeat                  Fasta file containing known repeat sequences of the species, this will be used directly for masking \\n                               (if --skip_denovo_masking) or in combination with the denovo masking. [default: null]\\n\\nOutput option:\\n--outdir                       Output directory. \\n--tracedir                     Pipeline information. \\n--publish_dir_mode             Option for nextflow to move data to the output directory. [default: copy]\\n--tmpdir                       Database directory. \\n\\nFunannotate params:\\n--run_funannotate              Whether to use funannotate for gene prediction. [default: true]\\n--organism                     Fungal-specific option. Should be change to \"fungus\" if the annotated organism is fungal. [default: other]\\n--ploidy                       Set the ploidy for gene prediction, in case of haploid, a cleaning step will be performed by funannotate to remove\\n                               duplicated contigs/scaffold. [default: 2]\\n--buscodb                      BUSCO database used for AUGUSTUS training and evaluation. [default: eukaryota]\\n--buscoseed                    AUGUSTUS pre-trained species to start BUSCO. Will be override if rnaseq data is provided. [default: null]\\n\\nBraker params:\\n--run_braker                   Whether to use BRAKER for gene prediction. [default: false]\\n\\nSkipping options:\\n--skip_rename                  Skip renaming genome fasta file by funannotate sort. \\n--skip_all_masking             Skip all masking processes, please be sure that your --genome input is soft-masked before triggering this \\n                               parameter. [default: false]\\n--skip_denovo_masking          Skip denovo masking using RepeatModeler, this option can only be run when --knownrepeat fasta is provided. \\n                               [default: false]\\n--skip_functional_annotation   Skip functional annotation step. [default: false]\\n--skip_read_preprocessing      Skip RNASeq preprocessing step. [default: false]\\n\\nExecution/Engine profiles:\\nThe pipeline supports profiles to run via different Executers and Engines e.g.: -profile local,conda\\n\\nExecuter (choose one):\\n  local\\n  slurm\\n\\nEngines (choose one):\\n  conda\\n  mamba\\n  docker\\n  singularity\\n\\nPer default: -profile slurm,singularity is executed.\\n```\\n\\n## Evaluating output GFF to the exon level\\n\\nWe provided a script to analyze the output GFF of ANNOTATO (which also could be applied to the GFF file output of other pipelines) to report the number of exons per mRNA/tRNA. To run this, simply use:\\n\\n```\\npython bin/analyze_exons.py -f ${GFF}\\n```\\n\\nBelow is the sample output of this script\\n\\n```\\nINFORMATION REGARDING mRNA\\nNumber of transcripts: 33086\\nLargest number of exons in all transcripts: 128\\nMonoexonic transcripts: 4085\\nMultiexonic transcripts: 29001\\nMono:Mult Ratio: 0.14\\nBoxplot of number of exons per transcript:\\nMin: 1\\n25%: 2\\n50%: 4\\n75%: 8\\nMax: 128\\nMean: 6.978812790908542\\n==================================================\\nINFORMATION REGARDING tRNA\\nNumber of transcripts: 2017\\nLargest number of exons in all transcripts: 1\\nMonoexonic transcripts: 2017\\nMultiexonic transcripts: 0\\nNo multiexonic transcripts, unable to calculate Mono:Mult Ratio\\nBoxplot of number of exons per transcript:\\nMin: 1\\n25%: 1\\n50%: 1\\n75%: 1\\nMax: 1\\nMean: 1.0\\n==================================================\\n```\\n\\nThis script was originally written by [Katharina Hoff](https://github.com/Gaius-Augustus/GALBA/blob/main/scripts/analyze_exons.py) and was modified accordingly to suit the analysis of GFF file.\\n\\n## Performance of the workflow on annotating difference eukaryote genomes\\n\\nThe following table is the result predicted by ANNOTATO on difference species during the [Europe BioHackathon 2023](https://github.com/elixir-europe/biohackathon-projects-2023/tree/main/20).\\n\\n| Species                    | Genome size | N.Genes | N.Exons | N.mRNA | BUSCO lineage | BUSCO score                             | OMArk Completeness                                                 | OMArk Consistency                                                                       |\\n| :---:                      | :---:       | :---:   | :---:   | :---:  | :---:         | :---:                                   | :---:                                                              | :---:                                                                                   |\\n| Drosophila melanogaster    | 143M        | 14,753  | 57,343  | 14,499 | diptera       | C:96.1%[S:95.6%,D:0.5%],F:1.2%,M:2.7%   | melanogaster subgroup, C:90.38%[S:84.32%,D:6.06%],M:9.62%,,n:12442 | A:94.21%[P:4.05%,F:7.28%],I:1.61%[P:0.5%,F:0.42%],C:0.00%[P:0.00%,F:0.00%],U:4.19%      |\\n| Helleia helle              | 547M        | 37,367  | 139,302 | 28,445 | lepidoptera   | C:74.6%[S:73.4%,D:1.2%],F:5.4%,M:20.0%  | Papilionidea, C:82.04%[S:66.12%,D:15.92%],M:17.96%, n:7939         | A:44.78%[P:14.41%,F:6.02%],I:3.53%[P:2.1%,F:0.7%],C:0.00%[P:0.00%,F:0.00%],U:51.69%     |\\n| Homo sapiens chrom 19      | 58M         | 1,872   | 11,937  | 1,862  | primates      | C:5.0%[S:4.8%,D:0.2%],F:0.5%,M:94.5%    | Hominidae, C:8.57%[S:7.74%,D:0.83%],M:91.43%, n=17843              | A:87.54%[P:12.73%,F:13.1%],I:4.78%[P:1.5%,F:2.04%],C:0.00%[P:0.00%,F:0.00%],U:7.68%     |\\n| Melampus jaumei            | 958M        | 61,128  | 335,483 | 60,720 | mollusca      | C:80.4%[S:67.2%,D:13.2%],F:3.8%,M:15.8% | Lophotrochozoa, C: 92.5%[S: 66.29%, D: 26.21%], M:7.5%, n:2373     | A:41.45%[P:15.72%,F:9.97%],I:15.97%[P:10.68%,F:3.07%],C:0.00%[P:0.00%,F:0.00%],U:42.57% |\\n| Phakellia ventilabrum      | 186M        | 19,073  | 157,441 | 18,855 | metazoa       | C:80.9%[S:79.2%,D:1.7%],F:6.5%,M:12.6%  | Metazoa, C:86.79%[S:76.9%,D:9.9%],M:13.21% , n:3021                | A:53.81%[P:18.92%,F:5.06%],I:5.0%[P:2.7%,F:0.68%],C:0.00%[P:0.00%,F:0.00%],U:41.19%     |\\n| *Pocillopora* cf. *effusa* | 347M        | 35,103  | 230,901 | 33,086 | metazoa       | C:95.1%[S:92.2%,D:2.9%],F:1.7%,M:3.2%   | Eumetazoa, C:94.16%[S:84.3%,D:9.86%],M:5.84%,n:3255                | A:52.94%[P:22.30%,F:3.69%],I:3.44%[P:2.08%,F:0.28%],C:0.00%[P:0.00%,F:0.00%],U:43.62%   |\\n| Trifolium dubium           | 679M        | 78,810  | 354,662 | 77,763 | fabales       | C:95.1%[S:19.5%,D:75.6%],F:1.5%,M:3.4%  | NPAAA clade, C:94.58%[S:19.21%,D:75.38%],M:5.42%,n:15412           | A:71.99%[P:11.03%,F:6.63%],I:2.77%[P:1.66%,F:0.52%],C:0.00%[P:0.00%,F:0.00%],U:25.23%   |\\n\\n## Future work\\n- Python wrapper function to remove intermediate files\\n- Adding functional annotation with `Interproscan` and `eggnog`\\n- Adding PASA results to further improve the accuracy of the training\\n- Adding custom parameter for both `BRAKER` and `funannotate`'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Metabarcoding/eDNA through Obitools](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/Obitools-metabarcoding/tutorial.html)\" .\\n\\nThis workflow allows to analyze DNA metabarcoding / eDNA data produced on Illumina sequencers using the OBITools.'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Biodiversity data exploration](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/biodiversity-data-exploration/tutorial.html)\"\\n\\nThis workflow allows to explore biodiversity data looking at homoscedasticity, normality or collinearity of presences-absence or abundance data and at comparing beta diversity taking into account space, time and species components'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Sentinel 2 biodiversity](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/species-distribution-modeling/tutorial.html)\" .\\n\\nThis workflow allows to analyze remote sensing sentinel 2 satellites data to compute spectral indices such as the NDVI and visualizing biodiversity indicators\\n'\n",
      " '\\n\\nGalaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"Antarctic sea ecoregionalization\" .\\n\\nThis workflow allows to analyze marine benthic biodiversity data to compute ecoregions regarding environmental data.\\n'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"Deep learning to predict animal behavior\" .\\n\\nThis workflow allows to analyze animal behavior data through deep learning.\\n'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, to analyze crowdsourcing results of the SPIPOLL hoverflies GAPARS European project activity on MMOS server.\\n'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[Champs blocs](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/champs-blocs/tutorial.html)\" .\\n\\nThis workflow allows to produce Visual Rollover Indicator and dissimilarity as diversity indices on boulder fields.\\n'\n",
      " 'Galaxy Workflow created on Galaxy-E european instance, ecology.usegalaxy.eu, related to the Galaxy training tutorial \"[OBIS marine indicators](https://training.galaxyproject.org/training-material/topics/ecology/tutorials/obisindicators/tutorial.html)\" .\\n\\nThis workflow allows to compute and visualize marine biodiversity indicators from OBIS data.\\n'\n",
      " \"**Name:** K-means  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: Public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nK-means clustering is a method of cluster analysis that aims to partition ''n'' points into ''k'' clusters in which each point belongs to the cluster with the nearest mean. It follows an iterative refinement strategy to find the centers of natural clusters in the data.\\n\\nWhen executed with COMPSs, K-means first generates the input points by means of initialization tasks. For parallelism purposes, the points are split in a number of fragments received as parameter, each fragment being created by an initialization task and filled with random points.\\n\\nAfter the initialization, the algorithm goes through a set of iterations. In every iteration, a computation task is created for each fragment; then, there is a reduction phase where the results of each computation are accumulated two at a time by merge tasks; finally, at the end of the iteration the main program post-processes the merged result, generating the current clusters that will be used in the next iteration. Consequently, if ''F'' is the total number of fragments, K-means generates ''F'' computation tasks and ''F-1'' merge tasks per iteration.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --classpath=application_sources/jar/kmeans.jar kmeans.KMeans <...>\\n```\\n\\nwhere ''<...>'':\\n* -c Number of clusters\\n* -i Number of iterations\\n* -n Number of points\\n* -d Number of dimensions\\n* -f Number of fragments\\n\\n# Execution Examples\\n```\\nruncompss --classpath=application_sources/jar/kmeans.jar kmeans.KMeans\\nruncompss --classpath=application_sources/jar/kmeans.jar kmeans.KMeans -c 4 -i 10 -n 2000 -d 2 -f 2\\n```\\n\\n# Build\\n## Option 1: Native java\\n```\\ncd application_sources/; javac src/main/java/kmeans/*.java\\ncd src/main/java/; jar cf kmeans.jar kmeans/\\ncd ../../../; mv src/main/java/kmeans.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\ncd application_sources/\\nmvn clean package\\n```\\n\"\n",
      " '# covid-sequence-analysis-workflow\\n\\nThis is the official repository of the SARS-CoV-2 variant surveillance pipeline developed by Danish Technical University (DTU), Eotvos Lorand University (ELTE), EMBL-EBI, Erasmus Medical Center (EMC) under the [Versatile Emerging infectious disease Observatory (VEO)](https://www.globalsurveillance.eu/projects/veo-versatile-emerging-infectious-disease-observatory) project. The project consists of 20 European partners. It is funded by the European Commission.\\n\\nThe pipeline has been integrated on EMBL-EBI infrastructure to automatically process raw SARS-CoV-2 read data, presenting in the COVID-19 Data Portal: https://www.covid19dataportal.org/sequences?db=sra-analysis-covid19&size=15&crossReferencesOption=all#search-content.\\n\\n## Architecture\\n\\nThe pipeline supports sequence reads from both Illumina and Nanopore platforms. It is designed to be highly portable for both Google Cloud Platform and High Performance Computing cluster with IBM Spectrum LSF. We have performed secondary and tertiary analysis on millions of public samples. The pipeline shows good performance for large scale production. \\n\\n![Component diagram](doc/img/pipeline.components.png)\\n\\nThe pipeline takes SRA from the public FTP from ENA. It submits analysis objects back to ENA on the fly. The intermediate results and logs are stored in the cloud storage buckets or high performance local POSIX file system. The metadata is stored in Google BigQuery for metadata and status tracking and analysis. The runtime is created with Docker / Singularity containers and NextFlow. \\n\\n## Process to run the pipelines\\n\\nThe pipeline requires the Nextflow Tower for the application level monitoring. A free test account can be created for evaluation purposes at https://tower.nf/.\\n\\n### Preparation\\n\\n1. Store `export TOWER_ACCESS_TOKEN=\\'...\\'` in `$HOME/.bash_profile`. Restart the current session or source the updated `$HOME/.bash_profile`.\\n2. Run `git clone https://github.com/enasequence/covid-sequence-analysis-workflow`.\\n3. Create `./covid-sequence-analysis-workflow/data/projects_accounts.csv` with submission_account_id and submission_passwor, for example:\\n>  project_id,center_name,meta_key,submission_account_id,submission_password,ftp_password\\n>  PRJEB45555,\"European Bioinformatics Institute\",public,,,\\n\\n### Running pipelines\\n\\n1. Run `./covid-sequence-analysis-workflow/init.sra_index.sh` to initialize or reinitialize the metadata in BigQuery.\\n2. Run `./covid-sequence-analysis-workflow/./start.lsf.jobs.sh` with proper parameters to start the batch jobs on LSF or `./covid-sequence-analysis-workflow/./start.gls.jobs.sh` with proper parameters to start the batch jobs on GCP.\\n\\n### Error handling\\n\\nIf a job is killed or died, run the following to update the metadata to avoid reprocessing samples completed successfully.\\n\\n1. Run `./covid-sequence-analysis-workflow/update.receipt.sh <batch_id>` to collect the submission receipts and to update submission metadata. The script can be run at anytime. It needs to be run if a batch job is killed instead of completed for any reason.\\n2. Run `./covid-sequence-analysis-workflow/set.archived.sh` to update stats for analyses submitted. The script can be run at anytime. It needs to be run at least once before ending a snapshot to make sure that the stats are up-to-date.\\n\\nTo reprocess the samples failed, delete the record in `sra_processing`.\\n'\n",
      " '# ![sanger-tol/readmapping](docs/images/sanger-tol-readmapping_logo.png)\\n\\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.6563577-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.6563577)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/readmapping)\\n\\n## Introduction\\n\\n**sanger-tol/readmapping** is a bioinformatics best-practice analysis pipeline for mapping reads generated using Illumina, HiC, PacBio and Nanopore technologies against a genome assembly.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nOn merge to `dev` and `main` branch, automated continuous integration tests run the pipeline on a full-sized dataset on the Wellcome Sanger Institute HPC farm using the Nextflow Tower infrastructure. This ensures that the pipeline runs on full sized datasets, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources.\\n\\n## Pipeline summary\\n\\n<img src=\"https://raw.githubusercontent.com/sanger-tol/readmapping/976525ad7b5327607a049aa85bbca36a48c6ba48/docs/images/sanger-tol-readmapping_workflow.png\" height=\"700\">\\n\\n## Quick Start\\n\\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.10.1`)\\n\\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n   ```bash\\n   nextflow run sanger-tol/readmapping -profile test,YOURPROFILE --outdir <OUTDIR>\\n   ```\\n\\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\\n\\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\\n\\n4. Start running your own analysis!\\n\\n   ```bash\\n   nextflow run sanger-tol/readmapping --input samplesheet.csv --fasta genome.fa.gz --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\\n   ```\\n\\n## Credits\\n\\nsanger-tol/readmapping was originally written by [Priyanka Surana](https://github.com/priyanka-surana).\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline:\\n\\n- [Matthieu Muffato](https://github.com/muffato) for the text logo\\n- [Guoying Qi](https://github.com/gq1) for being able to run tests using Nf-Tower and the Sanger HPC farm\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don\\'t hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/readmapping/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\\n\\n## Citations\\n\\nIf you use sanger-tol/readmapping for your analysis, please cite it using the following doi: [10.5281/zenodo.6563577](https://doi.org/10.5281/zenodo.6563577)\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n'\n",
      " \"# ![sanger-tol/ensemblgenedownload](docs/images/sanger-tol-ensemblgenedownload_logo.png)\\n\\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+CI%22)\\n\\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblgenedownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblgenedownload/actions?query=workflow%3A%22nf-core+linting%22) -->\\n\\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183206-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183206)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n\\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\\n\\n## Introduction\\n\\n**sanger-tol/ensemblgenedownload** is a pipeline that downloads gene annotations from Ensembl into the Tree of Life directory structure.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\\n\\n## Pipeline summary\\n\\n## Overview\\n\\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories, and geneset versions.\\nAssembly accession numbers are optional. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\\nThe pipeline downloads the Fasta files of the genes (cdna, cds, and protein sequences) as well as the GFF3 file.\\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\\n\\nSteps involved:\\n\\n- Download from Ensembl the GFF3 file, and the sequences of the genes in\\n  Fasta format.\\n- Compress and index all Fasta files with `bgzip`, `samtools faidx`, and\\n  `samtools dict`.\\n- Compress and index the GFF3 file with `bgzip` and `tabix`.\\n\\n## Quick Start\\n\\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\\n\\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n   ```bash\\n   nextflow run sanger-tol/ensemblgenedownload -profile test,YOURPROFILE --outdir <OUTDIR>\\n   ```\\n\\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\\n\\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\\n\\n4. Start running your own analysis!\\n\\n   ```console\\n   nextflow run sanger-tol/ensemblgenedownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\\n   ```\\n\\n## Documentation\\n\\nThe sanger-tol/ensemblgenedownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\\n\\n## Credits\\n\\nsanger-tol/ensemblgenedownload was originally written by @muffato.\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblgenedownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\\n\\n## Citations\\n\\nIf you use sanger-tol/ensemblgenedownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183206](https://doi.org/10.5281/zenodo.7183206)\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n\"\n",
      " \"# ![sanger-tol/ensemblrepeatdownload](docs/images/sanger-tol-ensemblrepeatdownload_logo.png)\\n\\n[![GitHub Actions CI Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20CI/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+CI%22)\\n\\n<!-- [![GitHub Actions Linting Status](https://github.com/sanger-tol/ensemblrepeatdownload/workflows/nf-core%20linting/badge.svg)](https://github.com/sanger-tol/ensemblrepeatdownload/actions?query=workflow%3A%22nf-core+linting%22) -->\\n\\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.7183380-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.7183380)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n\\n[![Get help on Slack](http://img.shields.io/badge/slack-SangerTreeofLife%20%23pipelines-4A154B?labelColor=000000&logo=slack)](https://SangerTreeofLife.slack.com/channels/pipelines)\\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40sangertol-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/sangertol)\\n[![Watch on YouTube](http://img.shields.io/badge/youtube-tree--of--life-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/channel/UCFeDpvjU58SA9V0ycRXejhA)\\n\\n## Introduction\\n\\n**sanger-tol/ensemblrepeatdownload** is a pipeline that downloads repeat annotations from Ensembl into a Tree of Life directory structure.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the GitHub CI infrastructure. This ensures that the pipeline runs in a third-party environment, and has sensible resource allocation defaults set to run on real-world datasets.\\n\\n## Pipeline summary\\n\\n## Overview\\n\\nThe pipeline takes a CSV file that contains assembly accession number, Ensembl species names (as they may differ from Tree of Life ones !), output directories.\\nAssembly accession numbers are optional too. If missing, the pipeline assumes it can be retrieved from files named `ACCESSION` in the standard location on disk.\\nThe pipeline downloads the repeat annotation as the masked Fasta file and a BED file.\\nAll files are compressed with `bgzip`, and indexed with `samtools faidx` or `tabix`.\\n\\nSteps involved:\\n\\n- Download the masked fasta file from Ensembl.\\n- Extract the coordinates of the masked regions into a BED file.\\n- Compress and index the BED file with `bgzip` and `tabix`.\\n\\n## Quick Start\\n\\n1. Install [`Nextflow`](https://www.nextflow.io/docs/latest/getstarted.html#installation) (`>=22.04.0`)\\n\\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) (you can follow [this tutorial](https://singularity-tutorial.github.io/01-installation/)), [`Podman`](https://podman.io/), [`Shifter`](https://nersc.gitlab.io/development/shifter/how-to-use/) or [`Charliecloud`](https://hpc.github.io/charliecloud/) for full pipeline reproducibility _(you can use [`Conda`](https://conda.io/miniconda.html) both to install Nextflow itself and also to manage software within pipelines. Please only use it within pipelines as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_.\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n   ```bash\\n   nextflow run sanger-tol/ensemblrepeatdownload -profile test,YOURPROFILE --outdir <OUTDIR>\\n   ```\\n\\n   Note that some form of configuration will be needed so that Nextflow knows how to fetch the required software. This is usually done in the form of a config profile (`YOURPROFILE` in the example command above). You can chain multiple config profiles in a comma-separated string.\\n\\n   > - The pipeline comes with config profiles called `docker`, `singularity`, `podman`, `shifter`, `charliecloud` and `conda` which instruct the pipeline to use the named tool for software management. For example, `-profile test,docker`.\\n   > - Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n   > - If you are using `singularity`, please use the [`nf-core download`](https://nf-co.re/tools/#downloading-pipelines-for-offline-use) command to download images first, before running the pipeline. Setting the [`NXF_SINGULARITY_CACHEDIR` or `singularity.cacheDir`](https://www.nextflow.io/docs/latest/singularity.html?#singularity-docker-hub) Nextflow options enables you to store and re-use the images from a central location for future pipeline runs.\\n   > - If you are using `conda`, it is highly recommended to use the [`NXF_CONDA_CACHEDIR` or `conda.cacheDir`](https://www.nextflow.io/docs/latest/conda.html) settings to store the environments in a central location for future pipeline runs.\\n\\n4. Start running your own analysis!\\n\\n   ```console\\n   nextflow run sanger-tol/ensemblrepeatdownload --input $PWD/assets/samplesheet.csv --outdir <OUTDIR> -profile <docker/singularity/podman/shifter/charliecloud/conda/institute>\\n   ```\\n\\n## Documentation\\n\\nThe sanger-tol/ensemblrepeatdownload pipeline comes with documentation about the pipeline [usage](docs/usage.md) and [output](docs/output.md).\\n\\n## Credits\\n\\nsanger-tol/ensemblrepeatdownload was originally written by @muffato.\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don't hesitate to get in touch on the [Slack `#pipelines` channel](https://sangertreeoflife.slack.com/channels/pipelines). Please [create an issue](https://github.com/sanger-tol/ensemblrepeatdownload/issues/new/choose) on GitHub if you are not on the Sanger slack channel.\\n\\n## Citations\\n\\nIf you use sanger-tol/ensemblrepeatdownload for your analysis, please cite it using the following doi: [10.5281/zenodo.7183380](https://doi.org/10.5281/zenodo.7183380)\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n\"\n",
      " \"[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.XXXXXXX-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.XXXXXXX)\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\\n\\n## Introduction\\n\\n**sanger-tol/treeval** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/).\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\\n\\n1. Parse input yaml ( YAML_INPUT )\\n2. Generate my.genome file ( GENERATE_GENOME )\\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\\n5. Generate a repeat density graph ( REPEAT_DENSITY )\\n6. Generate a gap track ( GAP_FINDER )\\n7. Generate a map of self complementary sequence ( SELFCOMP )\\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\\n11. Generate a telomere track based on input motif ( TELO_FINDER )\\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\\n\\n## Usage\\n\\n> **Note**\\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\\n> with `-profile test` before running the workflow on actual data.\\n\\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\\n\\nNow, you can run the pipeline using:\\n\\n```bash\\n# For the FULL pipeline\\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\\n\\n# For the RAPID subset\\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\\n```\\n\\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\\n\\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\\n\\n> **Warning:**\\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\\n\\n## Credits\\n\\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline:\\n\\n<ul>\\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\\n  <li>@mcshane - For guidance on algorithms </li>\\n  <li>@muffato - For code reviews and code support</li>\\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\\n</ul>\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\n## Citations\\n\\n<!--TODO: Citation-->\\n\\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.XXXXXX](https://doi.org/10.5281/zenodo.XXXXXX).\\n\\n### Tools\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nYou can cite the `nf-core` publication as follows:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n\"\n",
      " '**Contact Person:** support-compss@bsc.es  \\n**Access Level:** public  \\n**License Agreement:** Apache2  \\n**Platform:** COMPSs  \\n\\n# Description\\n\\nSimple is an application that takes one value and increases it by five units. The purpose of this application is to show how tasks are managed by COMPSs.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/simple.py initValue\\n```\\n\\nwhere:\\n* initValue: Initial value for counter\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/simple.py 1\\nruncompss src/simple.py 1\\npython -m pycompss src/simple.py 1\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " \"# Evaluation of Swin Transformer and knowledge transfer for denoising of super-resolution structured illumination microscopy data\\n\\nIn recent years, convolutional neural network (CNN)-based methods have shown remarkable performance in the denoising and reconstruction of super-resolved structured illumination microscopy (SR-SIM) data. Therefore, CNN-based architectures have been the main focus of existing studies. Recently, however, an alternative and highly\\ncompetitive deep learning architecture, Swin Transformer, has been proposed for image restoration tasks. In this work, we present SwinT-fairSIM, a novel method for restoring SR-SIM images with low signal-to-noise ratio (SNR) based on Swin Transformer. The experimental results show that SwinT-fairSIM outperforms previous CNN-based denoising methods. Furthermore, the generalization capabilities of deep learning methods for image restoration tasks on real fluorescence microscopy data have not been fully explored yet, i.e., the extent to which trained artificial neural networks are limited to specific types of cell structures and noise. Therefore, as a second contribution, we benchmark two types of transfer learning, i.e., direct transfer and fine-tuning, in combination with SwinT-fairSIM and two CNN-based methods for denoising SR-SIM data. Direct transfer does not prove to be a viable strategy, but fine-tuning achieves results comparable to conventional training from scratch while saving computational time and potentially reducing the amount of required training data. As a third contribution, we published four datasets of raw SIM images and already reconstructed SR-SIM images. These datasets cover two types of cell structures, tubulin filaments and vesicle structures. Different noise levels are available for the tubulin filaments. These datasets are structured in such a way that they can be easily used by the research community for research on denoising, super-resolution, and transfer learning strategies.\\n\\nThe SIM microscopy datasets that were used during this work can be downloaded through this link: http://dx.doi.org/10.5524/102461\\n\\n\\n## Installation:\\n\\nThis implementation requires the Tensorflow-GPU2.5 version. To avoid package conflicts, we recommend you create a new environment by using our provided environment.yml file. To create a new environment please run the following script:\\n\\n>  conda env create -f environment.yml\\n\\n## How to use this code:\\n\\nThis code can be used to train a denoising model from scratch or to fine-tune a pretrained model. After the installation of the Python environment from the yml file, the next step is to set the input parameters in the JSON parameter file (i.e., ParameterFile.json). Most of the input parameters are self-explanatory but below we will discuss some of the important input parameters from the JSON file:\\n\\n- TrainNetworkfromScratch: This input parameter will train the model from scratch If set to True, otherwise, for fine-tuning, It should be False.\\n- ActivateTrainandTestModel: This parameter will be set to False If you want to use this code for evaluation of the trained model or the reproducibility of the results by using pretrained models.\\n- PretrainedmodelPath: This parameter is mandatory in case of fine-tuning or evaluation of a pretrained model.\\n- FineTuneStartingpoint and FineTuneEndingpoint: These two input parameters are essential in the fine-tuning of a pretrained model. All the layers between the starting and ending points will be frozen during the fine-tuning of the pretrained model.\\n\\nAfter the assignment of the input parameters. You can run the following script from the command line to start training the model:\\n\\n> python MainModule.py 'ParameterFile.json'\\n\\n## Reproducibility and evaluation:\\n\\nTo reproduce the results of the paper all the trained models used in this work are available in the 'Models' directory at [zenodo](https://doi.org/10.5281/zenodo.7626173). This code is capable of performing all the necessary steps for the training and test phases. It will automatically evaluate the model and generate a result directory to write all the results. Similarly, during the training process, It will also create a model directory and save the trained model along with the best checkpoints in the model directory.   \\n\\n## Important Note:\\n\\nThis code will work with at least one GPU.\\n\\n## Reference:\\n\\nPlease cite our paper in case you use this code for any scientific publication. We will soon upload the citation index!\\n\\n\\n\\n\\n\"\n",
      " 'This workflow is created as part of a tutorial listed on GTN. The workflow shows the steps in human copy number variance detection using the Contrl_FREEC tool.'\n",
      " 'This workflow is composed with the XCMS tool R package (Smith, C.A. 2006) able to extract, filter, align and fill gapand the possibility to annotate isotopes, adducts and fragments using the CAMERA R package (Kuhl, C 2012).\\n\\n\\nhttps://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-preprocessing/tutorial.html '\n",
      " '**Name:** Increment  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nIncrement is an application that takes three different values and increases them a number of given times.\\n\\nThe purpose of this application is to show parallelism between the different increments.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/increment.py N initValue1 initValue2 initValue3\\n```\\n\\nwhere:\\n* N: Number of times to increase the counters\\n* initValue1: Initial value for counter 1\\n* initValue2: Initial value for counter 2\\n* initValue3: Initial value for counter 3\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/increment.py 10 1 2 3\\nruncompss src/wordcount.py src/increment.py 10 1 2 3\\npython -m pycompss src/wordcount.py src/increment.py 10 1 2 3\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " 'This workflow is composed with the XCMS tool R package (Smith, C.A. 2006) able to extract and the metaMS R package (Wehrens, R 2014) for the field of untargeted metabolomics. \\n\\nhttps://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gcms/tutorial.html'\n",
      " 'Workflow for Creating a large disease network from various datasets and databases for IBM, and applying the active subnetwork identification method MOGAMUN.'\n",
      " '# ONTViSc (ONT-based Viral Screening for Biosecurity)\\n\\n## Introduction\\neresearchqut/ontvisc is a Nextflow-based bioinformatics pipeline designed to help diagnostics of viruses and viroid pathogens for biosecurity. It takes fastq files generated from either amplicon or whole-genome sequencing using Oxford Nanopore Technologies as input.\\n\\nThe pipeline can either: 1) perform a direct search on the sequenced reads, 2) generate clusters, 3) assemble the reads to generate longer contigs or 4) directly map reads to a known reference. \\n\\nThe reads can optionally be filtered from a plant host before performing downstream analysis.\\n\\n## Pipeline overview\\n- Data quality check (QC) and preprocessing\\n  - Merge fastq files (optional)\\n  - Raw fastq file QC (Nanoplot)\\n  - Trim adaptors (PoreChop ABI - optional)\\n  - Filter reads based on length and/or quality (Chopper - optional)\\n  - Reformat fastq files so read names are trimmed after the first whitespace (bbmap)\\n  - Processed fastq file QC (if PoreChop and/or Chopper is run) (Nanoplot)\\n- Host read filtering\\n  - Align reads to host reference provided (Minimap2)\\n  - Extract reads that do not align for downstream analysis (seqtk)\\n- QC report\\n  - Derive read counts recovered pre and post data processing and post host filtering\\n- Read classification analysis mode\\n- Clustering mode\\n  - Read clustering (Rattle)\\n  - Convert fastq to fasta format (seqtk)\\n  - Cluster scaffolding (Cap3)\\n  - Megablast homology search against ncbi or custom database (blast)\\n  - Derive top candidate viral hits\\n- De novo assembly mode\\n  - De novo assembly (Canu or Flye)\\n  - Megablast homology search against ncbi or custom database or reference (blast)\\n  - Derive top candidate viral hits\\n- Read classification mode\\n  - Option 1 Nucleotide-based taxonomic classification of reads (Kraken2, Braken)\\n  - Option 2 Protein-based taxonomic classification of reads (Kaiju, Krona)\\n  - Option 3 Convert fastq to fasta format (seqtk) and perform direct homology search using megablast (blast)\\n- Map to reference mode\\n  - Align reads to reference fasta file (Minimap2) and derive bam file and alignment statistics (Samtools)\\n\\nDetailed instructions can be found on [GitHub](https://github.com/eresearchqut/ontvisc/).\\nA step-by-step guide with instructions on how to set up and execute the ONTvisc pipeline on one of the HPC systems: Lyra (Queensland University of Technology), Setonix (Pawsey) and Gadi (National Computational Infrastructure) can be found [here](https://mantczakaus.github.io/ontvisc_guide/).\\n\\n### Authors\\nMarie-Emilie Gauthier <gauthiem@qut.edu.au>  \\nCraig Windell <c.windell@qut.edu.au>  \\nMagdalena Antczak <magdalena.antczak@qcif.edu.au>  \\nRoberto Barrero <roberto.barrero@qut.edu.au>  '\n",
      " \"**Name:** Java Wordcount  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nWordcount application. There are two versions of Wordcount, depending on how the input data is given.\\n\\n## Version 1\\n''Single input file'', where all the text is given in the same file and the chunks are calculated with a BLOCK_SIZE parameter.\\n\\n## Version 2\\n''Multiple input files'', where the text fragments are already in different files under the same directory\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --classpath=application_sources/jar/wordcount.jar wordcount.multipleFiles.Wordcount DATA_FOLDER\\nruncompss --classpath=application_sources/jar/wordcount.jar wordcount.uniqueFile.Wordcount DATA_FILE BLOCK_SIZE\\n```\\n\\nwhere:\\n* DATA_FOLDER: Absolute path to the base folder of the dataset files\\n* DATA_FILE: Absolute path to the dabase file\\n* BLOCK_SIZE: Number of bytes of each block\\n\\n# Execution Examples\\n```\\nruncompss --classpath=application_sources/jar/wordcount.jar wordcount.multipleFiles.Wordcount dataset/data-set/\\nruncompss --classpath=application_sources/jar/wordcount.jar wordcount.uniqueFile.Wordcount dataset/data-set/file_small.txt 650\\nruncompss --classpath=application_sources/jar/wordcount.jar wordcount.uniqueFile.Wordcount dataset/data-set/file_long.txt 250000\\n\\n```\\n\\n# Build\\n\\n## Option 1: Native java\\n```\\ncd application_sources/; javac src/main/java/wordcount/*.java\\ncd src/main/java/; jar cf wordcount.jar wordcount/\\ncd ../../../; mv src/main/java/wordcount.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\ncd application_sources/\\nmvn clean package\\n```\\n\"\n",
      " 'Analyse Bulk RNA-Seq data in preparation for downstream Pathways analysis with MINERVA'\n",
      " \"**Name:** Word Count  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nWordcount is an application that counts the number of words for a given set of files.\\n\\nTo allow parallelism the file is divided in blocks that are treated separately and merged afterwards.\\n\\nResults are printed to a Pickle binary file, so they can be checked using: python -mpickle result.txt\\n\\nThis example also shows how to manually add input or output datasets to the workflow provenance recording (using the 'input' and 'output' terms in the ro-crate-info.yaml file).\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python $(pwd)/application_sources/src/wordcount_blocks.py filePath resultPath blockSize\\n```\\n\\nwhere:\\n* filePath: Absolute path of the file to parse\\n* resultPath: Absolute path to the result file\\n* blockSize: Size of each block. The lower the number, the more tasks will be generated in the workflow\\n\\n# Execution Examples\\n```\\nruncompss --lang=python $(pwd)/application_sources/src/wordcount_blocks.py $(pwd)/dataset/data/compss.txt result.txt 300\\nruncompss $(pwd)/application_sources/src/wordcount_blocks.py $(pwd)/dataset/data/compss.txt result.txt 300\\npython -m pycompss $(pwd)/application_sources/src/wordcount.py $(pwd)/dataset/data/compss.txt result.txt 300\\n```\\n\\n# Build\\nNo build is required\\n\"\n",
      " 'This portion of the workflow produces sets of feature Counts ready for analysis by limma/etc.'\n",
      " '**Name:** Lanczos SVD  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum4  \\n\\nLanczos SVD for computing singular values needed to reach an epsilon of 1e-3 on a matrix of (150000, 150).  \\nThe input matrix is generated synthetically.  \\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " \"This is a Nextflow implementaion of the GATK Somatic Short Variant Calling workflow. This workflow can be used to discover somatic short variants (SNVs and indels) from tumour and matched normal BAM files following GATK's Best Practices Workflow. The workflowis currently optimised to run efficiently and at scale on the National Compute Infrastructure, Gadi.\"\n",
      " '![bacpage](https://raw.githubusercontent.com/CholGen/bacpage/split_into_command/.github/logo_dark.png){width=500}\\n\\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology.\\n\\n# Introduction\\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\\n\\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. Installation is fast and straightfoward. The pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\\nA successful run generates consensus sequences, typing information, phylogenetic tree, and quality control report.\\n\\n# Features\\nWe anticipate the pipeline will be able to perform the following functions:\\n- [x] Reference-based assembly of Illumina paired-end reads\\n- [x] *De novo* assembly of Illumina paired-end reads\\n- [ ] *De novo* assembly of ONT long reads\\n- [x] Run quality control checks\\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \\n- [x] MLST profiling and virulence factor detection\\n- [x] Antimicrobial resistance genes detection\\n- [ ] Plasmid detection\\n\\n# Installation\\n1. Install `miniconda` by running the following two command:\\n```commandline\\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\\nbash Mambaforge-$(uname)-$(uname -m).sh\\n```\\n\\n2. Clone the repository:\\n```commandline\\ngit clone https://github.com/CholGen/bacpage.git\\n```\\n\\n3. Install and activate the pipeline\\'s conda environment:\\n```commandline\\ncd bacpage/\\nmamba env create -f environment.yaml\\nmamba activate bacpage\\n```\\n\\n4. Install the `bacpage` command:\\n```commandline\\npip install .\\n```\\n\\n5. Test the installation:\\n```commandline\\nbacpage -h\\nbacpage version\\n```\\nThese command should print the help and version of the program. Please create an issue if this is not the case.\\n\\n# Usage\\n0. Navigate to the pipeline\\'s directory.\\n1. Copy the `example/` directory to create a directory specifically for each batch of samples.\\n```commandline\\ncp example/ <your-project-directory-name>\\n```\\n2. Place raw sequencing reads in the `input/` directory of your project directory.\\n3. Record the name and absolute path of raw sequencing reads in the `sample_data.csv` found within your project directory.\\n4. Replace the values `<your-project-directory-name>` and `<sequencing-directory>` in `config.yaml` found within your project directory, with the absolute path of your project directory and pipeline directory, respectively.\\n5. Determine how many cores are available on your computer:\\n```commandline\\ncat /proc/cpuinfo | grep processor\\n```\\n6. From the pipeline\\'s directory, run the entire pipeline on your samples using the following command:\\n```commandline\\nsnakemake --configfile <your-project-directory-name>/config.yaml --cores <cores>\\n```\\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in `<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment and quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`. A phylogeny comparing your sequences to the background dataset can be found at `<your-project-directory-name>/results/phylogeny/phylogeny.tree`\\n'\n",
      " \"This workflow processes the CMO fastqs with CITE-seq-Count and include the translation step required for cellPlex processing. In parallel it processes the Gene Expresion fastqs with STARsolo, filter cells with DropletUtils and reformat all outputs to be easily used by the function 'Read10X' from Seurat.\"\n",
      " '# BACPAGE\\n\\nThis repository contains an easy-to-use pipeline for the assembly and analysis of bacterial genomes using ONT long-read or Illumina short-read technology. \\nRead the complete documentation and instructions for bacpage and each of its functions [here](https://cholgen.github.io/sequencing-resources/bacpage-command.html)\\n\\n# Introduction\\nAdvances in sequencing technology during the COVID-19 pandemic has led to massive increases in the generation of sequencing data. Many bioinformatics tools have been developed to analyze this data, but very few tools can be utilized by individuals without prior bioinformatics training.\\n\\nThis pipeline was designed to encapsulate pre-existing tools to automate analysis of whole genome sequencing of bacteria. \\nInstallation is fast and straightfoward. \\nThe pipeline is easy to setup and contains rationale defaults, but is highly modular and configurable by more advance users.\\nBacpage has individual commands to generate consensus sequences, perform *de novo* assembly, construct phylogenetic tree, and generate quality control reports.\\n\\n# Features\\nWe anticipate the pipeline will be able to perform the following functions:\\n- [x] Reference-based assembly of Illumina paired-end reads\\n- [x] *De novo* assembly of Illumina paired-end reads\\n- [ ] *De novo* assembly of ONT long reads\\n- [x] Run quality control checks\\n- [x] Variant calling using [bcftools](https://github.com/samtools/bcftools)\\n- [x] Maximum-likelihood phylogenetic inference of processed samples and background dataset using [iqtree](https://github.com/iqtree/iqtree2) \\n- [x] MLST profiling and virulence factor detection\\n- [x] Antimicrobial resistance genes detection\\n- [ ] Plasmid detection\\n\\n# Installation\\n1. Install `mamba` by running the following two command:\\n```commandline\\ncurl -L -O \"https://github.com/conda-forge/miniforge/releases/latest/download/Mambaforge-$(uname)-$(uname -m).sh\"\\nbash Mambaforge-$(uname)-$(uname -m).sh\\n```\\n\\n2. Clone the bacpage repository:\\n```commandline\\ngit clone https://github.com/CholGen/bacpage.git\\n```\\n\\n3. Switch to the development branch of the pipeline:\\n```commandline\\ncd bacpage/\\ngit checkout -b split_into_command\\n```\\n\\n3. Install and activate the pipeline\\'s conda environment:\\n```commandline\\nmamba env create -f environment.yaml\\nmamba activate bacpage\\n```\\n\\n4. Install the `bacpage` command:\\n```commandline\\npip install .\\n```\\n\\n5. Test the installation:\\n```commandline\\nbacpage -h\\nbacpage version\\n```\\nThese command should print the help and version of the program. Please create an issue if this is not the case.\\n\\n# Updating\\n\\n1. Navigate to the directory where you cloned the bacpage repository on the command line:\\n```commandline\\ncd bacpage/\\n```\\n2. Activate the bacpage conda environment:\\n```commandline\\nmamba activate bacpage\\n```\\n3. Pull the lastest changes from GitHub:\\n```commandline\\ngit pull\\n```\\n4. Update the bacpage conda environemnt:\\n```commandline\\nmamba env update -f environment.yaml\\n```\\n5. Reinstall the `bacpage` command:\\n```commandline\\npip install .\\n```\\n\\n# Usage\\n0. Activate the bacpage conda environment:\\n```commandline\\nmamba activate bacpage\\n```\\n1. Create a directory specifically for the batch of samples you would like to analyze (called a project directory).\\n```commandline\\nbacpage setup [your-project-directory-name]\\n```\\n2. Place paired sequencing reads in the `input/` directory of your project directory.\\n3. From the pipeline\\'s directory, run the reference-based assembly pipeline on your samples using the following command:\\n```commandline\\nbacpage assemble [your-project-directory-name]\\n```\\nThis will generate a consensus sequence in FASTA format for each of your samples and place them in \\n`<your-project-directory-name>/results/consensus_sequences/<sample>.masked.fasta`. An HTML report containing alignment \\nand quality metrics for your samples can be found at `<your-project-directory-name>/results/reports/qc_report.html`.\\n'\n",
      " 'The workflow takes ONT reads collection, runs SeqKit and Nanoplot. The main outputs are a table and plots of raw reads stats.'\n",
      " 'The workflow takes a trimmed Illumina paired-end reads collection, runs Meryl to create a K-mer database, Genomescope2 to estimate genome properties and Smudgeplot to estimate ploidy. The main results are K-mer ddatabase and genome profiling plots, tables, and values useful for downstream analysis. Default K-mer length and ploidy for Genomescope are 21 and 2, respectively. '\n",
      " '# Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data\\n\\n> Hall, M, Coin, L., Pangenome databases provide superior host removal and mycobacteria classification from clinical metagenomic data. bioRxiv 2023. doi: [10.1101/2023.09.18.558339][doi]\\n\\nBenchmarking different ways of doing read (taxonomic) classification, with a focus on\\nremoval of contamination and classification of _M. tuberculosis_ reads.\\n\\nThis repository contains the code and snakemake pipeline to build/download the\\ndatabases, obtain all results from [the paper][doi], along with accompanying configuration\\nfiles.\\n\\nCustom databases have all been uploaded to Zenodo, along with the simulated reads:\\n\\n- Nanopore simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339788>\\n- Illumina simulated metagenomic reads - <https://doi.org/10.5281/zenodo.8339790>\\n- Nanopore and Illumina artificial real reads - <https://doi.org/10.5281/zenodo.10472796>\\n- Kraken2 database built from the Human Pangenome Reference Consortium\\n  genomes - <https://doi.org/10.5281/zenodo.8339731>\\n- Kraken2 database built from the kraken2 Human\\n  library - <https://doi.org/10.5281/zenodo.8339699>\\n- Kraken2 database built from a *Mycobacterium* representative set of\\n  genomes - <https://doi.org/10.5281/zenodo.8339821>\\n- A (fasta) database of representative genomes from the *Mycobacterium*\\n  genus - <https://doi.org/10.5281/zenodo.8339940>\\n- A (fasta) database of *M. tuberculosis* genomes from a variety of\\n  lineages - <https://doi.org/10.5281/zenodo.8339947>\\n- The fasta file built from the [Clockwork](https://github.com/iqbal-lab-org/clockwork)\\n  decontamination pipeline - <https://doi.org/10.5281/zenodo.8339802>\\n\\n## Example usage\\n\\nWe provide some usage examples showing how to download the databases and then use them\\non your reads.\\n\\n### Human read removal\\n\\nThe method we found to give the best balance of runtime, memory usage, and precision and\\nrecall was kraken2 with a database built from the Human Pangenome Reference Consortium\\ngenomes.\\n\\nThis example has been wrapped into a standalone tool called [`nohuman`](https://github.com/mbhall88/nohuman/) which takes a fastq as input and returns a fastq with human reads removed.\\n\\n#### Download human database\\n\\n```\\nmkdir HPRC_db/\\ncd HPRC_db\\nURL=\"https://zenodo.org/record/8339732/files/k2_HPRC_20230810.tar.gz\"\\nwget \"$URL\"\\ntar -xzf k2_HPRC_20230810.tar.gz\\nrm k2_HPRC_20230810.tar.gz\\n```\\n\\n#### Run kraken2 with HPRC database\\n\\nYou\\'ll need [kraken2](https://github.com/DerrickWood/kraken2) installed for this step.\\n\\n```\\nkraken2 --threads 4 --db HPRC_db/ --output classifications.tsv reads.fq\\n```\\n\\nIf you are using Illumina reads, a slight adjustment is needed\\n\\n```\\nkraken2 --paired --threads 4 --db HPRC_db/ --output classifications.tsv reads_1.fq reads_2.fq\\n```\\n\\n#### Extract non-human reads\\n\\nYou\\'ll need [seqkit](https://github.com/shenwei356/seqkit) installed for this step\\n\\nFor Nanopore data\\n\\n```\\nawk -F\\'\\\\t\\' \\'$1==\"U\" {print $2}\\' classifications.tsv | \\\\\\n  seqkit grep -f - -o reads.depleted.fq reads.fq\\n```\\n\\nFor Illumina data\\n\\n```\\nawk -F\\'\\\\t\\' \\'$1==\"U\" {print $2}\\' classifications.tsv > ids.txt\\nseqkit grep --id-regexp \\'^(\\\\S+)/[12]\\' -f ids.txt -o reads_1.depleted.fq reads_1.fq\\nseqkit grep --id-regexp \\'^(\\\\S+)/[12]\\' -f ids.txt -o reads_2.depleted.fq reads_2.fq\\n```\\n\\n### *M. tuberculosis* classification/enrichment\\n\\nFor this step we recommend either [minimap2](https://github.com/lh3/minimap2) or kraken2\\nwith a *Mycobacterium* genus database. We leave it to the user to decide which approach\\nthey prefer based on the results in our manuscript.\\n\\n#### Download databases\\n\\n```\\nmkdir Mycobacterium_db\\ncd Mycobacterium_db\\n# download database for use with minimap2\\nURL=\"https://zenodo.org/record/8339941/files/Mycobacterium.rep.fna.gz\"\\nwget \"$URL\"\\nIDS_URL=\"https://zenodo.org/record/8343322/files/mtb.ids\"\\nwget \"$IDS_URL\"\\n# download kraken database\\nURL=\"https://zenodo.org/record/8339822/files/k2_Mycobacterium_20230817.tar.gz\"\\nwget \"$URL\"\\ntar -xzf k2_Mycobacterium_20230817.tar.gz\\nrm k2_Mycobacterium_20230817.tar.gz\\n```\\n\\n#### Classify reads\\n\\n**minimap2**\\n\\n```\\n# nanopore\\nminimap2 --secondary=no -c -t 4 -x map-ont -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads.depleted.fq\\n# illumina\\nminimap2 --secondary=no -c -t 4 -x sr -o reads.aln.paf Mycobacterium_db/Mycobacterium.rep.fna.gz reads_1.depleted.fq reads_2.depleted.fq\\n```\\n\\n**kraken2**\\n\\n```\\n# nanopore\\nkraken2 --db Mycobacterium_db --threads 4 --report myco.kreport --output classifications.myco.tsv reads.depleted.fq\\n# illumina\\nkraken2 --db Mycobacterium_db --paired --threads 4 --report myco.kreport --output classifications.myco.tsv reads_1.depleted.fq reads_2.depleted.fq\\n```\\n\\n#### Extract *M. tuberculosis* reads\\n\\n**minimap2**\\n\\n```\\n# nanopore\\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 | \\\\\\n  seqkit grep -f - -o reads.enriched.fq reads.depleted.fq\\n# illumina\\ngrep -Ff Mycobacterium_db/mtb.ids reads.aln.paf | cut -f1 > keep.ids\\nseqkit grep -f keep.ids -o reads_1.enriched.fq reads_1.depleted.fq\\nseqkit grep -f keep.ids -o reads_2.enriched.fq reads_2.depleted.fq\\n```\\n\\n**kraken2**\\n\\nWe\\'ll use\\nthe [`extract_kraken_reads.py` script](https://github.com/jenniferlu717/KrakenTools#extract_kraken_readspy)\\nfor this\\n\\n```\\n# nanopore\\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads.depleted.fq -o reads.enriched.fq -t 1773 -r myco.kreport --include-children\\n# illumina\\npython extract_kraken_reads.py -k classifications.myco.tsv -1 reads_1.depleted.fq -2 reads_2.depleted.fq -o reads_1.enriched.fq -o2 reads_2.enriched.fq -t 1773 -r myco.kreport --include-children\\n```\\n\\n[doi]: https://doi.org/10.1101/2023.09.18.558339 \\n'\n",
      " 'The workflow takes a trimmed Illumina WGS paired-end reads collection, Collapsed contigs, and the values for transition parameter and max coverage depth (calculated from WF1) to run Purge_Dups. It produces purged Collapsed contigs assemblies, and runs all the QC analysis (gfastats, BUSCO, and Merqury). '\n",
      " 'The workflow takes trimmed HiC forward and reverse reads, and one assembly (e.g.: Hap1 or Pri or Collapsed) to produce a scaffolded assembly using YaHS. It also runs all the QC analyses (gfastats, BUSCO, and Merqury). '\n",
      " '# Framework for construction of phylogenetic networks on High Performance Computing (HPC) environment\\n\\n## Introduction\\n\\nPhylogeny refers to the evolutionary history and relationship between biological lineages related by common descent. Reticulate evolution refers to the origination of lineages through the complete or partial merging of ancestor lineages. Networks may be used to represent lineage independence events in non-treelike phylogenetic processes.\\n\\nThe methodology for reconstructing networks is still in development. Here we explore two methods for reconstructing rooted explicit phylogenetic networks, PhyloNetworks and Phylonet, which employ computationally expensive and time consuming algorithms. The construction of phylogenetic networks follows a coordinated processing flow of data sets analyzed and processed by the coordinated execution of a set of different programs, packages, libraries or pipelines, called workflow activities. \\n\\nIn view of the complexity in modeling network experiments, the present work introduces a workflow for phylogenetic network analyses coupled to be executed in High-Performance Computing (HPC) environments. The workflow aims to integrate well-established software, pipelines and scripts, implementing a challenging task since these tools do not consistently profit from the HPC environment, leading to an increase in the expected makespan and idle computing resources.\\n\\n## Requirements\\n\\n1. Python >= 3.8\\n   1. Biopython >= 1.75\\n   2. Pandas >= 1.3.2\\n   3. Parsl >= 1.0\\n3. Raxml >= 8.2.12\\n4. Astral  >= 5.7.1\\n5. SnaQ (PhyloNetworks) >= 0.13.0\\n6. MrBayes >= 3.2.7a\\n7. BUCKy >=  1.4.4\\n8. Quartet MaxCut >= 2.10\\n9. PhyloNet >= 3.8.2\\n10. Julia >= 1.4.1\\n11. IQTREE >= 2.0\\n\\n\\n## How to use\\n\\n### Setting up the framework\\n\\nThe framework uses a file to get all the needed parameters. For default it loads the file *default.ini* in the config folder, but you can explicitly load other files using the argument ``-s name_of_the_file``, *e.g.* ``-s config/test.ini``.\\n\\n* Edit *parl.env* with the environment variables you may need, such as modules loadeds in SLURM\\n* Edit *work.config* with the directories of your phylogeny studies (the framework receives as input a set of homologous gene alignments of species in the nexus format).\\n* Edit *default.ini* with the path for each of the needed softwares and the parameters of the execution provider.\\n\\nFor default, the execution logs are created in the ``runinfo`` folder. To change it you can use the `-r folder_path` parameter.\\n\\n#### Contents of the configuration file\\n\\n* General settings\\n\\n```ini\\n[GENERAL]\\nExecutionProvider = SLURM\\nScriptDir \\t\\t= ./scripts\\nEnviron\\t\\t\\t= config/parsl.env\\nWorkload\\t\\t= config/work.config\\nNetworkMethod   = MP\\nTreeMethod      = RAXML\\nBootStrap       = 1000\\n```\\n\\n1. The framework can be executed in a HPC environment using the Slurm resource manager using the parameter ``ExecutionProvider`` equals to ``SLURM`` or locally with ``LOCAL``. \\n2. The path of the scripts folder is assigned  in ``ScriptDir``. It\\'s recommended to use the absolute path to avoid errors.\\n3. The ``Environ`` parameter contains the path of the file used to set environment variables. More details can be seen below.\\n4. In ``Workload`` is the path of the experiments that will be performed.\\n5. ``NetworkMethod`` and ``TreeMethod`` are the default network and tree methods that will be used to perform the workloads\\' studies.\\n6. ``Bootstrap`` is the parameter used in all the software that use bootstrap (RAxML, IQTREE and ASTRAL)\\n\\n* Workflow execution settings\\n \\n  When using SLURM, these are the needed parameters:\\n  ```ini\\n  [WORKFLOW]\\n  Monitor\\t\\t\\t= False\\n  PartCore\\t= 24\\n  PartNode\\t= 1\\n  Walltime\\t= 00:20:00\\n  ```\\n\\n  1. ``Monitor`` is a parameter to use parsl\\'s monitor module in HPC environment. It can be *true* or *false*. If you want to use it, it\\'s necessary to set it as *true* and manually change the address in ``infra_manager.py``\\n  2. If you are using it in a HPC environment (using SLURM), the framework is going to submit in a job. ``PartCore`` is the number of cores of the node; ``PartNode`` is the number of nodes of the partition; and the ``Walltime`` parameter is the maximum amount of time the job will be able to run.\\n\\n  However, if the the desired execution method is the LocalProvider, _i.e._ the execution is being performed in your own machine, only these parameters are necessary:\\n\\n  ```ini\\n  [WORKFLOW]\\n  Monitor\\t\\t\\t= False\\n  MaxCore\\t= 6\\n  CoresPerWorker\\t= 1\\n\\n  ```\\n\\n* RAxML settings\\n\\n  ```ini\\n  [RAXML]\\n  RaxmlExecutable = raxmlHPC-PTHREADS\\n  RaxmlThreads \\t= 6\\n  RaxmlEvolutionaryModel = GTRGAMMA --HKY85\\n  ```\\n\\n* IQTREE settings\\n\\n  ```ini\\n  [IQTREE]\\n  IqTreeExecutable = iqtree2\\n  IqTreeEvolutionaryModel = TIM2+I+G \\n  IqTreeThreads = 6\\n  ```\\n\\n* ASTRAL settings\\n\\n  ```ini\\n  [ASTRAL]\\n  AstralExecDir \\t= /opt/astral/5.7.1\\n  AstralJar \\t\\t= astral.jar\\n  ```\\n\\n* PhyloNet settings\\n\\n  ```ini\\n  [PHYLONET]\\n  PhyloNetExecDir \\t= /opt/phylonet/3.8.2/\\n  PhyloNetJar \\t\\t= PhyloNet.jar\\n  PhyloNetThreads     = 6\\n  PhyloNetHMax        = 3\\n  PhyloNetRuns        = 5\\n  ```\\n\\n* SNAQ settings\\n\\n  ```ini\\n  [SNAQ]\\n  SnaqThreads\\t\\t= 6\\n  SnaqHMax        = 3\\n  SnaqRuns        = 3\\n  ```\\n\\n* Mr. Bayes settings\\n\\n  ```ini\\n  [MRBAYES]\\n  MBExecutable\\t= mb\\n  MBParameters\\t= set usebeagle=no beagledevice=cpu beagleprecision=double; mcmcp ngen=100000 burninfrac=.25 samplefreq=50 printfreq=10000 diagnfreq=10000 nruns=2 nchains=2 temp=0.40 swapfreq=10\\n  ```\\n\\n* Bucky settings\\n\\n  ```ini\\n  [BUCKY]\\n  BuckyExecutable = bucky\\n  MbSumExecutable = mbsum\\n  ```\\n\\n* Quartet MaxCut\\n\\n  ```ini\\n  QUARTETMAXCUT]\\n  QmcExecDir       = /opt/quartet/\\n  QmcExecutable    = find-cut-Linux-64\\n  ```\\n\\n#### Workload file\\n\\nFor default the workload file is ``work.config`` in the *config* folder. The file contains the absolute paths of the experiment\\'s folders.\\n\\n```\\n/home/rafael.terra/Biocomp/data/Denv_1\\n```\\n\\nYou can comment folders using the # character in the beginning of the path. *e. g.* ``#/home/rafael.terra/Biocomp/data/Denv_1``. That way the framework won\\'t read this path.\\n\\nYou can also run a specific flow for a path using ``@TreeMethod|NetworkMethod`` in the end of a path. Where *TreeMethod* can be RAXML, IQTREE or MRBAYES and *NetworkMethod* can be MPL or MP (case sensitive). The supported flows are: ``RAXML|MPL``, ``RAXML|MP``, ``IQTREE|MPL``, ``IQTREE|MP`` and ``MRBAYES|MPL``. For example:\\n\\n```\\n/home/rafael.terra/Biocomp/data/Denv_1@RAXML|MPL\\n```\\n\\n#### Environment file\\n\\nThe environment file contains all the environment variables (like module files used in SLURM) used during the framework execution. Example:\\n\\n```sh\\nmodule load python/3.8.2\\nmodule load raxml/8.2_openmpi-2.0_gnu\\nmodule load java/jdk-12\\nmodule load iqtree/2.1.1\\nmodule load bucky/1.4.4\\nmodule load mrbayes/3.2.7a-OpenMPI-4.0.4\\nsource /scratch/app/modulos/julia-1.5.1.sh\\n```\\n\\n#### Experiment folder\\n\\nEach experiment folder needs to have a *input folder* containing a *.tar.gz* compressed file and a *.json* with the following content. **The framework considers that there is only one file of each extension in the input folder**.\\n\\n```json\\n{\\n\\t\"Mapping\":\"\",\\n\\t\"Outgroup\":\"\"\\n}\\n```\\n\\nWhere ``Mapping`` is a direct mapping of the taxon, when there are multiple alleles per species, in the format ``species1:taxon1,taxon2;species2:taxon3,taxon4`` *(white spaces are not supported)* and ``Outgroup`` is the taxon used to root the network. The Mapping parameter is optional (although it has to be in the json file without value), but the outgroup is obligatory. It\\'s important to say that the flow *MRBAYES|MPL* doesn\\'t support multiple alleles per species. Example:\\n\\n```json\\n{\\n  \"Mapping\": \"dengue_virus_type_2:FJ850082,FJ850088,JX669479,JX669482,JX669488,KP188569;dengue_virus_type_3:FJ850079,FJ850094,JN697379,JX669494;dengue_virus_type_1:FJ850073,FJ850084,FJ850093,JX669465,JX669466,JX669475,KP188545,KP188547;dengue_virus_type_4:JN559740,JQ513337,JQ513341,JQ513343,JQ513344,JQ513345,KP188563,KP188564;Zika_virus:MH882543\", \\n  \"Outgroup\": \"MH882543\"\\n}\\n```\\n\\n\\n## Running the framework\\n\\n* In a local machine:\\n\\n  After setting up the framework, just run ``python3 parsl_workflow.py``.\\n  \\n* In a SLURM environment:\\n\\n  Create an submition script that inside contains: ``python3 parsl_workflow.py``.\\n\\n  ```sh\\n  #!/bin/bash\\n  #SBATCH --time=15:00:00\\n  #SBATCH -e slurm-%j.err\\n  #SBATCH -o slurm-%j.out\\n  module load python/3.9.6\\n  cd /path/to/biocomp\\n  python3 parsl_workflow.py\\n  ```\\n\\nThe framework is under heavy development. If you notice any bug, please create an issue here on GitHub.\\n\\n### Running in a DOCKER container\\n\\nThe framework is also available to be used in Docker. It can be built from source or pushed from DockerHub.\\n\\n#### Building it from the source code\\n\\nAdapt the default settings file ``config/default.ini`` according to your machine, setting the number of threads and bootstrap. After that, run ``docker build -t hp2net .`` in the project\\'s root folder.\\n\\n#### Downloading it from Dockerhub\\n\\nThe docker image can also be downloaded from [Docker hub](https://hub.docker.com/repository/docker/rafaelstjf/hp2net/general). To do that, just run the command ``docker pull rafaelstjf/hp2net:main``\\n\\n#### Running\\n\\nThe first step to run the framework is to setup your dataset. To test if the framework is running without problems in your machine, you can use the [example datasets](example_data).\\n\\n![Alt text](docs/example_data.png)\\n\\nExtracting the ``example_data.zip`` file, a new folder called ``with_outgroup`` is created. This folder contain four datasets of DENV sequences.\\n\\nThe next step is the creation of the settings and workload files. For the settings file, download the [default.ini](config/default.ini) from this repository and change it to you liking (the path of all software are already configured to run on docker). The workload file is a text file containing the absolute path of the datasets, followed by the desired pipeline, as shown before in this document. Here for example purposes, the ``input.txt`` file was created.\\n\\n![Alt text](docs/example_files.png)\\n\\nWith all the files prepared, the framework can be executed from the ``example_data`` folder as following:\\n\\n``docker run --rm -v $PWD:$PWD rafaelstjf/hp2net:main -s $PWD/default.ini -w $PWD/input.txt``\\n\\n**Important:** the docker doesn\\'t save your logs, for that add the parameter: ``-r $PWD/name_of_your_log_folder``.\\n\\n---\\nIf you are running it on **Santos Dumont Supercomputer**, both downloading and execution of the docker container need to be performed from a submission script and executed using ``sg docker -c \"sbatch script.sh\"``. The snippet below shows an example of submission script.\\n\\n```sh\\n#!/bin/bash\\n#SBATCH --nodes=1\\n#SBATCH --ntasks-per-node=24\\n#SBATCH -p cpu_small\\n#SBATCH -J Hp2NET\\n#SBATCH --exclusive\\n#SBATCH --time=02:00:00\\n#SBATCH -e slurm-%j.err\\n#SBATCH -o slurm-%j.out\\n\\nDIR=\\'/scratch/pcmrnbio2/rafael.terra/WF_parsl/example_data\\'\\ndocker  pull rafaelstjf/hp2net:main\\n\\ndocker run --rm -v $DIR:$DIR rafaelstjf/hp2net:main -s ${DIR}/sdumont.ini -w ${DIR}/entrada.txt -r ${DIR}/logs\\n```\\n\\n## If you use it, please cite\\n\\nTerra, R., Coelho, M., Cruz, L., Garcia-Zapata, M., Gadelha, L., Osthoff, C., ... & Ocana, K. (2021, July). Gerência e Análises de Workflows aplicados a Redes Filogenéticas de Genomas de Dengue no Brasil. In *Anais do XV Brazilian e-Science Workshop* (pp. 49-56). SBC.\\n\\n**Also cite all the coupled software!**\\n\\n'\n",
      " '![workflow](https://github.com/naturalis/barcode-constrained-phylogeny/actions/workflows/python-package-conda.yml/badge.svg)\\n[![License: Apache-2.0](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10519081.svg)](https://doi.org/10.5281/zenodo.10519081)\\n\\n![Logo](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/logo-small.png?raw=true)\\n\\n# Bactria: BarCode TRee Inference and Analysis\\nThis repository contains code and data for building very large, topologically-constrained \\nbarcode phylogenies through a divide-and-conquer strategy. Such trees are useful as \\nreference materials for curating barcode data by detecting rogue terminals (indicating\\nincorrect taxonomic annotation) and in the comparable calculation of alpha and beta \\nbiodiversity metrics across metabarcoding assays. \\n\\nThe input data for the approach we develop here currently comes from BOLD data dumps. \\nThe international database [BOLD Systems](https://www.boldsystems.org/index.php) \\ncontains DNA barcodes for hundreds of thousands of species, with multiple barcodes per \\nspecies. The data dumps we use here are TSV files whose columns conform to the nascent\\nBCDM (barcode data model) vocabulary. As such, other data sources that conform to this\\nvocabulary could in the future be used as well, such as [UNITE](https://unite.ut.ee/).\\n\\nTheoretically, such data could be filtered and aligned per DNA marker to make \\nphylogenetic trees. However, there are two limiting factors: building very large \\nphylogenies is computationally intensive, and barcodes are not considered ideal for \\nbuilding big trees because they are short (providing insufficient signal to resolve large \\ntrees) and because they tend to saturate across large patristic distances.\\n\\n![concept](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/concept.png)\\n\\nBoth problems can be mitigated by using the \\n[Open Tree of Life](https://tree.opentreeoflife.org/opentree/argus/opentree13.4@ott93302) \\nas a further source of phylogenetic signal. The BOLD data can be split into chunks that \\ncorrespond to Open Tree of Life clades. These chunks can be made into alignments and \\nsubtrees. The OpenTOL can be used as a constraint in the algorithms to make these. The \\nchunks are then combined in a large synthesis by grafting them on a backbone made from \\nexemplar taxa from the subtrees. Here too, the OpenTOL is a source of phylogenetic \\nconstraint.\\n\\nIn this repository this concept is developed for both animal species and plant species.\\n\\n## Installation\\n\\nThe pipeline and its dependencies are managed using conda. On a linux or osx system, you \\ncan follow these steps to set up the `bactria` Conda environment using an `environment.yml` \\nfile and a `requirements.txt` file:\\n\\n1. **Clone the Repository:**  \\n   Clone the repository containing the environment files to your local machine:\\n   ```bash\\n   git clone https://github.com/naturalis/barcode-constrained-phylogeny.git\\n   cd barcode-constrained-phylogeny\\n   ```\\n2. **Create the Conda Environment:**\\n   Create the bactria Conda environment using the environment.yml file with the following \\n   command:\\n   ```bash\\n   conda env create -f workflow/envs/environment.yml\\n   ```\\n   This command will create a new Conda environment named bactria with the packages \\n   specified in the environment.yml file. This step is largely a placeholder because\\n   most of the dependency management is handled at the level of individual pipeline\\n   steps, which each have their own environment specification.\\n3. **Activate the Environment:**\\n   After creating the environment, activate it using the conda activate command:\\n   ```bash\\n   conda activate bactria\\n   ```\\n4. **Verify the Environment:**\\n   Verify that the bactria environment was set up correctly and that all packages were \\n   installed using the conda list command:\\n   ```bash\\n   conda list\\n   ```\\n   This command will list all packages installed in the active conda environment. You should \\n   see all the packages specified in the environment.yml file and the requirements.txt file.\\n\\n## How to run\\n\\nThe pipeline is implemented using snakemake, which is available within the conda \\nenvironment that results from the installation. Important before running the snakemake pipeline \\nis to change in [config/config.yaml](config/config.yaml) the number of threads available on your \\ncomputer. Which marker gene is used in the pipeline is also specified in the config.yaml (default \\nCOI-5P). Prior to execution, the BOLD data package to use (we used the \\n[release of 30 December 2022](https://www.boldsystems.org/index.php/datapackage?id=BOLD_Public.30-Dec-2022)) \\nmust be downloaded manually and stored in the [resources/](resources/) directory. If a BOLD release \\nfrom another date is used the file names in config.yaml need to be updated. \\n\\nHow to run the entire pipeline:\\n\\n```bash \\nsnakemake -j {number of threads} --use-conda\\n```\\n\\nSnakemake rules can be performed separately:\\n```bash \\nsnakemake -R {Rule} -j {number of threads} --use-conda\\n```\\n\\nEnter the same number at {number of threads} as you filled in previously in src/config.yaml.\\nIn {Rule} insert the rule to be performed.\\n\\nHere is an overview of all the rules in the Snakefile:\\n\\n![graphviz (1)](https://github.com/naturalis/barcode-constrained-phylogeny/blob/main/doc/dag.svg)\\n(zoomed view is available [here](https://raw.githubusercontent.com/naturalis/barcode-constrained-phylogeny/main/doc/dag.svg))\\n\\n## Repository layout\\n\\nBelow is the top-level layout of the repository. This layout is in line with \\n[community standards](https://snakemake.readthedocs.io/en/stable/snakefiles/deployment.html) and must be adhered to.\\nAll of these subfolders contains further explanatory READMEs to explain their contents in more detail.\\n\\n- [config](config/) - configuration files\\n- [doc](doc/) - documentation and background literature\\n- [logs](logs/) - where log files are written during pipeline runtime\\n- [resources](resources/) - external data resources (from BOLD and OpenTree) are downloaded here\\n- [results](results/) - intermediate and final results are generated here\\n- [workflow](workflow/) - script source code and driver snakefile \\n\\n## License\\n\\n&copy; 2023 Naturalis Biodiversity Center\\n\\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except \\nin compliance with the License. You may obtain a copy of the License at\\n\\n[http://www.apache.org/licenses/LICENSE-2.0](http://www.apache.org/licenses/LICENSE-2.0)\\n   \\nUnless required by applicable law or agreed to in writing, software distributed under the License \\nis distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express \\nor implied. See the License for the specific language governing permissions and limitations under \\nthe License.'\n",
      " 'Lysozyme in water full COMPSs application, using dataset_small'\n",
      " 'Wordcount merge version COMPSs application'\n",
      " 'Wordcount reduce version COMPSs application'\n",
      " 'Cholesky factorisation COMPSs application' 'K-means COMPSs application'\n",
      " 'Cluster Comparison COMPSs application'\n",
      " 'Lysozyme in water sample COMPSs application'\n",
      " \"Run velocyto to get loom with counts of spliced and unspliced. It will extract the 'barcodes' from the bundled outputs.\"\n",
      " \"![](https://github.com/AusARG/pipesnake/blob/main/docs/images/pipesnake_Logo.png)\\n&nbsp;\\n\\nWelcome to the *pipesnake*.  \\nLet's get started. \\n\\n---\\n\\n# Introduction\\n\\n**pipesnake** is a bioinformatics best-practice analysis pipeline for phylogenomic reconstruction starting from short-read 'second-generation' sequencing data.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies.\\n\\n---\\n\\n# Motivation + Project Background\\n\\nWe developed *pipesnake* as part of the [***Aus***tralian ***A***mphibian and ***R***eptile ***G***enomics](https://ausargenomics.com/) (*AusARG*) initiative.  **AusARG** is a national collaborative project aiming to facilitate the development of genomics resources for Australia's unique amphibian and reptile fauna. This pipeline was developed specifically as part of the *AusARG Phylogenomics Working Group* with the goal of collecting a consistent set of phylogenomic data for all of Australia's frogs and reptiles, under similar assembly, alignment, and tree estimation procedures. \\n\\n*pipesnake* is however, applicable to much broader phylogenomic questions, and is appropriate for processing exon-capture or transcriptomic data, so long as the **input is second-generation (short-read) data**. \"\n",
      " '## EBP-Nor Genome Assembly pipeline\\n\\nThis repository contains the EBP-Nor genome assembly pipeline. This pipeline is implemented in snakemake.\\nThis pipeline is developed to create haplotype-resolved genome assemblies from PacBio HiFi reads and HiC reads,\\nand is primarly designed for diploid eukaryotic organisms. The pipeline is designed to work on a linux cluster with slurm as workload manager.\\n\\n## Requirements & Setup\\n\\nSome software need to be configured/installed before the pipeline can be run\\n\\n### Conda setup\\n\\nMost required software, including snakemake itself, can be installed using [conda](https://conda.io/projects/conda/en/latest/user-guide/install/index.html).\\n\\nOnce conda is installed, you can create a new environment containing most necessary software from the provided asm_pipeline.yaml file as follows:\\n\\n```shell\\nconda create -n asm_pipeline --file=worfklow/envs/asm_pipeline.yaml\\n```\\n\\n### Other software setup\\n\\nThe following software need to be installed manually:\\n\\n- KMC v3.1.1 (https://github.com/tbenavi1/KMC)\\n- HiFiAdapterFilt (https://github.com/sheinasim/HiFiAdapterFilt)\\n- Oatk (https://github.com/c-zhou/oatk)\\n- OatkDB (https://github.com/c-zhou/OatkDB)\\n- NCBI FCS-Adaptor (https://github.com/ncbi/fcs/wiki/FCS-adaptor)\\n- NCBI FCS-GX (https://github.com/ncbi/fcs/wiki/FCS-GX)\\n\\nPlease refer to their respective installation instructions to properly install them. You will need to privide the installation paths of these software to the config file (see Parameter section).\\n\\n### BUSCO database setup\\n\\nAs in general, computing nodes are not connected to the internet, BUSCO lineage datasets need to be downloaded manually before running the pipeline.\\nThis can easily be done by running\\n\\n```shell\\nbusco --download eukaryota\\n```\\n\\nYou will need to specify the folder where you downloaded the busco lineages in the config file (see Parameter section).\\n\\n### Data\\n\\nThis pipeline is created for using PacBio HiFi reads together with paired-end Hi-C data.\\nYou will need to specify the absolute paths to these files in the config file (see Parameters section).\\n\\n### Parameters\\n\\nThe necessary config files for running the pipeline can be found in the config folder.\\n\\nGeneral snakemake and cluster submission parameters are defined in ```config/config.yaml```, \\ndata- and software-specfic parameters are defined in ```config/asm_params.yaml```.\\n\\nFirst, define the paths of the input files you want to use:\\n- pacbio: path to the location of the PacBio HiFi reads (```.fastq.gz```)\\n- hicF and hicR: path to the forward and reverse HiC reads respectively\\n\\nFor software not installed by conda, the installation path needs to be provided to the Snakemake pipeline by editing following parameters in the ```config/asm_params.yaml```:\\n\\n- Set the \"adapterfilt_install_dir\" parameter to the installation path of HiFiAdapterFilt\\n- Set the \"KMC_path\" parameter to the installation path of KMC\\n- Set the \"oatk_dir\" parameter to the installation path of oatk\\n- Set the \"oatk_db\" parameter to the directory where you downloaded the oatk_db files\\n- Set the \"fcs_path\" parameter to the location of the ```run_fcsadaptor.sh``` and ```fcs.py``` scripts\\n- Set the \"fcs_adaptor_image\" and \"fcs_gx_image\" parameters to the paths to the ```fcs-adaptor.sif``` and ```fcs-gx.sif``` files respectively\\n- Set the \"fcs_gx_db\" parameter to the path of the fcs-gx database\\n\\nA couple of other parameters need to be verified as well in the config/asm_params.yaml file before running the pipeline:\\n\\n- The location of the input data (```input_dir```) should be set to the folder containing the input data.\\n- The location of the downloaded busco lineages (```busco_db_dir```) should be set to the folder containing the busco lineages files downloaded earlier\\n- The required BUSCO lineage for running the BUSCO analysis needs to set (```busco_lineage``` parameter). Run ```busco --list-datasets``` to get an overview of all available datasets.\\n- The required oatk lineage for running organelle genome assembly (```oatk_lineage``` parameter). Check https://github.com/c-zhou/OatkDB for an overview of available lineages.\\n- A boolean value wether the species is plant (for plastid prediction) or not (```oatk_isPlant```; set to either True or False)\\n- The NCBI taxid of your species, required for the decontamination step (```taxid``` parameter)\\n\\n## Usage and run modes\\n\\nBefore running, make sure to activate the conda environment containing the necessary software: ```conda activate asm_assembly```.\\nTo run the pipeline, run the following command:\\n\\n```\\nsnakemake --profile config/ --configfile config/asm_params.yaml --snakefile workflow/Snakefile {run_mode}\\n```\\n\\nIf you invoke the snakemake command in another directory than the one containing the ```workflow``` and ```config``` folders, \\nor if the config files (```config.yaml``` and ```asm_params.yaml```) are in another location, you need to specify their correct paths on the command line.\\n\\nThe workflow parameters can be modified in 3 ways:\\n- Directly modifying the ```config/asm_parameters.yaml``` file\\n- Overriding the default parameters on the command line: ```--config parameter=new_value```\\n- Overriding the default parameters using a different yaml file: ```--configfile path_to_parameters.yaml```\\n\\nThe pipeline has different runing modes, and the run mode should always be the last argument on the command line:\\n\\n- \"all\" (default): will run the full workflow including pre-assembly (genomescope & smudgeplot), assembly, scaffolding, decontamination, and organelle assembly\\n- \"pre_assembly\": will run only the pre-assembly steps (genomescope & smudgeplot)\\n- \"assembly\": will filter the HiFi reads and assemble them using hifiasm (also using the Hi-C reads), and run busco\\n- \"scaffolding\": will run all steps necessary for scaffolding (filtering, assembly, HiC filtering, scaffolding, busco), but without pre-assembly\\n- \"decontamination\": will run assembly, scaffolding, and decontamination, but without pre-assembly and busco analyses\\n- \"organelles\": will run only organnelle genome assembly\\n\\n## Output\\n\\nAll generated output will be present in the \"results\" directory, which will be created in the folder from where you invoke the snakemake command.\\nThis results directory contains different subdirectories related to the different steps in the assembly:\\n- results/pre_assembly: genomescope and smudgeplot output (each in its own subfolder)\\n- results/assembly: Hifiasm assembly output and corresponding busco results\\n- results/scaffolding: scaffolding output, separated in two folders:\\n  - meryl: meryl databases used for filtering HiC reads\\n  - yahs: scaffolding output, including final scaffolds and their corresponding busco results\\n- results/decontamination: decontamination output of the final scaffolded assembly\\n- results/organelles: assembled organellar genomes\\n\\nAdditionally, a text file containing all software versions will be created in the specified input directory.\\nThe log files of the different steps in the workflow can be found in the ```logs``` directory that will be created.'\n",
      " '**Name:** Random Forest \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum4  \\nThis is an example of Random Forest algorithm from dislib. To show the usage, the code generates a synthetical input matrix.\\nThe results are printed by screen.\\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " 'Refining Genome Annotations with Apollo'\n",
      " 'Assemble long reads with Flye, then view assembly statistics and assembly graph'\n",
      " '# Purge Duplicate Contigs\\n\\nPurge contigs marked as duplicates by purge_dups in a single haplotype(could be haplotypic duplication or overlap duplication)\\nThis workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5)\\n\\n## Inputs\\n\\n1. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow)\\n2. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow)\\n3. Assembly to purge (e.g. hap1) [fasta] (Generated by the contigging workflow)\\n4. K-mer database [meryldb]  (Generated by the k-mer profiling workflow)\\n5. Assembly to leave alone (used for merqury statistics) (e.g. hap2) [fasta] (Generated by the contigging workflow)\\n6. Estimated Genome Size [txt]\\n7. Database for busco lineage (recommended: latest)\\n8. Busco lineage (recommended: vertebrata)\\n9. Name of un-altered assembly\\n10. Name of purged assembly\\n\\n\\n## Outputs\\n\\n1. Haplotype 1 purged assembly (Fasta and gfa)\\n2. Haplotype 2 purged assembly (Fasta and gfa)\\n3. QC: BUSCO report for both assemblies\\n4. QC: Merqury report for both assemblies\\n5. QC: Assembly statistics for both assemblies\\n6. QC: Nx plot for both assemblies\\n7. QC: Size plot for both assemblies'\n",
      " 'Masking repeats in a genome using RepeatMasker'\n",
      " 'Structural and functional genome annotation with Funannotate'\n",
      " 'Functional annotation of protein sequences'\n",
      " 'From Copernicus Sentinel 5P data to panoply visualization of volcanic activity impact to atmosphere'\n",
      " 'Finding potential muon stopping sites in crystalline copper'\n",
      " 'Calculating and visualizing marine biodiversity indicators'\n",
      " 'NDVI data with OpenEO to time series visualisation with HoloViz'\n",
      " 'This is a Galaxy workflow for performing molecular dynamics simulations and analysis with flavivirus helicases bound to a ligand/drug molecule. \\nThe associated input files can be found at:\\nhttps://zenodo.org/records/7493015\\nThe associated output files can be found at:\\nhttps://zenodo.org/records/7850935'\n",
      " 'This is a Galaxy workflow for performing molecular dynamics simulations and analysis with flavivirus helicases in the Apo or unbound state. The associated input files can be found at: https://zenodo.org/records/7493015 The associated output files can be found at: https://zenodo.org/records/7850935'\n",
      " 'This is a Galaxy workflow for performing molecular dynamics simulations and analysis with coronavirus helicases bound to a ligand/drug molecule. The associated input files can be found at: https://zenodo.org/records/7492987. The associated output files can be found at: https://zenodo.org/records/7851000.'\n",
      " 'This is a Galaxy workflow for performing molecular dynamics simulations and analysis with coronavirus helicases in the Apo or unbound state. The associated input files can be found at: https://zenodo.org/records/7492987. The associated output files can be found at: https://zenodo.org/records/7851000.'\n",
      " 'Protype demonstrator of a workflow reducing HESS and INTEGRAL/SPI-ACS data to common Light Curve format and combining the lightcurves into a multi-wavelength observation.'\n",
      " 'This workflow performs segmentation and counting of cell nuclei using fluorescence microscopy images. The segmentation step is performed using Otsu thresholding (Otsu, 1979). The workflow is based on the tutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/imaging-introduction/tutorial.html'\n",
      " '# CMIP tutorial using BioExcel Building Blocks (biobb)\\n\\n***\\n\\nThis tutorial aims to illustrate the process of computing **classical molecular interaction potentials** from **protein structures**, step by step, using the **BioExcel Building Blocks library (biobb)**. Examples shown are **Molecular Interaction Potentials (MIPs) grids, protein-protein/ligand interaction potentials, and protein titration**. The particular structures used are the **Lysozyme** protein (PDB code [1AKI](https://www.rcsb.org/structure/1aki)), and a MD simulation of the complex formed by the **SARS-CoV-2 Receptor Binding Domain and the human Angiotensin Converting Enzyme 2** (PDB code [6VW1](https://www.rcsb.org/structure/6vw1)).\\n\\nThe code wrapped is the ***Classical Molecular Interaction Potentials (CMIP)*** code:\\n\\n**Classical molecular interaction potentials: Improved setup procedure in molecular dynamics simulations of proteins.**\\n*Gelpí, J.L., Kalko, S.G., Barril, X., Cirera, J., de la Cruz, X., Luque, F.J. and Orozco, M. (2001)*\\n*Proteins, 45: 428-437. [https://doi.org/10.1002/prot.1159](https://doi.org/10.1002/prot.1159)*\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " '# Molecular Structure Checking using BioExcel Building Blocks (biobb)\\n\\n***\\n\\nThis tutorial aims to illustrate the process of **checking** a **molecular structure** before using it as an input for a **Molecular Dynamics** simulation. The workflow uses the **BioExcel Building Blocks library (biobb)**. The particular structure used is the crystal structure of **human Adenylate Kinase 1A (AK1A)**, in complex with the **AP5A inhibitor** (PDB code [1Z83](https://www.rcsb.org/structure/1z83)).  \\n\\n**Structure checking** is a key step before setting up a protein system for **simulations**. A number of **common issues** found in structures at **Protein Data Bank** may compromise the success of the **simulation**, or may suggest that longer **equilibration** procedures are necessary.\\n\\nThe **workflow** shows how to:\\n\\n- Run **basic manipulations on structures** (selection of models, chains, alternative locations\\n- Detect and fix **amide assignments** and **wrong chiralities**\\n- Detect and fix **protein backbone** issues (missing fragments, and atoms, capping)\\n- Detect and fix **missing side-chain atoms**\\n- **Add hydrogen atoms** according to several criteria\\n- Detect and classify **atomic clashes**\\n- Detect possible **disulfide bonds (SS)**\\n\\nAn implementation of this workflow in a **web-based Graphical User Interface (GUI)** can be found in the [https://mmb.irbbarcelona.org/biobb-wfs/](https://mmb.irbbarcelona.org/biobb-wfs/) server (see [https://mmb.irbbarcelona.org/biobb-wfs/help/create/structure#check](https://mmb.irbbarcelona.org/biobb-wfs/help/create/structure#check)).\\n\\n***\\n\\n## Copyright & Licensing\\nThis software has been developed in the [MMB group](http://mmb.irbbarcelona.org) at the [BSC](http://www.bsc.es/) & [IRB](https://www.irbbarcelona.org/) for the [European BioExcel](http://bioexcel.eu/), funded by the European Commission (EU H2020 [823830](http://cordis.europa.eu/projects/823830), EU H2020 [675728](http://cordis.europa.eu/projects/675728)).\\n\\n* (c) 2015-2023 [Barcelona Supercomputing Center](https://www.bsc.es/)\\n* (c) 2015-2023 [Institute for Research in Biomedicine](https://www.irbbarcelona.org/)\\n\\nLicensed under the\\n[Apache License 2.0](https://www.apache.org/licenses/LICENSE-2.0), see the file LICENSE for details.\\n\\n![](https://bioexcel.eu/wp-content/uploads/2019/04/Bioexcell_logo_1080px_transp.png \"Bioexcel\")'\n",
      " 'PyCOMPSs implementation of Probabilistic Tsunami Forecast (PTF). PTF explicitly treats data- and forecast-uncertainties, enabling alert level definitions according to any predefined level of conservatism, which is connected to the average balance of missed-vs-false-alarms. Run of the Boumerdes-2003 event test-case with 1000 scenarios, 8h tsunami simulation for each and forecast calculations for partial and full ensembles with focal mechanism and tsunami data updates.'\n",
      " 'Lysozyme in water full COMPSs application run at MareNostrum IV, using full dataset with two workers'\n",
      " 'PyCOMPSs implementation of Probabilistic Tsunami Forecast (PTF). PTF explicitly treats data- and forecast-uncertainties, enabling alert level definitions according to any predefined level of conservatism, which is connected to the average balance of missed-vs-false-alarms. Run of the Kos-Bodrum 2017 event test-case with 1000 scenarios, 8h tsunami simulation for each and forecast calculations for partial and full ensembles with focal mechanism and tsunami data updates.'\n",
      " 'This project is about the automated quantification of wound healing in high-throughput microscopy scratch assays.'\n",
      " '# score-assemblies\\n\\nA Snakemake-wrapper for evaluating *de novo* bacterial genome assemblies, e.g. from Oxford Nanopore (ONT) or Illumina sequencing.\\n\\nThe workflow includes the following programs:\\n* [pomoxis](https://github.com/nanoporetech/pomoxis) assess_assembly and assess_homopolymers\\n* dnadiff from the [mummer](https://mummer4.github.io/index.html) package\\n* [NucDiff](https://github.com/uio-cels/NucDiff/)\\n* [QUAST](http://quast.sourceforge.net/quast)\\n* [BUSCO](https://busco.ezlab.org/)\\n* [ideel](https://github.com/mw55309/ideel/), which uses [prodigal](https://github.com/hyattpd/Prodigal) and [diamond](https://github.com/bbuchfink/diamond)\\n* [bakta](https://github.com/oschwengers/bakta)\\n\\n\\n'\n",
      " '# ont-assembly-snake\\n\\nA Snakemake wrapper for easily creating *de novo* bacterial genome assemblies from Oxford Nanopore (ONT) sequencing data, and optionally Illumina data,\\nusing any combination of read filtering, assembly, long and short read polishing, and reference-based polishing.\\n\\n## Included programs\\n\\n| read filtering | assembly | long read polishing | short read polishing | reference-based polishing |\\n| --- | --- | --- | --- | --- |\\n| [Filtlong](https://github.com/rrwick/Filtlong)<br/> [Rasusa](https://github.com/mbhall88/rasusa) | [Flye](https://github.com/fenderglass/Flye)<br/> [raven](https://github.com/lbcb-sci/raven)<br/> [miniasm](https://github.com/lh3/miniasm)<br/> [Unicycler](https://github.com/rrwick/Unicycler)<br/> [Canu](https://github.com/marbl/canu)  | [racon](https://github.com/lbcb-sci/racon)<br/> [medaka](https://github.com/nanoporetech/medaka) | [pilon](https://github.com/broadinstitute/pilon/wiki)<br/> [Polypolish](https://github.com/rrwick/Polypolish)<br/> [POLCA](https://github.com/alekseyzimin/masurca#polca) | [Homopolish](https://github.com/ythuang0522/homopolish)<br/> [proovframe](https://github.com/thackl/proovframe) | \\n\\n'\n",
      " 'The workflow takes raw ONT reads and trimmed Illumina WGS paired reads collections, and the estimated genome size and Max depth (both calculated from WF1) to run Flye and subsequently polish the assembly with HyPo. It produces collapsed assemblies (unpolished and polished) and runs all the QC analyses (gfastats, BUSCO, and Merqury).'\n",
      " 'The workflow takes raw ONT reads and trimmed Illumina WGS paired reads collections, the ONT raw stats table (calculated from WF1) and the estimated genome size (calculated from WF1) to run NextDenovo and subsequently polish the assembly with HyPo. It produces collapsed assemblies (unpolished and polished) and runs all the QC analyses (gfastats, BUSCO, and Merqury).'\n",
      " 'dada2 amplicon analysis for paired end data\\n\\nThe workflow has three main outputs: \\n- the sequence table (output of makeSequenceTable)\\n- the taxonomy (output of assignTaxonomy)\\n- the counts which allow to track the number of sequences in the samples through the steps (output of sequence counts)'\n",
      " '# skim2mt\\n\\n**skim2mt** is a snakemake pipeline for the batch assembly, annotation, and phylogenetic analysis of mitochondrial genomes from low coverage genome skims. The pipeline was designed to work with sequence data from museum collections. However, it should also work with genome skims from recently collected samples.\\n\\n## Contents\\n - [Setup](#setup)\\n - [Example data](#example-data)\\n - [Input](#input)\\n - [Output](#output)\\n - [Filtering contaminants](#filtering-contaminants)\\n - [Assembly and annotation only](#assembly-and-annotation-only)\\n - [Running your own data](#running-your-own-data)\\n - [Getting help](#getting-help)\\n - [Citations](#citations)\\n\\n## Setup\\n\\nThe pipeline is written in Snakemake and uses conda and singularity to install the necessary tools.\\n\\nIt is *strongly recommended* to install conda using Mambaforge. See details here https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\\n\\nOnce conda is installed, you can pull the github repo and set up the base conda environment.\\n\\n```\\n# get github repo\\ngit clone https://github.com/o-william-white/skim2mt\\n\\n# change dir\\ncd skim2mt\\n\\n# setup conda env\\nconda env create -n snakemake -f workflow/envs/conda_env.yaml\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Example data\\n\\nBefore you run your own data, it is recommended to run the example datasets provided . This will confirm there are no user-specific issues with the setup and it also installs all the dependencies. The example data includes simulated mitochondrial data from 25 different butterfly species. \\n\\nTo run the example data, use the code below. **Note that you need to change the user email to your own address**. The email is required by the Bio Entrez package to fetch reference sequences. The first time you run the pipeline, it will take some time to install each of the conda environments, so it is a good time to take a tea break :).\\n```\\nconda activate snakemake\\n\\nsnakemake \\\\\\n   --cores 4 \\\\\\n   --use-conda \\\\\\n   --use-singularity \\\\ \\n   --config user_email=user@example_email.com\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Input\\n\\nSnakemake requires a `config.yaml` and `samples.csv` to define input parameters and sequence data for each sample. \\n\\nFor the example data provided, the config file is located here `config/config.yaml` and it looks like this:\\n```\\n# path to sample sheet csv with columns for ID,forward,reverse,taxid,seed,gene\\nsamples: config/samples.csv\\n\\n# user email\\nuser_email: user@example_email.com\\n\\n# getorganelle reference (go_fetch, custom)\\ngo_reference: go_fetch\\n\\n# forward adapter\\nforward_adapter: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA\\n\\n# reverse adapter\\nreverse_adapter: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\\n\\n# fastp deduplication (True/False)\\nfastp_dedup: True\\n\\n# mitos refseq database (refseq39, refseq63f, refseq63m, refseq63o, refseq89f, refseq89m, refseq89o)\\nmitos_refseq: refseq39\\n\\n# mito code (2 = Vertebrate, 4 = Mold, 5 = Invertebrate, 9 = Echinoderm, 13 = Ascidian, 14 = Alternative flatworm)\\nmitos_code: 5\\n\\n# alignment trimming method to use (gblocks or clipkit)\\nalignment_trim: gblocks\\n\\n# alignment missing data threshold for alignment (0.0 - 1.0)\\nmissing_threshold: 0.5\\n\\n# name of outgroup sample (optional)\\n# use \"NA\" if there is no obvious outgroup\\n# if more than one outgroup use a comma separated list i.e. \"sampleA,sampleB\"\\noutgroup: Eurema_blanda\\n\\n# plot dimensions (cm)\\nplot_height: 20\\nplot_width: 20\\n```\\n\\nThe example samples.csv file is located here `config/samples.csv` and it looks like this (note that the seed and gene columns are only required if the custom getorganelle database option is specified in the config file):\\n\\n\\n ID | forward | reverse | taxid | seed | gene \\n----|---------|---------|-------|------|------\\nAdelpha_iphiclus | .test/reads/Adelpha_iphiclus_1.fq.gz | .test/reads/Adelpha_iphiclus_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAnartia_jatrophae_saturata | .test/reads/Anartia_jatrophae_saturata_1.fq.gz | .test/reads/Anartia_jatrophae_saturata_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAraschnia_levana | .test/reads/Araschnia_levana_1.fq.gz | .test/reads/Araschnia_levana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAuzakia_danava | .test/reads/Auzakia_danava_1.fq.gz | .test/reads/Auzakia_danava_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nBaeotus_beotus | .test/reads/Baeotus_beotus_1.fq.gz | .test/reads/Baeotus_beotus_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nCatacroptera_cloanthe | .test/reads/Catacroptera_cloanthe_1.fq.gz | .test/reads/Catacroptera_cloanthe_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nChalinga_pratti | .test/reads/Chalinga_pratti_1.fq.gz | .test/reads/Chalinga_pratti_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nDiaethria_gabaza_eupepla | .test/reads/Diaethria_gabaza_eupepla_1.fq.gz | .test/reads/Diaethria_gabaza_eupepla_2.fq.gz | 127268 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nDoleschallia_melana | .test/reads/Doleschallia_melana_1.fq.gz | .test/reads/Doleschallia_melana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nEurema_blanda | .test/reads/Eurema_blanda_1.fq.gz | .test/reads/Eurema_blanda_2.fq.gz | 42450 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nHypolimnas_usambara | .test/reads/Hypolimnas_usambara_1.fq.gz | .test/reads/Hypolimnas_usambara_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nJunonia_villida | .test/reads/Junonia_villida_1.fq.gz | .test/reads/Junonia_villida_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nKallima_paralekta | .test/reads/Kallima_paralekta_1.fq.gz | .test/reads/Kallima_paralekta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nKallimoides_rumia | .test/reads/Kallimoides_rumia_1.fq.gz | .test/reads/Kallimoides_rumia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nLitinga_cottini | .test/reads/Litinga_cottini_1.fq.gz | .test/reads/Litinga_cottini_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nMallika_jacksoni | .test/reads/Mallika_jacksoni_1.fq.gz | .test/reads/Mallika_jacksoni_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nModuza_procris | .test/reads/Moduza_procris_1.fq.gz | .test/reads/Moduza_procris_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nParasarpa_zayla | .test/reads/Parasarpa_zayla_1.fq.gz | .test/reads/Parasarpa_zayla_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nPhaedyma_columella | .test/reads/Phaedyma_columella_1.fq.gz | .test/reads/Phaedyma_columella_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nPrecis_pelarga | .test/reads/Precis_pelarga_1.fq.gz | .test/reads/Precis_pelarga_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nProtogoniomorpha_temora | .test/reads/Protogoniomorpha_temora_1.fq.gz | .test/reads/Protogoniomorpha_temora_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nSalamis_cacta | .test/reads/Salamis_cacta_1.fq.gz | .test/reads/Salamis_cacta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nSmyrna_blomfildia | .test/reads/Smyrna_blomfildia_1.fq.gz | .test/reads/Smyrna_blomfildia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nTacola_larymna | .test/reads/Tacola_larymna_1.fq.gz | .test/reads/Tacola_larymna_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nYoma_algina | .test/reads/Yoma_algina_1.fq.gz | .test/reads/Yoma_algina_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\n\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Output\\n\\nAll output files are saved to the `results` direcotry. Below is a table summarising all of the output files generated by the pipeline.\\n\\n| Directory             | Description               |\\n|-----------------------|---------------------------|\\n| fastqc_raw            | Fastqc reports for raw input reads |\\n| fastp                 | Fastp reports from quality control of raw reads |\\n| fastqc_qc             | Fastqc reports for quality controlled reads |\\n| go_fetch              | Optional output containing reference databasesused by GetOrganelle |\\n| getorganelle          | GetOrganelle output with a directory for each sample |\\n| assembled_sequence    | Assembled sequences selected from GetOrganelle output and renamed |\\n| seqkit                | Seqkit summary of each assembly |\\n| blastn                | Blastn output of each assembly |\\n| minimap               | Mapping output of quality filtered reads against each assembly |\\n| blobtools             | Blobtools assembly summary collating blastn and mapping output |\\n| assess_assembly       | Plots of annotations, mean depth, GC content and proportion mismatches |\\n| annotations           | Annotation outputs of mitos |\\n| summary               | Summary per sample (seqkit stats), contig (GC content, length, coverage, taxonomy and annotations) and annotated gene counts |\\n| annotated_genes  | Unaligned fasta files of annotated genes identified across all samples |\\n| mafft                 | Mafft aligned fasta files of annotated genes identified across all samples |\\n| mafft_filtered        | Mafft aligned fasta files after the removal of sequences based on a missing data threshold |\\n| alignment_trim        | Ambiguous parts of alignment removed using either gblocks or clipkit |\\n| iqtree                | Iqtree phylogenetic analysis of annotated genes |\\n| plot_tree             | Plots of phylogenetic trees |\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Filtering contaminants\\n\\nIf you are working with museum collections, it is possible that you may assemble and annotate sequences from contaminant/non-target species. *Contaminant sequences can be identified based on the blast search output or unusual placement in the phylogenetic trees* (see blobtools and plot_tree outputs). \\n\\nA supplementary python script `format_alignments.py `is provided to remove putative contaminants from alignments, and format the alignments for downstream phylogenetic analysis.\\n\\nFor example, let\\'s say we wanted to remove all sequences from the sample \"Kallima_paralekta\" and atp6 gene sequences, you could run the script as shown below. The script works by identifying and removing sequences that have names with  `Kallima_paralekta` or `atp6` in the sequence names. The filtered alignments are written to a new output directory `filter_alignments_output`.\\n\\n```\\npython workflow/scripts/format_alignments.py  \\\\\\n   --input results/mafft_filtered/ \\\\\\n   --cont Kallima_paralekta atp6 \\\\\\n   --output filter_alignments_output\\n```\\n\\n*Note that the output fasta files have been reformatted so each alignment file is named after the gene and each sequence is named after the sample.* This is useful if you would like to run our related pipeline **gene2phylo** for further phylogenetic analyses.\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Assembly and annotation only\\n\\nIf you are only interested in the assembly of mitochondrial sequences and annotation of genes without the phylogenetic analysis, you can stop the pipeline from running the gene alignment and phylogenetic analyses using the `--omit-from` parameter.\\n```\\nsnakemake \\\\\\n   --cores 4 \\\\\\n   --use-conda \\\\\\n   --use-singularity \\\\\\n   --config user_email=user@example_email.com \\\\\\n   --omit-from mafft\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Running your own data\\n\\nThe first thing you need to do is generate your own config.yaml and samples.csv files, using the files provided as a template.\\n\\nGetOrganelle requires reference data in the format of seed and gene reference fasta files. By default the pipeline uses a basic python script called go_fetch.py https://github.com/o-william-white/go_fetch to download and format reference data formatted for GetOrganelle. \\n\\ngo_fetch.py works by searching NCBI based on the NCBI taxonomy specified by the taxid column in the samples.csv file. Note that the seed and gene columns in the samples.csv file are only required if you want to provide your own custom GetOrganelle seed and gene reference databases. \\n\\nYou can use the default reference data for GetOrganelle, but I would recommend using custom reference databases where possible. See here for details of how to set up your own databases https://github.com/Kinggerm/GetOrganelle/wiki/FAQ#how-to-assemble-a-target-organelle-genome-using-my-own-reference\\n\\n## Getting help\\n\\nIf you have any questions, please do get in touch in the issues or by email o.william.white@gmail.com\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Citations\\n\\nIf you use the pipeline, please cite our bioarxiv preprint: https://doi.org/10.1101/2023.08.11.552985\\n\\nSince the pipeline is a wrapper for several other bioinformatic tools we also ask that you cite the tools used by the pipeline:\\n - Fastqc https://github.com/s-andrews/FastQC\\n - Fastp https://doi.org/10.1093/bioinformatics/bty560\\n - GetOrganelle https://doi.org/10.1186/s13059-020-02154-5\\n - Blastn https://doi.org/10.1186/1471-2105-10-421\\n - Minimap2 https://doi.org/10.1093/bioinformatics/bty191\\n - Blobtools https://doi.org/10.12688/f1000research.12232.1\\n - Seqkit https://doi.org/10.1371/journal.pone.0163962\\n - MITOS2 https://doi.org/10.1016/j.ympev.2012.08.023\\n - Gblocks (default) https://doi.org/10.1093/oxfordjournals.molbev.a026334\\n - Clipkit (optional) https://doi.org/10.1371/journal.pbio.3001007\\n - Mafft https://doi.org/10.1093/molbev/mst010\\n - Iqtree https://doi.org/10.1093/molbev/msu300\\n - ete3 https://doi.org/10.1093/molbev/msw046\\n - ggtree https://doi.org/10.1111/2041-210X.12628\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2mt\">↥ back to top</a></b>\\n</div>\\n<br/>\\n'\n",
      " '# skim2rrna\\n\\n**skim2rrna** is a snakemake pipeline for the batch assembly, annotation, and phylogenetic analysis of ribosomal genes from low coverage genome skims. The pipeline was designed to work with sequence data from museum collections. However, it should also work with genome skims from recently collected samples.\\n\\n## Contents\\n - [Setup](#setup)\\n - [Example data](#example-data)\\n - [Input](#input)\\n - [Output](#output)\\n - [Filtering contaminants](#filtering-contaminants)\\n - [Assembly and annotation only](#assembly-and-annotation-only)\\n - [Running your own data](#running-your-own-data)\\n - [Getting help](#getting-help)\\n - [Citations](#citations)\\n\\n## Setup\\n\\nThe pipeline is written in Snakemake and uses conda and singularity to install the necessary tools.\\n\\nIt is *strongly recommended* to install conda using Mambaforge. See details here https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\\n\\nOnce conda is installed, you can pull the github repo and set up the base conda environment.\\n\\n```\\n# get github repo\\ngit clone https://github.com/o-william-white/skim2rrna\\n\\n# change dir\\ncd skim2rrna\\n\\n# setup conda env\\nconda env create -n snakemake -f workflow/envs/conda_env.yaml\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Example data\\n\\nBefore you run your own data, it is recommended to run the example datasets provided . This will confirm there are no user-specific issues with the setup and it also installs all the dependencies. The example data includes simulated ribosomal data from 25 different butterfly species. \\n\\nTo run the example data, use the code below. **Note that you need to change the user email to your own address**. The email is required by the Bio Entrez package to fetch reference sequences. The first time you run the pipeline, it will take some time to install each of the conda environments, so it is a good time to take a tea break :).\\n```\\nconda activate snakemake\\n\\nsnakemake \\\\\\n   --cores 4 \\\\\\n   --use-conda \\\\\\n   --use-singularity \\\\ \\n   --config user_email=user@example_email.com\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Input\\n\\nSnakemake requires a `config.yaml` and `samples.csv` to define input parameters and sequence data for each sample. \\n\\nFor the example data provided, the config file is located here `config/config.yaml` and it looks like this:\\n```\\n# path to sample sheet csv with columns for ID,forward,reverse,taxid,seed,gene\\nsamples: config/samples.csv\\n\\n# user email\\nuser_email: user@example_email.com\\n\\n# getorganelle reference (go_fetch, custom)\\ngo_reference: go_fetch\\n\\n# forward adapter\\nforward_adapter: AGATCGGAAGAGCACACGTCTGAACTCCAGTCA\\n\\n# reverse adapter\\nreverse_adapter: AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\\n\\n# fastp deduplication (True/False)\\nfastp_dedup: True\\n\\n# barrnap kindgom (Bacteria:bac, Archaea:arc, Eukaryota:euk, None:NA)\\nbarrnap_kingdom: euk\\n\\n# alignment trimming method to use (gblocks or clipkit)\\nalignment_trim: gblocks\\n\\n# alignment missing data threshold for alignment (0.0 - 1.0)\\nmissing_threshold: 0.5\\n\\n# name of outgroup sample (optional)\\n# use \"NA\" if there is no obvious outgroup\\n# if more than one outgroup use a comma separated list i.e. \"sampleA,sampleB\"\\noutgroup: Eurema_blanda\\n\\n# plot dimensions (cm)\\nplot_height: 20\\nplot_width: 20\\n```\\n\\nThe example samples.csv file is located here `config/samples.csv` and it looks like this (note that the seed and gene columns are only required if the custom getorganelle database option is specified in the config file):\\n\\n\\n ID | forward | reverse | taxid | seed | gene \\n----|---------|---------|-------|------|------\\nAdelpha_iphiclus | .test/reads/Adelpha_iphiclus_1.fq.gz | .test/reads/Adelpha_iphiclus_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAnartia_jatrophae_saturata | .test/reads/Anartia_jatrophae_saturata_1.fq.gz | .test/reads/Anartia_jatrophae_saturata_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAraschnia_levana | .test/reads/Araschnia_levana_1.fq.gz | .test/reads/Araschnia_levana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nAuzakia_danava | .test/reads/Auzakia_danava_1.fq.gz | .test/reads/Auzakia_danava_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nBaeotus_beotus | .test/reads/Baeotus_beotus_1.fq.gz | .test/reads/Baeotus_beotus_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nCatacroptera_cloanthe | .test/reads/Catacroptera_cloanthe_1.fq.gz | .test/reads/Catacroptera_cloanthe_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nChalinga_pratti | .test/reads/Chalinga_pratti_1.fq.gz | .test/reads/Chalinga_pratti_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nDiaethria_gabaza_eupepla | .test/reads/Diaethria_gabaza_eupepla_1.fq.gz | .test/reads/Diaethria_gabaza_eupepla_2.fq.gz | 127268 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nDoleschallia_melana | .test/reads/Doleschallia_melana_1.fq.gz | .test/reads/Doleschallia_melana_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nEurema_blanda | .test/reads/Eurema_blanda_1.fq.gz | .test/reads/Eurema_blanda_2.fq.gz | 42450 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nHypolimnas_usambara | .test/reads/Hypolimnas_usambara_1.fq.gz | .test/reads/Hypolimnas_usambara_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nJunonia_villida | .test/reads/Junonia_villida_1.fq.gz | .test/reads/Junonia_villida_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nKallima_paralekta | .test/reads/Kallima_paralekta_1.fq.gz | .test/reads/Kallima_paralekta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nKallimoides_rumia | .test/reads/Kallimoides_rumia_1.fq.gz | .test/reads/Kallimoides_rumia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nLitinga_cottini | .test/reads/Litinga_cottini_1.fq.gz | .test/reads/Litinga_cottini_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nMallika_jacksoni | .test/reads/Mallika_jacksoni_1.fq.gz | .test/reads/Mallika_jacksoni_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nModuza_procris | .test/reads/Moduza_procris_1.fq.gz | .test/reads/Moduza_procris_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nParasarpa_zayla | .test/reads/Parasarpa_zayla_1.fq.gz | .test/reads/Parasarpa_zayla_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nPhaedyma_columella | .test/reads/Phaedyma_columella_1.fq.gz | .test/reads/Phaedyma_columella_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nPrecis_pelarga | .test/reads/Precis_pelarga_1.fq.gz | .test/reads/Precis_pelarga_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nProtogoniomorpha_temora | .test/reads/Protogoniomorpha_temora_1.fq.gz | .test/reads/Protogoniomorpha_temora_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nSalamis_cacta | .test/reads/Salamis_cacta_1.fq.gz | .test/reads/Salamis_cacta_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nSmyrna_blomfildia | .test/reads/Smyrna_blomfildia_1.fq.gz | .test/reads/Smyrna_blomfildia_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nTacola_larymna | .test/reads/Tacola_larymna_1.fq.gz | .test/reads/Tacola_larymna_2.fq.gz | 100750 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\nYoma_algina | .test/reads/Yoma_algina_1.fq.gz | .test/reads/Yoma_algina_2.fq.gz | 40040 | .test/seed_mitochondrion.fasta | .test/gene_mitochondrion.fasta\\n\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Output\\n\\nAll output files are saved to the `results` direcotry. Below is a table summarising all of the output files generated by the pipeline.\\n\\n| Directory             | Description               |\\n|-----------------------|---------------------------|\\n| fastqc_raw            | Fastqc reports for raw input reads |\\n| fastp                 | Fastp reports from quality control of raw reads |\\n| fastqc_qc             | Fastqc reports for quality controlled reads |\\n| go_fetch              | Optional output containing reference databases used by GetOrganelle |\\n| getorganelle          | GetOrganelle output with a directory for each sample |\\n| assembled_sequence    | Assembled sequences selected from GetOrganelle output and renamed |\\n| seqkit                | Seqkit summary of each assembly |\\n| blastn                | Blastn output of each assembly |\\n| minimap               | Mapping output of quality filtered reads against each assembly |\\n| blobtools             | Blobtools assembly summary collating blastn and mapping output |\\n| annotations           | Annotation outputs of mitos |\\n| summary               | Summary per sample (seqkit stats), contig (GC content, length, coverage, taxonomy and annotations) and annotated gene counts |\\n| annotated_genes  | Unaligned fasta files of annotated genes identified across all samples |\\n| mafft                 | Mafft aligned fasta files of annotated genes identified across all samples |\\n| mafft_filtered        | Mafft aligned fasta files after the removal of sequences based on a missing data threshold |\\n| alignment_trim        | Ambiguous parts of alignment removed using either gblocks or clipkit |\\n| iqtree                | Iqtree phylogenetic analysis of annotated genes |\\n| plot_tree             | Plots of phylogenetic trees |\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Filtering contaminants\\n\\nIf you are working with museum collections, it is possible that you may assemble and annotate sequences from contaminant/non-target species. *Contaminant sequences can be identified based on the blast search output or unusual placement in the phylogenetic trees* (see blobtools and plot_tree outputs). \\n\\nA supplementary python script `format_alignments.py `is provided to remove putative contaminants from alignments, and format the alignments for downstream phylogenetic analysis.\\n\\nFor example, let\\'s say we wanted to remove all sequences from the sample \"Kallima_paralekta\" and 5.8S ribosomal sequence, you could run the script as shown below. The script works by identifying and removing sequences that have names with  `Kallima_paralekta` or `5_8S` in the sequence names. The filtered alignments are written to a new output directory `filter_alignments_output`.\\n\\n```\\npython workflow/scripts/format_alignments.py  \\\\\\n   --input results/mafft_filtered/ \\\\\\n   --cont Kallima_paralekta 5_8S \\\\\\n   --output filter_alignments_output\\n```\\n\\n*Note that the output fasta files have been reformatted so each alignment file is named after the gene and each sequence is named after the sample.* This is useful if you would like to run our related pipeline **gene2phylo** for further phylogenetic analyses.\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Assembly and annotation only\\n\\nIf you are only interested in the assembly of ribosomal sequences and annotation of genes without the phylogenetic analysis, you can stop the pipeline from running the gene alignment and phylogenetic analyses using the `--omit-from` parameter.\\n```\\nsnakemake \\\\\\n   --cores 4 \\\\\\n   --use-conda \\\\\\n   --use-singularity \\\\\\n   --config user_email=user@example_email.com \\\\\\n   --omit-from mafft \\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Running your own data\\n\\nThe first thing you need to do is generate your own config.yaml and samples.csv files, using the files provided as a template.\\n\\nGetOrganelle requires reference data in the format of seed and gene reference fasta files. By default the pipeline uses a basic python script called go_fetch.py https://github.com/o-william-white/go_fetch to download and format reference data formatted for GetOrganelle. \\n\\ngo_fetch.py works by searching NCBI based on the NCBI taxonomy specified by the taxid column in the samples.csv file. Note that the seed and gene columns in the samples.csv file are only required if you want to provide your own custom GetOrganelle seed and gene reference databases. \\n\\nYou can use the default reference data for GetOrganelle, but I would recommend using custom reference databases where possible. See here for details of how to set up your own databases https://github.com/Kinggerm/GetOrganelle/wiki/FAQ#how-to-assemble-a-target-organelle-genome-using-my-own-reference\\n\\n## Getting help\\n\\nIf you have any questions, please do get in touch in the issues or by email o.william.white@gmail.com\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Citations\\n\\nIf you use the pipeline, please cite our bioarxiv preprint: https://doi.org/10.1101/2023.08.11.552985\\n\\nSince the pipeline is a wrapper for several other bioinformatic tools we also ask that you cite the tools used by the pipeline:\\n - Fastqc https://github.com/s-andrews/FastQC\\n - Fastp https://doi.org/10.1093/bioinformatics/bty560\\n - GetOrganelle https://doi.org/10.1186/s13059-020-02154-5\\n - Blastn https://doi.org/10.1186/1471-2105-10-421\\n - Minimap2 https://doi.org/10.1093/bioinformatics/bty191\\n - Blobtools https://doi.org/10.12688/f1000research.12232.1\\n - Seqkit https://doi.org/10.1371/journal.pone.0163962\\n - MITOS2 https://doi.org/10.1016/j.ympev.2012.08.023\\n - Gblocks (default) https://doi.org/10.1093/oxfordjournals.molbev.a026334\\n - Clipkit (optional) https://doi.org/10.1371/journal.pbio.3001007\\n - Mafft https://doi.org/10.1093/molbev/mst010\\n - Iqtree https://doi.org/10.1093/molbev/msu300\\n - ete3 https://doi.org/10.1093/molbev/msw046\\n - ggtree https://doi.org/10.1111/2041-210X.12628\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#skim2rrna\">↥ back to top</a></b>\\n</div>\\n<br/>\\n'\n",
      " '# gene2phylo\\n\\n**gene2phylo** is a snakemake pipeline for batch phylogenetic analysis of a given set of input genes. \\n\\n## Contents\\n - [Setup](#setup)\\n - [Example data](#example-data)\\n - [Input](#input)\\n - [Output](#output)\\n - [Running your own data](#running-your-own-data)\\n - [Getting help](#getting-help)\\n - [Citations](#citations)\\n\\n## Setup\\n\\nThe pipeline is written in Snakemake and uses conda to install the necessary tools.\\n\\nIt is *strongly recommended* to install conda using Mambaforge. See details here https://snakemake.readthedocs.io/en/stable/getting_started/installation.html\\n\\nOnce conda is installed, you can pull the github repo and set up the base conda environment.\\n\\n```\\n# get github repo\\ngit clone https://github.com/o-william-white/gene2phylo\\n\\n# change dir\\ncd gene2phylo\\n\\n# setup conda env\\nconda env create -n snakemake -f workflow/envs/conda_env.yaml\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Example data\\n\\nBefore you run your own data, it is recommended to run the example datasets provided . This will confirm there are no user-specific issues with the setup and it also installs all the dependencies. The example data includes mitochondrial and ribosomal genes from 25 different butterfly species. \\n\\nTo run the example data, use the code below. The first time you run the pipeline, it will take some time to install each of the conda environments, so it is a good time to take a tea break :).\\n```\\nconda activate snakemake\\n\\nsnakemake \\\\\\n   --cores 4 \\\\\\n   --use-conda\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Input\\n\\nSnakemake requires a `config.yaml` to define input parameters. \\n\\nFor the example data provided, the config file is located here `config/config.yaml` and it looks like this:\\n```\\n# name of input directory containg genes\\ninput_dir: .test\\n\\n# realign (True or False)\\nrealign: True\\n\\n# alignment missing data threshold for alignment (0.0 - 1.0), only required if realign == True\\nmissing_threshold: 0.5\\n\\n# alignment trimming method to use (gblocks or clipkit), only required if realign == True\\nalignment_trim: gblocks\\n\\n# name of outgroup sample (optional)\\n# use \"NA\" if there is no obvious outgroup\\n# if more than one outgroup use a comma separated list i.e. \"sampleA,sampleB\"\\noutgroup: Eurema_blanda\\n\\n# plot dimensions (cm)\\nplot_height: 20\\nplot_width: 20\\n```\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Output\\n\\nAll output files are saved to the `results` direcotry. Below is a table summarising all of the output files generated by the pipeline.\\n\\n| Directory                 | Description               |\\n|---------------------------|---------------------------|\\n| mafft                     | Optional: Mafft aligned fasta files of all genes |\\n| mafft_filtered            | Optional: Mafft aligned fasta files after the removal of sequences based on a missing data threshold |\\n| alignment_trim            | Optional: Ambiguous parts of alignment removed using either gblocks or clipkit |\\n| iqtree                    | Iqtree phylogenetic analysis for each gene |\\n| iqtree_plots              | Plots of Iqtree phylogenetic tree for each gene  |\\n| concatenate_alignments    | Partitioned alignment of all genes  |\\n| iqtree_partitioned        | Iqtree partitioned phylogenetic analysis |\\n| iqtree_partitioned_plot   | Plot of Iqtree partitioned tree |\\n| astral                    | Astral phylogenetic analysis of all gene trees |\\n| astral_plot               | Plot of Astral tree |\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Running your own data\\n\\nFor the pipeline to function properly, the input gene alignments must be: \\n- in a single directory \\n- end with \".fasta\"\\n- named after the aligned gene (e.g. \"cox1.fasta\" or \"28S.fasta\")\\n- share identical sample names across alignments (e.g. all genes from sample A share the same name)\\n\\nPlease see the example data in the `.test/` directory as an example. \\n\\nThen you need to generate your own config.yaml file, using the example template provided.\\n\\n## Getting help\\n\\nIf you have any questions, please do get in touch in the issues or by email o.william.white@gmail.com\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n## Citations\\n\\nIf you use the pipeline, please cite our bioarxiv preprint: https://doi.org/10.1101/2023.08.11.552985\\n\\nSince the pipeline is a wrapper for several other bioinformatic tools we also ask that you cite the tools used by the pipeline:\\n - Gblocks (default) https://doi.org/10.1093/oxfordjournals.molbev.a026334\\n - Clipkit (optional) https://doi.org/10.1371/journal.pbio.3001007\\n - Mafft (optional) https://doi.org/10.1093/molbev/mst010\\n - Iqtree https://doi.org/10.1093/molbev/msu300\\n - Ete3 https://doi.org/10.1093/molbev/msw046\\n - Ggtree https://doi.org/10.1111/2041-210X.12628\\n - Astral https://doi.org/10.1186/s12859-018-2129-y\\n\\n<br/>\\n<div align=\"right\">\\n    <b><a href=\"#gene2phylo\">↥ back to top</a></b>\\n</div>\\n<br/>\\n\\n'\n",
      " 'Post-genome assembly quality control workflow using Quast, BUSCO, Meryl, Merqury and Fasta Statistics. Updates November 2023.  Inputs: reads as fastqsanger.gz (not fastq.gz), and assembly.fasta. New default settings for BUSCO: lineage = eukaryota; for Quast: lineage = eukaryotes, genome = large. Reports assembly stats into a table called metrics.tsv, including selected metrics from Fasta Stats, and read coverage; reports BUSCO versions and dependencies; and displays these tables in the workflow report. Note: a known bug is that sometimes the workflow report text resets to default text. To restore, look for an earlier workflow version with correct workflow report text, and copy and paste report text into current version.'\n",
      " '# HiC contact map generation\\n\\nSnakemake pipeline for the generation of `.pretext` and `.mcool` files for visualisation of HiC contact maps with the softwares PretextView and HiGlass, respectively.\\n\\n## Prerequisites\\n\\nThis pipeine has been tested using `Snakemake v7.32.4` and requires conda for installation of required tools. To run the pipline use the command:\\n\\n`snakemake --use-conda`\\n\\nThere are provided a set of configuration and running scripts for exectution on a slurm queueing system. After configuring the `cluster.json` file run:\\n\\n`./run_cluster`\\n\\n## Before starting\\n\\nYou need to create a temporary folder and specify the path in the `config.yaml` file. This should be able to hold the temporary files created when sorting the `.pairsam` file (100s of GB or even many TBs)\\n\\nThe path to the genome assemly must be given in the `config.yaml`.\\n\\nThe HiC reads should be paired and named as follows: `Library_1.fastq.gz Library_2.fastq.gz`. The pipeline can accept any number of paired HiC read files, but the naming must be consistent. The folder containing these files must be provided in the `config.yaml`.\\n'\n",
      " '# HiC scaffolding pipeline\\n\\nSnakemake pipeline for scaffolding of a genome using HiC reads using yahs.\\n\\n## Prerequisites\\n\\nThis pipeine has been tested using `Snakemake v7.32.4` and requires conda for installation of required tools. To run the pipline use the command:\\n\\n`snakemake --use-conda --cores N`\\n\\nwhere N is number of cores to use. There are provided a set of configuration and running scripts for exectution on a slurm queueing system. After configuring the `cluster.json` file run:\\n\\n`./run_cluster`\\n\\n## Before starting\\n\\nYou need to create a temporary folder and specify the path in the `config.yaml` file. This should be able to hold the temporary files created when sorting the `.pairsam` file (100s of GB or even many TBs)\\n\\nThe path to the genome assemly must be given in the `config.yaml`.\\n\\nThe HiC reads should be paired and named as follows: `Library_1.fastq.gz Library_2.fastq.gz`. The pipeline can accept any number of paired HiC read files, but the naming must be consistent. The folder containing these files must be provided in the `config.yaml`.'\n",
      " '**Name:** Matmul GPU Case 1 Cache-OFF  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs 3.3  \\n**Machine**: Minotauro-MN4  \\n\\nMatmul running on the GPU without Cache.  \\nLaunched using 32 GPUs (16 nodes).  \\nPerforms C = A @ B  \\nWhere A: shape (320, 56_900_000)  block_size (10, 11_380_000)  \\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B: shape (56_900_000, 10)    &nbsp;&nbsp;block_size (11_380_000, 10)  \\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C: shape (320, 10)                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block_size (10, 10)  \\nTotal dataset size 291 GB.  \\nVersion dislib-0.9\\n\\nAverage task execution time: 56 seconds\\n'\n",
      " '**Name:** Matmul GPU Case 1 Cache-ON  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: Minotauro-MN4  \\n\\nMatmul running on the GPU leveraging COMPSs GPU Cache for deserialization speedup.  \\nLaunched using 32 GPUs (16 nodes).  \\nPerforms C = A @ B  \\nWhere A: shape (320, 56_900_000)  block_size (10, 11_380_000)  \\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B: shape (56_900_000, 10)    &nbsp;&nbsp;block_size (11_380_000, 10)  \\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;C: shape (320, 10)                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;block_size (10, 10)  \\nTotal dataset size 291 GB.  \\nVersion dislib-0.9\\n\\nAverage task execution time: 32 seconds\\n'\n",
      " '**Name:** K-Means GPU Cache OFF  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: Minotauro-MN4  \\n\\nK-Means running on GPUs.    \\nLaunched using 32 GPUs (16 nodes). Parameters used: K=40 and 32 blocks of size (1_000_000, 1200).  \\nIt creates a block for each GPU. Total dataset shape is (32_000_000, 1200).  \\nVersion dislib-0.9\\n\\nAverage task execution time: 194 seconds'\n",
      " '**Name:** K-Means GPU Cache ON  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: Minotauro-MN4  \\n\\nK-Means running on the GPU leveraging COMPSs GPU Cache for deserialization speedup.  \\nLaunched using 32 GPUs (16 nodes). Parameters used: K=40 and 32 blocks of size (1_000_000, 1200).  \\nIt creates a block for each GPU. Total dataset shape is (32_000_000, 1200).  \\nVersion dislib-0.9\\n\\nAverage task execution time: 16 seconds'\n",
      " '**Name:** Dislib Distributed Training - Cache OFF  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: Minotauro-MN4  \\n\\nPyTorch distributed training of CNN on GPU.  \\nLaunched using 32 GPUs (16 nodes).  \\nDataset: Imagenet  \\nVersion dislib-0.9  \\nVersion PyTorch 1.7.1+cu101  \\n\\nAverage task execution time: 84 seconds'\n",
      " '**Name:** Dislib Distributed Training - Cache ON  \\n**Contact Person**: cristian.tatu@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: Minotauro-MN4  \\n\\nPyTorch distributed training of CNN on GPU and leveraging COMPSs GPU Cache for deserialization speedup.  \\nLaunched using 32 GPUs (16 nodes).  \\nDataset: Imagenet  \\nVersion dislib-0.9  \\nVersion PyTorch 1.7.1+cu101  \\n\\nAverage task execution time: 36 seconds'\n",
      " 'Calculates the Fibonacci series up to a specified length.'\n",
      " 'This is the workflow for the biodiversity component of the cultural ecosystems digital twin'\n",
      " 'This is the workflow for the recreation potential component of the cultural ecosystems digital twin'\n",
      " \"[![Cite with Zenodo](https://zenodo.org/badge/509096312.svg)](https://zenodo.org/doi/10.5281/zenodo.10047653)\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A522.10.1-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![Launch on Nextflow Tower](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Nextflow%20Tower-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/sanger-tol/treeval)\\n\\n## Introduction\\n\\n**sanger-tol/treeval [1.1.0 - Ancient Aurora]** is a bioinformatics best-practice analysis pipeline for the generation of data supplemental to the curation of reference quality genomes. This pipeline has been written to generate flat files compatible with [JBrowse2](https://jbrowse.org/jb2/) as well as HiC maps for use in Juicebox, PretextView and HiGlass.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nYou can also set up and attempt to run the pipeline here: https://gitpod.io/#https://github.com/BGAcademy23/treeval-curation\\nThis is a gitpod set up for BGA23 with a version of TreeVal, although for now gitpod will not run a nextflow pipeline die to issues with using singularity. We will be replacing this with an AWS instance soon.\\n\\nThe treeval pipeline has a sister pipeline currently named [curationpretext](https://github.com/sanger-tol/curationpretext) which acts to regenerate the pretext maps and accessory files during genomic curation in order to confirm interventions. This pipeline is sufficiently different to the treeval implementation that it is written as it's own pipeline.\\n\\n1. Parse input yaml ( YAML_INPUT )\\n2. Generate my.genome file ( GENERATE_GENOME )\\n3. Generate insilico digests of the input assembly ( INSILICO_DIGEST )\\n4. Generate gene alignments with high quality data against the input assembly ( GENE_ALIGNMENT )\\n5. Generate a repeat density graph ( REPEAT_DENSITY )\\n6. Generate a gap track ( GAP_FINDER )\\n7. Generate a map of self complementary sequence ( SELFCOMP )\\n8. Generate syntenic alignments with a closely related high quality assembly ( SYNTENY )\\n9. Generate a coverage track using PacBio data ( LONGREAD_COVERAGE )\\n10. Generate HiC maps, pretext and higlass using HiC cram files ( HIC_MAPPING )\\n11. Generate a telomere track based on input motif ( TELO_FINDER )\\n12. Run Busco and convert results into bed format ( BUSCO_ANNOTATION )\\n13. Ancestral Busco linkage if available for clade ( BUSCO_ANNOTATION:ANCESTRAL_GENE )\\n14. Count KMERs with FastK and plot the spectra using MerquryFK ( KMER )\\n15. Generate a coverge track using KMER data ( KMER_READ_COVERAGE )\\n\\n## Usage\\n\\n> **Note**\\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how\\n> to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline)\\n> with `-profile test` before running the workflow on actual data.\\n\\nCurrently, it is advised to run the pipeline with docker or singularity as a small number of major modules do not currently have a conda env associated with them.\\n\\nNow, you can run the pipeline using:\\n\\n```bash\\n# For the FULL pipeline\\nnextflow run main.nf -profile singularity --input treeval.yaml --outdir {OUTDIR}\\n\\n# For the RAPID subset\\nnextflow run main.nf -profile singularity --input treeval.yaml -entry RAPID --outdir {OUTDIR}\\n```\\n\\nAn example treeval.yaml can be found [here](assets/local_testing/nxOscDF5033.yaml).\\n\\nFurther documentation about the pipeline can be found in the following files: [usage](https://pipelines.tol.sanger.ac.uk/treeval/dev/usage), [parameters](https://pipelines.tol.sanger.ac.uk/treeval/dev/parameters) and [output](https://pipelines.tol.sanger.ac.uk/treeval/dev/output).\\n\\n> **Warning:**\\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\\n\\n## Credits\\n\\nsanger-tol/treeval has been written by Damon-Lee Pointon (@DLBPointon), Yumi Sims (@yumisims) and William Eagles (@weaglesBio).\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline:\\n\\n<ul>\\n  <li>@gq1 - For building the infrastructure around TreeVal and helping with code review</li>\\n  <li>@ksenia-krasheninnikova - For help with C code implementation and YAML parsing</li>\\n  <li>@mcshane - For guidance on algorithms </li>\\n  <li>@muffato - For code reviews and code support</li>\\n  <li>@priyanka-surana - For help with the majority of code reviews and code support</li>\\n</ul>\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\n## Citations\\n\\n<!--TODO: Citation-->\\n\\nIf you use sanger-tol/treeval for your analysis, please cite it using the following doi: [10.5281/zenodo.10047653](https://doi.org/10.5281/zenodo.10047653).\\n\\n### Tools\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nYou can cite the `nf-core` publication as follows:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n\"\n",
      " \"The input to this workflow is a data matrix of gene expression that was collected from a pediatric patient tumor patient from the KidsFirst Common Fund program [1]. The RNA-seq samples are the columns of the matrix, and the rows are the raw expression gene count for all human coding genes (Table 1). This data matrix is fed into TargetRanger [2] to screen for targets which are highly expressed in the tumor but lowly expressed across most healthy human tissues based on gene expression data collected from postmortem patients with RNA-seq by the GTEx Common Fund program [3]. Based on this analysis the gene IMP U3 small nucleolar ribonucleoprotein 3 (IMP3) was selected because it was the top candidate returned from the TargetRanger analysis (Tables 2-3). IMP3 is also commonly called insulin-like growth factor 2 mRNA-binding protein 3 (IGF2BP3). Next, we leverage unique knowledge from various other Common Fund programs to examine various functions and knowledge related to IMP3. First, we queried the LINCS L1000 data [4] from the LINCS program [5] converted into RNA-seq-like LINCS L1000 Signatures [6] using the SigCom LINCS API [7] to identify mimicker or reverser small molecules that maximally impact the expression of IMP3 in human cell lines (Fig. 1, Table 4). In addition, we also queried the LINCS L1000 data to identify single gene CRISPR knockouts that down-regulate the expression of IMP3 (Fig. 1, Table 5). These potential drug targets were filtered using the Common Fund IDG program's list of understudied proteins [8] to produce a set of additional targets (Table 6). Next, IMP3 was searched for knowledge provided by the with the Metabolomics Workbench MetGENE tool [9]. MetGENE aggregates knowledge about pathways, reactions, metabolites, and studies from the Metabolomics Workbench Common Fund supported resource [10]. The Metabolomics Workbench was searched to find associated metabolites linked to IMP3 [10]. Furthermore, we leveraged the Linked Data Hub API [11] to list knowledge about regulatory elements associated with IMP3 (Table 6). Finally, the GlyGen database [12] was queried to identify relevant sets of proteins that are the product of the IMP3 genes, as well as known post-translational modifications discovered on IMP3.\\n\\n1. Lonsdale, J. et al. The Genotype-Tissue Expression (GTEx) project. Nature Genetics vol. 45 580–585 (2013). doi:10.1038/ng.2653\\n2. Evangelista, J. E. et al. SigCom LINCS: data and metadata search engine for a million gene expression signatures. Nucleic Acids Research vol. 50 W697–W709 (2022). doi:10.1093/nar/gkac328\\n3. IDG Understudied Proteins, https://druggablegenome.net/AboutIDGProteinList\\n4. MetGENE, https://sc-cfdewebdev.sdsc.edu/MetGENE/metGene.php\\n5. The Metabolomics Workbench, https://www.metabolomicsworkbench.org/\\n6. Linked Data Hub, https://ldh.genome.network/cfde/ldh/\\n7. York, W. S. et al. GlyGen: Computational and Informatics Resources for Glycoscience. Glycobiology vol. 30 72–73 (2019). doi:10.1093/glycob/cwz080\"\n",
      " 'The tool provides a calculation of the power spectrum of Stochastic Gravitational Wave Backgorund (SGWB) from a first-order cosmological phase transition based on the parameterisations of Roper Pol et al. (2023). The power spectrum includes two components: from the sound waves excited by collisions of bubbles of the new phase and from the turbulence that is induced by these collisions.\\n\\nThe cosmological epoch of the phase transition is described by the temperature, T_star and by the number(s) of relativistic degrees of freedom, g_star that should be specified as parameters.\\n\\nThe phase transition itself is characterised by phenomenological parameters, alpha, beta_H and epsilon_turb, the latent heat, the ratio of the Hubble radius to the bubble size at percolation and the fraction of the energy otuput of the phase transition that goes into turbulence.\\n\\nThe product Model spectrum outputs the power spectrum for fixed values of these parameters. The product Phase transition parameters reproduces the constraints on the phase transition parameters from the Pulsar Timing Array gravitational wave detectors, reported by Boyer & Neronov (2024), including the estimate of the cosmological magnetic field induced by turbulence.\\n'\n",
      " '**Name:** Incrementation and Fibonacci     \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\n**Brief Overview:** Demonstrates COMPSs task parallelism with increment and Fibonacci computations. Helps to understand COMPSs.\\n\\n**Detailed Description:**\\n 1. Performs multiple increments of input values in parallel using COMPSs.\\n 2. Concurrently calculates Fibonacci numbers using recursive COMPSs tasks.\\n 3. Demonstrates task synchronization via `compss_wait_on`.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss src/increment_fibonacci.py value1 Value2 Value3 \\n#add more values if you want\\n\\n```\\n\\n# Execution Examples\\n```\\nruncompss src/increment_fibonacci.py 1 4 3 9 6 9 \\n\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " 'This workflow provides a calculaiton of the power spectrum of Stochastic Gravitational Wave Backgorund (SGWB) from a first-order cosmological phase transition based on the parameterisations of Roper Pol et al. (2023). The power spectrum includes two components: from the sound waves excited by collisions of bubbles of the new phase and from the turbulence that is induced by these collisions.\\n\\nThe cosmological epoch of the phase transition is described by the temperature, T_star and by the number(s) of relativistic degrees of freedom, g_star that should be specified as parameters.\\n\\nThe phase transition itself is characterised by phenomenological parameters, alpha, beta_H and epsilon_turb, the latent heat, the ratio of the Hubble radius to the bubble size at percolation and the fraction of the energy otuput of the phase transition that goes into turbulence.\\n\\n\\n'\n",
      " '![Perl CI](https://github.com/FabianDeister/Library_curation_BOLD/actions/workflows/ci.yml/badge.svg)\\n[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10975576.svg)](https://doi.org/10.5281/zenodo.10975576)\\n\\n# Library curation BOLD\\n\\n![alt text](https://github.com/FabianDeister/Library_curation_BOLD/blob/main/doc/IBOL_LOGO_TRANSPARENT.png?raw=true)\\n\\nThis repository contains scripts and synonymy data for pipelining the \\nautomated curation of [BOLD](https://boldsystems.org) data dumps in \\nBCDM TSV format. The goal is to implement the classification of barcode \\nreference sequences as is being developed by the \\n[BGE](https://biodiversitygenomics.eu) consortium. A living document\\nin which these criteria are being developed is located\\n[here](https://docs.google.com/document/d/18m-7UnoJTG49TbvTsq_VncKMYZbYVbau98LE_q4rQvA/edit).\\n\\nA further goal of this project is to develop the code in this repository\\naccording to the standards developed by the community in terms of automation,\\nreproducibility, and provenance. In practice, this means including the\\nscripts in a pipeline system such as [snakemake](https://snakemake.readthedocs.io/),\\nadopting an environment configuration system such as\\n[conda](https://docs.conda.io/), and organizing the folder structure\\nin compliance with the requirements of\\n[WorkFlowHub](https://workflowhub.eu/). The latter will provide it with \\na DOI and will help generate [RO-crate](https://www.researchobject.org/ro-crate/)\\ndocuments, which means the entire tool chain is FAIR compliant according\\nto the current state of the art.\\n\\n## Install\\nClone the repo:\\n```{shell}\\ngit clone https://github.com/FabianDeister/Library_curation_BOLD.git\\n```\\nChange directory: \\n```{shell}\\ncd Library_curation_BOLD\\n```\\nThe code in this repo depends on various tools. These are managed using\\nthe `mamba` program (a drop-in replacement of `conda`). The following\\nsets up an environment in which all needed tools are installed:\\n\\n```{shell}\\nmamba env create -f environment.yml\\n```\\n\\nOnce set up, this is activated like so:\\n\\n```{shell}\\nmamba activate bold-curation\\n```\\n\\n## How to run\\n### Bash\\nAlthough the aim of this project is to integrate all steps of the process\\nin a simple snakemake pipeline, at present this is not implemented. Instead,\\nthe steps are executed individually on the command line as perl scripts\\nwithin the conda/mamba environment. Because the current project has its own\\nperl modules in the `lib` folder, every script needs to be run with the \\nadditional include flag to add the module folder to the search path. Hence,\\nthe invocation looks like the following inside the scripts folder:\\n\\n```{shell}\\nperl -I../../lib scriptname.pl -arg1 val1 -arg2 val2\\n```\\n### snakemake\\n\\nFollow the installation instructions above.\\n\\nUpdate config/config.yml to define your input data.\\n\\nNavigate to the directory \"workflow\" and type:\\n```{shell}\\nsnakemake -p -c {number of cores} target\\n```\\n\\nIf running on an HPC cluster with a SLURM scheduler you could use a bash script like this one:\\n```{shell}\\n#!/bin/bash\\n#SBATCH --partition=hour\\n#SBATCH --output=job_curate_bold_%j.out\\n#SBATCH --error=job_curate_bold_%j.err\\n#SBATCH --mem=24G\\n#SBATCH --cpus-per-task=2\\n\\nsource activate bold-curation\\n\\nsnakemake -p -c 2 target\\n\\necho Complete!\\n```\\n'\n",
      " \"Parabricks-Genomics-nf is a GPU-enabled pipeline for alignment and germline short variant calling for short read sequencing data. The pipeline utilises [NVIDIA's Clara Parabricks](https://docs.nvidia.com/clara/parabricks/4.2.0/index.html) toolkit to dramatically speed up the execution of best practice bioinformatics tools. Currently, this pipeline is **configured specifically for [NCI's Gadi HPC](https://nci.org.au/our-systems/hpc-systems)**. \\n\\nNVIDIA's Clara Parabricks can deliver a significant speed improvement over traditional CPU-based methods, and is designed to be used only with NVIDIA GPUs. This pipeline is suitable for population screening projects as it executes Parabrick's implementations of BWA mem for short read alignment and Google's DeepVariant for short variant calling. Additionally, it uses standard CPU implementations of data quality evaluation tools [FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/) and [MultiQC](https://multiqc.info/) and [DNAnexus' GLnexus](https://academic.oup.com/bioinformatics/article/36/24/5582/6064144) for scalable gVCF merging and joint variant calling. Optionally, [Variant Effect Predictor (VEP)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-016-0974-4) can be run for variant annotation. \\n\"\n",
      " '# ProGFASTAGen\\n\\nThe ProGFASTAGen (**Pro**tein-**G**raph-**FASTA**-**Gen**erator or **Pro**t**G**raph-**FASTA**-**Gen**erator) repository contains workflows to generate so-called precursor-specific-FASTAs (using the precursors from MGF-files) including feature-peptides, like VARIANTs or CONFLICTs if desired, or global-FASTAs (as described in [ProtGraph](https://github.com/mpc-bioinformatics/ProtGraph)). The single workflow scripts have been implemented with [Nextflow-DSL-2](https://www.nextflow.io/docs/latest/dsl2.html) and are independent to each other. Each of these workflows can be used on their own or can be imported to other workflows for other use-cases. Further, we included three main-workflows, to show how the single workflows can be chained together. The `main_workflow_protein_fasta.nf`-workflow converts Thermo-RAW-files into MGF, searches with Comet (and Percolator) and the identification results are then further summarized. The workflows `main_workflow_global_fasta.nf` and `main_workflow_precursor_specific_fasta.nf` generate specific FASTA-files before search-engine-identification. Below are example nextflow-calls, which can be used.\\n\\nRegarding the precursor-specific-FASTA-generation: The source-code of the C++ implementation for traversal can be found in `bin`. There, four implementations are present: `Float/Int`-Versions as well as `DryRun/VarLimitter`-Versions of the traversal. The `Float/Int`-Versions can be faster/slower depending on th processor-architecture and can be used via a flag in the `create_precursor_specific_fasta.nf`-workflow. The `DryRun`-Version does not generate a FASTA but tests the used system (depending on a query-timeout) to determine the maximum number of variants which can be used, while not timing out. The actual FASTA-generation happens in the `VarLimitter`-Version using the generated protein-graphs at hand.\\n\\nin **Prerequisites** a small description of dependencies and how to set up the host system is given. **Individual steps** describes the single workflows and how they can be called, while **Main Workflow Scripts** shows example-calls of the main workflows. In **Regenerate Results from Publication**, the calls and parameters are shown, which were used in the publication. Using the same FASTA or UniProt flat file format with a similar server-setting should yield similar results as used in the publication.\\n\\n## Prerequisites\\n\\n### Executing on Linux\\n\\nThis workflow can be only executed on linux (tested on Ubuntu 22.04 and ArchLinux). Before setting up the `bin`-folder, some requiered binaries need to be present on the OS. (Focusing on Ubuntu:) The following packages need to be installed on Ubuntu (via `apt`), if not already:\\n\\n```text\\nbuild-essential\\nwget\\ncurl\\nunzip\\ncmake\\nmono-complete\\npython3-pip (or any environment with Python3, where pip is available)\\npython-is-python3 (needed for ubuntu, so that python points to python3)\\n```\\n\\nIf all packages are installed (and the python environment is set up), the setup-script needs to be executed, which downloads needed dependencies and compiles the source-code located in the `bin`-folder:\\n\\n```shell\\nchmod +x compile_and_setup_depencies.sh  # In case this file is not executable\\n./compile_and_setup_depencies.sh  # Downloads dependencies, compiles the C++-implementation and sets all binaries in the bin-folder as executable\\n```\\n\\nIf the script exits without errors, the provided workflows can be executed with the command `nextflow`.\\n\\n### Executing in Docker\\n\\nAlternatively, docker can be used. For this, please follow the [installation guide](https://docs.docker.com/engine/install/ubuntu/) for docker. After installing docker, a local docker-container can be build with all needed dependencies for the workflows. We provide a `Dockerfile` in the `docker`-folder. To build it, execute (while beeing with a shell in the root-folder of this repository) the following:\\n\\n```shell\\ndocker build -t progfastagen:local . -f docker/Dockerfile\\n```\\n\\nThis command builds a local docker container, tagging it with `progfastagen:local`, which can be later used by nextflow. To use it with nextflow, make sure that `nextflow` is installed on the host-system. For each of the workflow example calls below, the `-with-docker progfastagen:local` then needs to be appended, to let `nextflow` know to use the local docker-container.\\n\\n## Individual Steps\\n\\nEach step has been implemented in such a way, that it can be executed on its own. Each subsection below, provides a brief overview and an example call of the required parameters to demonstrate how the workflow can be called. If you are interested for all the available parameters within a workflow and want modify or tune them, then please refer to the source of the workflows, where each parameter is described briefly.\\n\\n### Converting RAW-files to MGF\\n\\nThe workflow `convert_to_mgf.nf` is a wrapper around the ThermoRawFileParser and converts RAW-files to the MGF-format. The `ctm_raws` parameter needs to be set, in order to generate the MGF-files:\\n\\n```text\\nnextflow run convert_to_mgf.nf \\\\\\n    --ctm_raws < Folder containing RAW-files > \\\\\\n    --ctm_outdir < Output-Folder, where the MGFs should be stored >\\n```\\n\\n### Generating a Precursor-Specific-FASTA\\n\\nThe workflow `create_precursor_specific_fasta.nf` generates a precursor-specific-FASTA-file, tailored to a set of MGF-files. Here, Protein-Graphs are generated, using the UniProt flat file format (which can be downloaded from [UniProt](https://www.uniprot.org/) by selecting `Text` as format) and a python script prepares the queries, by extracting the MS2-precursors from the MGF-files (using a tolerance, in ppm). Using the Protein-Graphs and a `DryRun`-Version of the traversal, the maximum-variant-limits are determined for each Protein-Graph (and mass-query-range) using a binary-search. These limits are then used for the actual ms2-specific-FASTA-generation in conjunction with the extracted MS2-precursors and a compacted FASTA is returned, which is tailored to the MGF-files.\\n\\nAltough of the complexity, the workflow only requires the following parameters to generate such a FASTA:\\n\\n```text\\nnextflow run create_precursor_specific_fasta.nf \\\\\\n    --cmf_mgf_files < Folder containing MGF-files > \\\\\\n    --cmf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\\\\n    --cmf_outdir <The Output-Folder where the traversal-limits are saved and the ms2-specific-FASTA is stored >\\n```\\n\\nThe optional parameter: `cmf_pg_additional_params` is added to ProtGraph directly, allowing every parameter, ProtGraph provides to be set there (e.g. useful if the digestion should be changed or features/PTMs should be included/excluded, etc...), allowing arbitrary settings to generate Protein-Graphs if desired. It defaults to use all features, ProtGraph can parse.\\n\\n**Note regarding PTMs/Tolerance**: The FASTA is tailored to the MS2-precursors, therefore variable and fixed modifications need to be set to the same settings as for the actual identification. This workflow defaults to carbamidomethylation (C, fixed) and oxidation (M, variable). See ProtGraph (and the workflow-parameter `cmf_pg_additional_params`) to set the PTMs accordingly in the Protein-Graphs. The same applies for the MS2-precursor-tolereance which can be set with `cmf_query_ppm` and defaults to `5ppm`.\\n\\n**Note regarding Limits**: This workflows defaults to allow up to 5 seconds per query and limits peptides to contain at most 5 variants (with a maximum of 5000 Da per peptide), resulting into FASTA-files which can be 15-200GB large (depending on dataset and species). Changing these settings can drastically increase/decrease the runtime/memory usage/disk usage. We advise to change those settings slightly and to pay attention on the runtime/memory usage/disk usage if run with the newly set limits (and dataset + species) the first time.\\n\\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\\n\\n### Generating a Global-FASTA\\n\\nThis workflow generates a so called global-FASTA, using ProtGraph, the UniProt flat file format and some global limits for writing out peptides/proteins. Global-FASTAs can be generated with the `create_global_fasta.nf`-workflow. To generate a global-FASTA, only a path to a single SP-EMBL-file (UniProt flat file format) is required. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting `Text` instead of `FASTA` as the download format.\\n\\n```text\\nnextflow run create_global_fasta.nf \\\\\\n    --cgf_sp_embl_file < Path to a SP-EMBL-File (UniProt flat file format) > \\\\\\n    --cgf_outdir < The output-folder, where the gloabl-FASTA and some Protein-Graph-statistics should be saved >\\n```\\n\\nPer default, this workflow does not export feature-peptides and is set to only export peptides with up to 5000 Da mass and maximum of two miscleavages. It is possible to generate global-FASTA with some specific features (like containing, `SIGNAL`, `PEPTIDE` or others) and other limits. The parameters `cgf_features_in_graphs` and `cgf_peptide_limits` can be set accordingly. These are added to ProtGraph directly, hence every parameter ProtGraph provides, can be set here (including different digestion settings).\\n\\n**Note**: A dry run with ProtGraph to generate statistics how many peptide would be theoretically exported is advised prior for testing. Some Protein-Graphs with some features (e.g. P53 using variants) can contain to many peptides, which could result to very long runtimes and huge FASTAs.\\n\\n**Note regarding identification**: If digestion is enabled (default is `Trypsin`), the resulting FASTA contains already digested entries, thus searching with a search-engine, the digestion should be set to `off/no_cut`.\\n\\n### Identification via Coment (and Percolator)\\n\\nWe provide an identification workflow to showcase, that the generated FASTAs can be used with search-engines. The workflow `identification_via_comet.nf` identifies MGF-files individually, using custom search-settings for Comet (and if desired rescores the results with Percolator), applies an FDR-cutoff using the q-value (for each file) and exposes the identification results into an output-folder.\\n\\nThree parameters are required, to execute the workflow:\\n\\n1. The MGFs which should be identified\\n2. The Comet-Parameter file to set the search-settings\\n3. The FASTA-file which should be used for identification\\n\\nBelow is an example call with all required parameters (Percolator is enabled by default):\\n\\n```text\\nnextflow run identification_via_comet.nf \\\\\\n    --idc_mgf_folder < Folder containing MGF-files > \\\\\\n    --idc_fasta_file < The FASTA which should be used for identification > \\\\\\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\\\\n    --idc_outdir < Output-Folder where the results of the identification files are stored >\\n```\\n\\nHere is another example call with all required parameters (this time, turning Percolator off):\\n\\n```text\\nnextflow run identification_via_comet.nf \\\\\\n    --idc_mgf_folder < Folder containing MGF-files > \\\\\\n    --idc_fasta_file < The FASTA which should be used for identification > \\\\\\n    --idc_search_parameter_file < The Comet-Parameters file (Search Configuration) > \\\\\\n    --idc_outdir < Output-Folder where the results of the identification files are stored > \\\\\\n    --idc_use_percolator 0\\n```\\n\\n**Note**: This identification-workflow defaults to an FDR-cutoff (q-value) of `--idc_fdr \"0.01\"`, reporting only 1% filtered PSMs. Arbitrary and multiple FDR-cutoffs can be set and can be changed to the desired value.\\n\\n### Summarization of results\\n\\nThe `summarize_ident_results.nf`-workflow genereates convenient summarization of the identification results. Here, the identification-results are binned into 4 groups:\\n\\n1. Unique PSMs (a match, which can only originate from one protein)\\n2. Shared PSMs (a match, which can originate from multiple proteins)\\n3. Unique Feature PSMs (as 1., but only containing peptides, which can be explained by a features)\\n4. Shared Feature PSMs (as 2., but only can be explained by features from all originating proteins)\\n\\nFurthermore, heatmaps are generated to provide an overview of found peptides across all MGFs/RAW-files.\\n\\nTo call this method, a `glob` needs to be specified in this workflow:\\n\\n```text\\nnextflow run summarize_ident_results.nf \\\\\\n    --sir_identified_files_glob < The glob matching the desired output from the identification results >\\n    --sir_outdir < The output directory where the summarized results should be saved >\\n```\\n\\nIn case, the identification workflow was executed using an FDR of 0.01, you could use the following `glob`:\\n\\n```text\\nnextflow run summarize_ident_results.nf \\\\\\n    --sir_identified_files_glob \"<Path_to_folder>/*qvalue_no_decoys_fdr_0.01.tsv\"\\n    --sir_outdir < The output directory where the summarized results should be saved >\\n```\\n\\n**Note**: This step can be used only if specific columns are present in the tables. Furthermore, it distinguishes between the identification results from a FASTA by UniProt or by ProtGraph. The additional parameters control, whether to bin results in group 3 and 4, decide if variable modifications should be considered as unique, as well as if a peptide, which originates multiple times to the same protein should be considered as unique. The main-workflows set these parameters accordingly and can be used as an example.\\n\\n## Main Workflow Scripts\\n\\nEach individual step described above, is also imported and chained into three main-workflows:\\n\\n1. `main_workflow_protein_fasta.nf` (UniProt-FASTA-search)\\n2. `main_workflow_global_fasta.nf` (Generation of a global-FASTA and search)\\n3. `main_workflow_precursor_specific_fasta.nf` (Generation of a precursor-specific-FASTA and search)\\n\\ngenerating summarized identification results across multiple RAW-files.\\n\\nIn each of these workflows, it is possible to modify the parameters of the imported subworkflows, by using the imported subworkflows parameters directly (as shown in the **Individual Steps** above).\\n\\nFor protein-FASTA identification, only three parameters are required:\\n\\n```text\\nnextflow run main_workflow_protein_fasta.nf \\\\\\n    --main_fasta_file < The FASTA-file, to be used for identification > \\\\\\n    --main_raw_files_folder < The folder containing RAW-files > \\\\\\n    --main_comet_params < The parameters file for comet (for identification) > \\\\\\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\\n```\\n\\nThis is also true for the other two workflows, where instead of a FASTA-file, the UniProt flat file format needs to be provided. Such a file can be downloaded from [UniProt](https://www.uniprot.org/) directly, by selecting the format `Text` instead of the format `FASTA`.\\n\\nHere are the correpsonding calls for global-FASTA and precurosr-specific-FASTA generation and identification:\\n\\n```text\\n# global-FASTA\\nnextflow run main_workflow_global_fasta.nf \\\\\\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\\\\n    --main_raw_files_folder < The folder containing RAW-files > \\\\\\n    --main_comet_params< The parameters file for comet (for identification) > \\\\\\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\\n\\n# precursor-specific-FASTA\\nnextflow run main_workflow_precursor_specific_fasta.nf \\\\\\n    --main_sp_embl_file < The SP-EMBL-file used for Protein-Graph- and FASTA-generation (UniProt flat file format) > \\\\\\n    --main_raw_files_folder < The folder containing RAW-files > \\\\\\n    --main_comet_params < The parameters file for comet (for identification) > \\\\\\n    --main_outdir < Output-Folder where all the results from the workflows should be saved >\\n```\\n\\n**Note**: Only defining the required parameters, uses the default parameters for every other setting. For all workflows, this would mean, that the FDR-cutoff (q-value) is set to `0.01` resulting into both FDRs considered. Furthermore, the global-FASTA and precursor-specific-FASTA workflows assume Trypsin digestion. For the global-FASTA-workflow, no features are exported by default, which may not be desired, if someone whishes to search for peptide-features (like `SIGNAL`, etc..). For the precursor-specific-FASTA-workflow, the PTMs carbamidomethylation (C, fixed) and oxidation (M, variable) are assumed, which may need to be modified.\\n\\n**Note regarding example calls**: Further below you can find the calls as used in the publication. These set the most minimal parameters for a correct execution on custom datasets and can be used as an example.\\n\\n## Regenerate Results from Publication\\n\\nIn this subsection you can find the nextflow-calls which were used to execute the 3 workflows. Executing this with the same UniProt flat file/FASTA-file should yield the similar/same results. For generated precursor-specific-FASTAs it may happen, that these are generated with slightly different variant-limits, therefore a slightly different FASTA to search with and slightly different identification results.\\n\\nThe FASTA/UniProt flat file used for identification can be found [here](https://cloud.mpc.rub.de/s/LJ2bgGNmsxzSaod). The Comet configuration files are provided in the `example_configuration`-folder. The datasets can be retrieved from [PRIDE](https://www.ebi.ac.uk/pride/).\\n\\n### PXD002171\\n\\n```shell\\n# PXD002171 Precursor-Specific\\nnextflow run main_workflow_precursor_specific_fasta.nf \\\\\\n    -with-report \"PXD002171_results_precursor_specific/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD002171_results_precursor_specific/nextflow_timeline.html\" \\\\\\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\\\\n    --main_raw_files_folder PXD002171 \\\\\\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\\\\n    --main_outdir PXD002171_results_precursor_specific \\\\\\n    --cmf_max_precursor_da 5000 \\\\\\n    --cmf_query_ppm 5 \\\\\\n    --cmf_timeout_for_single_query 5 \\\\\\n    --cmf_maximum_variant_limit 5 \\\\\\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -vm \\'M:15.994915\\' -vm \\'C:71.037114\\'\" \\\\\\n    --idc_fdr \"0.01\"\\n    \\n# PXD002171 Global digested FASTA\\nnextflow run main_workflow_global_fasta.nf \\\\\\n    -with-report \"PXD002171_global_fasta/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD002171_global_fasta/nextflow_timeline.html\" \\\\\\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\\\\n    --main_raw_files_folder PXD002171 \\\\\\n    --main_comet_params example_configurations/PXD002171_no_dig.txt \\\\\\n    --main_outdir PXD002171_global_fasta \\\\\\n    --cgf_features_in_graphs \"-ft None\" \\\\\\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\\\\n    --idc_fdr \"0.01\"\\n\\n# PXD002171 Protein FASTA\\nnextflow run main_workflow_protein_fasta.nf \\\\\\n    -with-report \"PXD002171_protein_fasta/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD002171_protein_fasta/nextflow_timeline.html\" \\\\\\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\\\\n    --main_raw_files_folder PXD002171 \\\\\\n    --main_comet_params example_configurations/PXD002171_trypsin_dig.txt \\\\\\n    --main_outdir PXD002171_protein_fasta \\\\\\n    --idc_fdr \"0.01\"\\n```\\n\\n### PXD028605\\n\\n```shell\\n# PXD028605 Precursor-Specific\\nnextflow run main_workflow_precursor_specific_fasta.nf \\\\\\n    -with-report \"PXD028605_results_precursor_specific/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD028605_results_precursor_specific/nextflow_timeline.html\" \\\\\\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\\\\n    --main_raw_files_folder PXD028605 \\\\\\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\\\\n    --main_outdir PXD028605_results_precursor_specific \\\\\\n    --cmf_max_precursor_da 5000 \\\\\\n    --cmf_query_ppm 20 \\\\\\n    --cmf_timeout_for_single_query 5 \\\\\\n    --cmf_maximum_variant_limit 5 \\\\\\n    --cmf_pg_additional_params \"-ft VARIANT -ft SIGNAL -ft INIT_MET -ft CONFLICT -ft VAR_SEQ -ft PEPTIDE -ft PROPEP -ft CHAIN -fm \\'C:57.021464\\' -vm \\'M:15.9949\\'\" \\\\\\n    --idc_fdr \"0.01\"\\n\\n# PXD028605 Global digested FASTA\\nnextflow run main_workflow_global_fasta.nf \\\\\\n    -with-report \"PXD028605_global_fasta/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD028605_global_fasta/nextflow_timeline.html\" \\\\\\n    --main_sp_embl_file 20230619_homo_sapiens_proteome.txt \\\\\\n    --main_raw_files_folder PXD028605 \\\\\\n    --main_comet_params example_configurations/PXD028605_no_dig.txt \\\\\\n    --main_outdir PXD028605_global_fasta \\\\\\n    --cgf_features_in_graphs \"-ft None\" \\\\\\n    --cgf_peptide_limits \"--pep_miscleavages 2 --pep_min_pep_length 5\" \\\\\\n    --idc_fdr \"0.01\"\\n\\n# PXD028605 Protein FASTA\\nnextflow run main_workflow_protein_fasta.nf \\\\\\n    -with-report \"PXD028605_protein_fasta/nextflow_report.html\" \\\\\\n    -with-timeline \"PXD028605_protein_fasta/nextflow_timeline.html\" \\\\\\n    --main_fasta_file 20230619_homo_sapiens_proteome.fasta \\\\\\n    --main_raw_files_folder PXD028605 \\\\\\n    --main_comet_params example_configurations/PXD028605_trypsin_dig.txt \\\\\\n    --main_outdir PXD028605_protein_fasta \\\\\\n    --idc_fdr \"0.01\"\\n```\\n'\n",
      " '**Name:** Matrix multiplication with Files, reproducibility example  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles (N hardcoded to 2, and M hardcoded to 8). The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\n# Reproducibility\\nTo reproduce the exact results of this example, follow the instructions at the [Workflow Provenance section at COMPSs User Manual](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/04_Workflow_Provenance.html), WITH data persistence, PyCOMPSs application\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/matmul_files.py inputs_folder/ outputs_folder/\\n```\\n\\nwhere:\\n* inputs_folder/: Folder where A and B matrices are located\\n* outputs_folder/: Folder with the resulting C matrix\\n\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/matmul_files.py inputs/ outputs/\\nruncompss src/matmul_files.py inputs/ outputs/\\npython -m pycompss src/matmul_files.py inputs/ outputs/\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " '**Name:** Matrix multiplication with Files, reproducibility example, without data persistence\\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles (N hardcoded to 2, and M hardcoded to 8). The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\n# Reproducibility\\nTo reproduce the exact results of this example, follow the instructions at the [Workflow Provenance section at COMPSs User Manual](https://compss-doc.readthedocs.io/en/stable/Sections/05_Tools/04_Workflow_Provenance.html), WITHOUT data persistence, PyCOMPSs application.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --lang=python src/matmul_files.py inputs_folder/ outputs_folder/\\n```\\n\\nwhere:\\n* inputs_folder/: Folder where A and B matrices are located\\n* outputs_folder/: Folder with the resulting C matrix\\n\\n\\n# Execution Examples\\n```\\nruncompss --lang=python src/matmul_files.py inputs/ outputs/\\nruncompss src/matmul_files.py inputs/ outputs/\\npython -m pycompss src/matmul_files.py inputs/ outputs/\\n```\\n\\n# Build\\nNo build is required\\n'\n",
      " 'Complete workflow for TANGO as reported in Lecomte et al (2024),\\n\"Revealing the dynamics and mechanisms of bacterial interactions in\\ncheese production with metabolic modelling\", Metabolic Eng. 83:24-38\\nhttps://doi.org/10.1016/j.ymben.2024.02.014\\n\\n1. Parameters for individual models are obtained by optimization\\n2. Individual dynamics and community dynamics are simulated\\n3. Figures for the manuscript are assembled from the results.'\n",
      " \"[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.11068736.svg)](https://doi.org/10.5281/zenodo.11068736)\\n\\n# JAX NGS Operations Nextflow DSL2 Pipelines\\n\\nThis repository contains production bioinformatic analysis pipelines for a variety of bulk 'omics data analysis. Please see the [Wiki documentation](https://github.com/TheJacksonLaboratory/cs-nf-pipelines/wiki) associated with this repository for all documentation and available analysis workflows.\\n\"\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nWorkflow information:\\n* Input = genome.fasta.\\n* Outputs = soft_masked_genome.fasta, hard_masked_genome.fasta, and table of repeats found. \\n* Runs RepeatModeler with default settings, uses the output of this (repeat library) as input into RepeatMasker. \\n* Runs RepeatMasker with default settings except for: Skip masking of simple tandem repeats and low complexity regions. (-nolow) : default set to yes.  Perform softmasking instead of hardmasking - set to yes. \\n* Converts the soft-masked genome to hard-masked for for use in other tools if required. \\n* Workflow report displays an edited table of repeats found. Note: a known bug is that sometimes the workflow report text resets to default text. To restore, look for an earlier workflow version with correct workflow report text, and copy and paste report text into current version.\\n'\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nAbout this workflow:\\n\\n* Repeat this workflow separately for datasets from different tissues. \\n* Inputs = collections of R1 files, and R2 files (all from a single tissue type). \\n* Runs FastQC with default settings, separately for raw reads R1 and R2 collections; all output to MultiQC. \\n* Runs Trimmomatic with initial ILLUMINACLIP step (using standard adapter sequence for TruSeq3 paired-ended), uses settings SLIDINGWINDOW:4:5 LEADING:5 TRAILING:5 MINLEN:25, retain paired (not unpaired) outputs. User can modify at runtime. \\n* Runs FastQC with default settings, separately for trimmed R1 and R2 collections; all output to MultiQC. \\n* From Trimmomatic output: concatenate all R1 reads; concatenate all R2 reads. \\n* Outputs = trimmed merged R1 file, trimmed merged R2 file. \\n* Log files from Trimmomatic to MultiQC, to summarise trimming results. \\n* Note: a known bug with MultiQC html output is that plot is labelled as \"R1\" reads, when it actually contains information from both R1 and R2 read sets - this is under investigation (and is due to a Trimmomatic output file labelling issue). \\n* MultiQC results table formatted to show % of reads retained after trimming, table included in workflow report. \\n* Note: a known bug is that sometimes the workflow report text resets to default text. To restore, look for an earlier workflow version with correct workflow report text, and copy and paste report text into current version. '\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nAbout this workflow:\\n\\n* Run this workflow per tissue. \\n* Inputs: masked_genome.fasta and the trimmed RNAseq reads (R1 and R2) from one type of tissue. \\n* Index genome and align reads to genome with HISAT2, with default settings except for: Advanced options: spliced alignment options: specify options: Transcriptome assembly reporting: selected option: Report alignments tailored for transcript assemblers including StringTie (equivalent to -dta flag). \\n* Runs samtools sort to sort bam by coordinate. \\n* Runs StringTie to generate gtf from sorted bam. \\n* Output: transcripts.gtf from a single tissue.'\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nAbout this workflow:\\n\\n* Inputs: multiple transcriptome.gtfs from different tissues, genome.fasta, coding_seqs.fasta, non_coding_seqs.fasta \\n* Runs StringTie merge to combine transcriptomes, with default settings except for -m = 30 and -F = 0.1, to produce a merged_transcriptomes.gtf. \\n* Runs Convert GTF to BED12 with default settings, to produce a merged_transcriptomes.bed. \\n* Runs bedtools getfasta with default settings except for -name = yes, -s = yes, -split - yes, to produce a merged_transcriptomes.fasta\\n* Runs CPAT to generate seqs with high coding probability. \\n* Filters out non-coding seqs from the merged_transcriptomes.fasta\\n* Output: filtered_merged_transcriptomes.fasta'\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nAbout this workflow:\\n\\n* Input: merged_transcriptomes.fasta. \\n* Runs TransDecoder to produce longest_transcripts.fasta\\n* (Runs both the LongOrfs and Predict parts together. Default settings except Long Orfs options: -m =20)\\n* Runs Busco on output. '\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\n****\\n\\nAbout this workflow:\\n\\n* Inputs: transdecoder-peptides.fasta, transdecoder-nucleotides.fasta\\n* Runs many steps to convert outputs into the formats required for Fgenesh - .pro, .dat and .cdna'\n",
      " 'This is part of a series of workflows to annotate a genome, tagged with `TSI-annotation`. \\nThese workflows are based on command-line code by Luke Silver, converted into Galaxy Australia workflows. \\n\\nThe workflows can be run in this order: \\n* Repeat masking\\n* RNAseq QC and read trimming\\n* Find transcripts\\n* Combine transcripts\\n* Extract transcripts\\n* Convert formats\\n* Fgenesh annotation\\n\\nFor this workflow:\\n\\nInputs:\\n* assembled-genome.fasta\\n* hard-repeat-masked-genome.fasta\\n* If using the mRNAs option, the additional inputs required are .cdna, .pro and .dat files. \\n\\nWhat it does:\\n* This workflow splits the input genomes into single sequences (to decrease computation time), annotates using FgenesH++, and merges the output. \\n\\nOutputs:\\n* genome annotation in gff3 format\\n* fasta files of mRNAs, cDNAs and proteins\\n* Busco report\\n\\n\\n\\n\\n'\n",
      " '# beacon-omop-worker-workflows'\n",
      " '# GBMatch_CNN\\nWork in progress...\\nPredicting TS &amp; risk from glioblastoma whole slide images\\n\\n# Reference\\nUpcoming paper: stay tuned...\\n\\n# Dependencies\\npython 3.7.7\\n\\nrandaugment by Khrystyna Faryna: https://github.com/tovaroe/pathology-he-auto-augment\\n\\ntensorflow 2.1.0\\n\\nscikit-survival 0.13.1\\n\\npandas 1.0.3\\n\\nlifelines 0.25.0\\n\\n# Description\\nThe pipeline implemented here predicts transcriptional subtypes and survival of glioblastoma patients based on H&E stained whole slide scans. Sample data is provided in this repository. To test the basic functionality with 5-fold-CV simply run train_model_OS.py (for survival) or train_model_TS.py (for transcriptional subtypes). Please note that this will not reproduce the results from the manuscript, as only a small fraction of the image data can be provided in this repository due to size constraints. In order to reproduce the results from the manuscript, please refer to the step by step guide below. The whole dataset can be accessed at https://www.medical-epigenomics.org/papers/GBMatch/.\\nIf you wish to adopt this pipeline for your own use, please be sure to set the correct parameters in config.py.\\n\\nMoreover, we provide a fully trained model in gbm_predictor.py for predicting new samples (supported WSI formats are ndpi and svs). To use GBMPredictor, simply initialize by calling \\n`gbm_predictor = GBMPredictor()`\\nand predict your sample by calling\\n`(predicted_TS, risk_group, median_riskscore) = gbm_predictor.predict(*path_to_slidescan*)`\\nHeatmaps and detailed results will be automatically saved in a subfolder in your sample path.\\n\\n# Reproducing the manuscript results - step by step guide\\n\\n## Training the CNN model\\n1. Clone this repository and install the dependencies in your environment. Make sure that the path for randaugment is correctly set in the config.py (should be correct by default).\\n2. Download all included image tiles at https://doi.org/10.5281/zenodo.8358673 and replace the data/training/image_tiles folder with the image_tiles folder from zenodo.\\n3. Run train_model_OS.py and/or train_model_TS.py to reproduce the training with 5-fold cross validation. Models and results will be saved in the data/models folder.\\n4. Run train_final_model_OS.py and/or train_final_model_TS.py to train the final model on the whole training dataset.\\n\\n## Validate the CNN model on TCGA data\\n1. Download scans and clinical data of the TCGA glioblastoma cohort from https://www.cbioportal.org/ and/or https://portal.gdc.cancer.gov/\\n2. Copy tumor segmentations from GBMatch_CNN/data/validation/segmentation into the same folder as the TCGA slide scans\\n3. Predict TCGA samples with gbm_predictor (see above).\\n(You can also find all prediction results in GBMatch_CNN/data/validation/TCGA_annotation_prediction.csv.)\\n\\n## Evaluation of the tumor microenvironment\\n1. Install qupath 0.3.0 (newer versions should also work): https://qupath.github.io/.\\n2. Download immunohistochemical slides from https://www.medical-epigenomics.org/papers/GBMatch/.\\n3. Download annotation (IHC_geojsons) from https://doi.org/10.5281/zenodo.8358673.\\n4. Create a new project and import all immunohistochemical slides & annotations.\\n5. Copy the CD34 and HLA-DR thresholder from GBMatch_CNN/qupath into your project.\\n6. Run GBMatch_CNN/qupath/IHC_eval.groovy for all slides - immunohistochemistry results will be saved to a IHC_results-folder.\\n7. Create a new project and import all HE image tiles.\\n8. Run GBMatch_CNN/qupath/cellularity.groovy for all slides - cellularity results will be saved to a HE-results-folder.\\n'\n",
      " '# GSC (Genotype Sparse Compression)\\nGenotype Sparse Compression (GSC) is an advanced tool for lossless compression of VCF files, designed to efficiently store and manage VCF files in a compressed format. It accepts VCF/BCF files as input and utilizes advanced compression techniques to significantly reduce storage requirements while ensuring fast query capabilities. In our study, we successfully compressed the VCF files from the 1000 Genomes Project (1000Gpip3), consisting of 2504 samples and 80 million variants, from an uncompressed VCF file of 803.70GB to approximately 1GB.\\n\\n## Requirements \\n### GSC requires:\\n\\n- **Compiler Compatibility**: GSC requires a modern C++14-ready compiler, such as:\\n  - g++ version 10.1.0 or higher\\n\\n- **Build System**: Make build system is necessary for compiling GSC.\\n\\n- **Operating System**: GSC supports 64-bit operating systems, including:\\n  - Linux (Ubuntu 18.04)\\n  \\n## Installation\\nTo download, build and install GSC use the following commands.\\n```bash\\ngit clone https://github.com/luo-xiaolong/GSC.git\\ncd GSC\\nmake\\n```\\nTo clean the GSC build use:\\n```bash\\nmake clean\\n```\\n## Usage\\n```bash\\nUsage: gsc [option] [arguments] \\nAvailable options: \\n        compress - compress VCF/BCF file\\n        decompress     - query and decompress to VCF/BCF file\\n```\\n- Compress the input VCF/BCF file\\n```bash\\nUsage of gsc compress:\\n\\n        gsc compress [options] [--in [in_file]] [--out [out_file]]\\n\\nWhere:\\n\\n        [options]              Optional flags and parameters for compression.\\n        -i,  --in [in_file]    Specify the input file (default: VCF or VCF.GZ). If omitted, input is taken from standard input (stdin).\\n        -o,  --out [out_file]  Specify the output file. If omitted, output is sent to standard output (stdout).\\n\\nOptions:\\n\\n        -M,  --mode_lossly     Choose lossy compression mode (lossless by default).\\n        -b,  --bcf             Input is a BCF file (default: VCF or VCF.GZ).\\n        -p,  --ploidy [X]      Set ploidy of samples in input VCF to [X] (default: 2).\\n        -t,  --threads [X]     Set number of threads to [X] (default: 1).\\n        -d,  --depth [X]       Set maximum replication depth to [X] (default: 100, 0 means no matches).\\n        -m,  --merge [X]       Specify files to merge, separated by commas (e.g., -m chr1.vcf,chr2.vcf), or \\'@\\' followed by a file containing a list of VCF files (e.g., -m @file_with_IDs.txt). By default, all VCF files are compressed.\\n```\\n- Decompress / Query\\n```bash\\nUsage of gsc decompress and query:\\n\\n        gsc decompress [options] --in [in_file] --out [out_file]\\n\\nWhere:\\n        [options]              Optional flags and parameters for compression.\\n        -i,  --in [in_file]    Specify the input file . If omitted, input is taken from standard input (stdin).\\n        -o,  --out [out_file]  Specify the output file (default: VCF). If omitted, output is sent to standard output (stdout).\\n\\nOptions:\\n\\n    General Options:\\n\\n        -M,  --mode_lossly      Choose lossy compression mode (default: lossless).\\n        -b,  --bcf              Output a BCF file (default: VCF).\\n\\n    Filter options (applicable in lossy compression mode only): \\n\\n        -r,  --range [X]        Specify range in format [start],[end] (e.g., -r 4999756,4999852).\\n        -s,  --samples [X]      Samples separated by comms (e.g., -s HG03861,NA18639) OR \\'@\\' sign followed by the name of a file with sample name(s) separated by whitespaces (for exaple: -s @file_with_IDs.txt). By default all samples/individuals are decompressed. \\n        --header-only           Output only the header of the VCF/BCF.\\n        --no-header             Output without the VCF/BCF header (only genotypes).\\n        -G,  --no-genotype      Don\\'t output sample genotypes (only #CHROM, POS, ID, REF, ALT, QUAL, FILTER, and INFO columns).\\n        -C,  --out-ac-an        Write AC/AN to the INFO field.\\n        -S,  --split            Split output into multiple files (one per chromosome).\\n        -I, [ID=^]              Include only sites with specified ID (e.g., -I \"ID=rs6040355\").\\n        --minAC [X]             Include only sites with AC <= X.\\n        --maxAC [X]             Include only sites with AC >= X.\\n        --minAF [X]             Include only sites with AF >= X (X: 0 to 1).\\n        --maxAF [X]             Include only sites with AF <= X (X: 0 to 1).\\n        --min-qual [X]          Include only sites with QUAL >= X.\\n        --max-qual [X]          Include only sites with QUAL <= X.\\n```\\n## Example\\nThere is an example VCF/VCF.gz/BCF file, `toy.vcf`/`toy.vcf.gz`/`toy.bcf`, in the toy folder, which can be used to test GSC\\n### compress\\n\\n#### lossless compression:\\nThe input file format is VCF. You can compress a VCF file in lossless mode using one of the following methods:\\n1. **Explicit input and output file parameters**:\\n   \\n   Use the `--in` option to specify the input VCF file and the `--out` option for the output compressed file.\\n   ```bash\\n   ./gsc compress --in toy/toy.vcf --out toy/toy_lossless.gsc\\n   ```\\n2. **Input file parameter and output redirection**:\\n   \\n   Use the `--out` option for the output compressed file and redirect the input VCF file into the command.\\n   ```bash\\n   ./gsc compress --out toy/toy_lossless.gsc < toy/toy.vcf\\n   ```\\n3. **Output file redirection and input file parameter**:\\n   \\n   Specify the input VCF file with the `--in` option and redirect the output to create the compressed file.\\n   ```bash\\n   ./gsc compress --in toy/toy.vcf > toy/toy_lossless.gsc\\n   ```\\n4. **Input and output redirection**:\\n   \\n   Use shell redirection for both input and output. This method does not use the `--in` and `--out` options.\\n   ```bash\\n   ./gsc compress < toy/toy.vcf > toy/toy_lossless.gsc\\n   ```\\nThis will create a file:\\n* `toy_lossless.gsc` - The compressed archive of the entire VCF file.\\n\\n#### lossy compression:\\n\\nThe input file format is VCF. The commands are similar to those used for lossless compression, with the addition of the `-M` parameter to enable lossy compression.\\n\\n   For example, to compress a VCF file in lossy mode:\\n\\n   ```bash\\n   ./gsc compress -M --in toy/toy.vcf --out toy/toy_lossy.gsc\\n   ```\\n   Or using redirection:\\n   ```bash\\n   ./gsc compress -M --out toy/toy_lossy.gsc < toy/toy.vcf\\n   ``` \\n   This will create a file:\\n   * `toy_lossy.gsc` - The compressed archive of the entire VCF file is implemented with lossy compression. It only retains the \\'GT\\' subfield within the INFO and FORMAT fields, and excludes all other subfields..\\n    \\n### Decompress   (The commands are similar to those used for compression)\\nlossless decompression:\\n\\nTo decompress the compressed toy_lossless.gsc into a VCF file named toy_lossless.vcf:\\n```bash\\n./gsc decompress --in toy/toy_lossless.gsc --out toy/toy_lossless.vcf\\n```\\nlossy decompression:\\n\\nTo decompress the compressed toy_lossy.gsc into a VCF file named toy_lossy.vcf:\\n```bash\\n./gsc decompress -M --in toy/toy_lossy.gsc --out toy/toy_lossy.vcf\\n```\\n## Dockerfile\\nDockerfile can be used to build a Docker image with all necessary dependencies and GSC compressor. The image is based on Ubuntu 18.04. To build a Docker image and run a Docker container, you need Docker Desktop (https://www.docker.com). Example commands (run it within a directory with Dockerfile):\\n```bash\\ndocker build -t gsc_project .\\ndocker run -it gsc_project\\n```\\n## Citations\\n- **bio.tools ID**: `gsc_genotype_sparse_compression`\\n- **Research Resource Identifier (RRID)**: `SCR_025071`\\n'\n",
      " '# Galaxy Workflow Documentation: MS Finder Pipeline\\n\\nThis document outlines a MSFinder Galaxy workflow designed for peak annotation. The workflow consists of several steps aimed at preprocessing MS data, filtering, enhancing, and running MSFinder.\\n\\n## Step 1: Data Collection and Preprocessing\\nCollect if the inchi and smiles are missing from the dataset, and subsequently filter out the spectra which are missing inchi and smiles.\\n\\n### 1.1 MSMetaEnhancer: Collect InChi, Isomeric_smiles, and Nominal_mass\\n- Utilizes MSMetaEnhancer to collect InChi and Isomeric_smiles using PubChem and IDSM databases.\\n- Utilizes MSMetaEnhancer to collect MW using RDkit (For GOLM).\\n\\n### 1.2 replace key\\n- replace isomeric_smiles key to smiles using replace text tool\\n- replace MW key to parent_mass using replace text tool (For GOLM)\\n\\n### 1.3 Matchms Filtering\\n- Filters out invalid SMILES and InChi from the dataset using Matchms filtering.\\n\\n## Step 2: Complex Removal and Subsetting Dataset\\nRemoves coordination complexes from the dataset.\\n\\n### 2.1 Remove Complexes and Subset Data\\n- Removes complexes from the dataset.\\n- Exports metadata using Matchms metadata export, cuts the SMILES column, removes complexes using Rem_Complex tool, and updates the dataset using Matchms subsetting.\\n\\n## Step 3: Data Key Manipulation\\nAdd missing metadata required by the MSFinder for annotation.\\n\\n### 3.1 Matchms Remove Key\\n- Removes existing keys such as adduct, charge, and ionmode from the dataset.\\n\\n### 3.2 Matchms Add Key\\n- Adds necessary keys like charge, ionmode, and adduct to the dataset.\\n\\n### 3.3 Matchms Filtering\\n- Derives precursor m/z using parent mass and adduct information using matchms filtering.\\n\\n### 3.4 Matchms Convert\\n- Converts the dataset to Riken format for compatibility with MSFinder using matchms convert.\\n\\n## Step 4: Peak Annotation\\n### 4.1 Recetox-MSFinder\\n- Executes MSFinder with a 0.5 Da tolerance for both MS1 and MS2, including all element checks and an extended range for peak annotation.\\n\\n## Step 5: Error Handling and Refinement\\nCheck the MSFinder output to see if the output is the results or the log file. If the output is log file remove the smile from the dataset using matchms subsetting tool and rerun MSFinder.\\n\\n### 5.1 Error Handling\\n- Handles errors in peak annotation by removing SMILES that are not accepted by MSFinder.\\n- Reruns MSFinder after error correction or with different parameter (if applicable).\\n\\n## Step 6: High-res Annotation\\n### 6.1 High-Res Peak Overwriting\\n- Utilizes the Use_Theoretical_mz_Annotations tool to Overwrite experimentally measured mz values for peaks with theoretical values from peak comments.\\n'\n",
      " '# pod5_by_pore\\n\\nA Snakemake workflow to take the POD5 files produced by an Oxford Nanopore sequencing run and\\nre-batch them by pore (ie. by channel).\\n\\nThis is useful if you want to run duplex basecalling because you can meaningfully run\\n\"dorado duplex\" on a single (or a subset of) the POD5 files.\\n\\n## Know issues\\n\\nIt is assumed all POD5 input files are from the same sequencing run, but this is not checked.\\n'\n",
      " 'Use DADA2 for sequence quality control. DADA2 is a pipeline for detecting and correcting (where possible) Illumina amplicon sequence data. As implemented in the q2-dada2 plugin, this quality control process will additionally filter any phiX reads (commonly present in marker gene Illumina sequence data) that are identified in the sequencing data, and will filter chimeric sequences.'\n",
      " 'Importing single-end multiplexed data (not demultiplexed yet)'\n",
      " 'High-Performance Computing (HPC) environments are integral to quantum chemistry and computationally intense research, yet their complexity poses challenges for non-HPC experts. Navigating these environments proves challenging for researchers lacking extensive computational knowledge, hindering efficient use of domain specific research software. The prediction of mass spectra for in silico annotation is therefore inaccessible for many wet lab scientists. Our main goal is to facilitate non-experts in HPC navigate this complexity and make semi-empirical Quantum Chemistry (QC)-based predictions available without needing advanced computational skills.\\nTo address this challenge, a comprehensive approach is proposed. We chose specific file formats for storing molecular structures, ensuring compatibility across diverse tools and platforms. The xTB quantum chemistry package for molecular geometry optimization is leveraged for its capability to balance between accuracy and computational cost, making it well-suited for non-HPC focused applications. Integrating QC-based Mass Spectrometry (QCxMS) into Galaxy enables the prediction of mass spectra and offers insights into molecular composition and properties.\\nOur workflow demonstrates the utility of computing spectra using QCxMS along with complementary tools. We also present details of runtime performance metrics for four distinct molecules. This work highlights how non-HPC users can execute these predictions with ease, without requiring advanced computational skills. Additionally, a Docker image is created to encapsulate necessary tools, accompanied by user-friendly wrappers, simplifying the entire process for non-expert users.\\nWithin this context, potential improvements are considered, focusing on improving the Conda package for better performance by incorporating Fortran and Intel compiler optimizations. These considerations play a crucial role in refining the proposed methodology, enhancing user experience, and expanding the reach of semi-empirical predictions in quantum chemistry for mass spectra predictions'\n",
      " 'B/T cell repertoire analysis pipeline with immcantation framework. WIP, currently requires a bunch of changes first.'\n",
      " '# ![nf-core/ampliseq](docs/images/nf-core-ampliseq_logo.png)\\n\\n**16S rRNA amplicon sequencing analysis workflow using QIIME2**.\\n\\n[![nf-core](https://img.shields.io/badge/nf--core-pipeline-brightgreen.svg)](https://nf-co.re/)\\n[![DOI](https://zenodo.org/badge/150448201.svg)](https://zenodo.org/badge/latestdoi/150448201)\\n[![Cite Preprint](https://img.shields.io/badge/Cite%20Us!-Cite%20Publication-important)](https://doi.org/10.3389/fmicb.2020.550420)\\n\\n[![GitHub Actions CI Status](https://github.com/nf-core/ampliseq/workflows/nf-core%20CI/badge.svg)](https://github.com/nf-core/ampliseq/actions)\\n[![GitHub Actions Linting Status](https://github.com/nf-core/ampliseq/workflows/nf-core%20linting/badge.svg)](https://github.com/nf-core/ampliseq/actions)\\n[![Nextflow](https://img.shields.io/badge/nextflow-%E2%89%A520.04.0-brightgreen.svg)](https://www.nextflow.io/)\\n\\n[![install with bioconda](https://img.shields.io/badge/install%20with-bioconda-brightgreen.svg)](https://bioconda.github.io/)\\n[![Docker](https://img.shields.io/docker/automated/nfcore/ampliseq.svg)](https://hub.docker.com/r/nfcore/ampliseq)\\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23ampliseq-4A154B?logo=slack)](https://nfcore.slack.com/channels/ampliseq)\\n\\n## Introduction\\n\\n**nfcore/ampliseq** is a bioinformatics analysis pipeline used for 16S rRNA or ITS amplicon sequencing data (currently supported is Illumina paired end or PacBio).\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It comes with docker containers making installation trivial and results highly reproducible.\\n\\n## Quick Start\\n\\n1. Install [`nextflow`](https://nf-co.re/usage/installation)\\n\\n2. Install any of [`Docker`](https://docs.docker.com/engine/installation/), [`Singularity`](https://www.sylabs.io/guides/3.0/user-guide/) or [`Podman`](https://podman.io/) for full pipeline reproducibility _(please only use [`Conda`](https://conda.io/miniconda.html) as a last resort; see [docs](https://nf-co.re/usage/configuration#basic-configuration-profiles))_\\n\\n3. Download the pipeline and test it on a minimal dataset with a single command:\\n\\n    ```bash\\n    nextflow run nf-core/ampliseq -profile test,<docker/singularity/podman/conda/institute>\\n    ```\\n\\n    > Please check [nf-core/configs](https://github.com/nf-core/configs#documentation) to see if a custom config file to run nf-core pipelines already exists for your Institute. If so, you can simply use `-profile <institute>` in your command. This will enable either `docker` or `singularity` and set the appropriate execution settings for your local compute environment.\\n\\n4. Start running your own analysis!\\n\\n    ```bash\\n    nextflow run nf-core/ampliseq -profile <docker/singularity/podman/conda/institute> --input \"data\" --FW_primer GTGYCAGCMGCCGCGGTAA --RV_primer GGACTACNVGGGTWTCTAAT --metadata \"data/Metadata.tsv\"\\n    ```\\n\\nSee [usage docs](https://nf-co.re/ampliseq/usage) and [parameter docs](https://nf-co.re/ampliseq/parameters) for all of the available options when running the pipeline.\\n\\n## Pipeline Summary\\n\\nBy default, the pipeline currently performs the following:\\n\\n* Sequencing quality control ([FastQC](https://www.bioinformatics.babraham.ac.uk/projects/fastqc/))\\n* Trimming of reads ([Cutadapt](https://journal.embnet.org/index.php/embnetjournal/article/view/200))\\n* Illumina read processing with [QIIME2](https://www.nature.com/articles/s41587-019-0209-9)\\n* Infer Amplicon Sequence Variants (ASVs) ([DADA2](https://doi.org/10.1038/nmeth.3869))\\n* Taxonomical classification based on [SILVA](https://www.arb-silva.de/) [v132](https://www.arb-silva.de/documentation/release-132/) or [UNITE](https://unite.ut.ee/) database\\n* excludes unwanted taxa, produces absolute and relative feature/taxa count tables and plots, plots alpha rarefaction curves, computes alpha and beta diversity indices and plots thereof ([QIIME2](https://www.nature.com/articles/s41587-019-0209-9))\\n* Calls differentially abundant taxa ([ANCOM](https://www.ncbi.nlm.nih.gov/pubmed/26028277))\\n* Overall pipeline run summaries ([MultiQC](https://multiqc.info/))\\n\\n## Documentation\\n\\nThe nf-core/ampliseq pipeline comes with documentation about the pipeline: [usage](https://nf-co.re/ampliseq/usage) and [output](https://nf-co.re/ampliseq/output).\\n\\n## Credits\\n\\nnf-core/ampliseq was originally written by Daniel Straub ([@d4straub](https://github.com/d4straub)) and Alexander Peltzer ([@apeltzer](https://github.com/apeltzer)) for use at the [Quantitative Biology Center (QBiC)](http://www.qbic.life) and [Microbial Ecology, Center for Applied Geosciences](http://www.uni-tuebingen.de/de/104325), part of Eberhard Karls Universität Tübingen (Germany).\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline (in alphabetical order):\\n\\n* [Daniel Lundin](https://github.com/erikrikarddaniel)\\n* [Diego Brambilla](https://github.com/DiegoBrambilla)\\n* [Emelie Nilsson](https://github.com/emnilsson)\\n* [Jeanette Tångrot](https://github.com/jtangrot)\\n* [Sabrina Krakau](https://github.com/skrakau)\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don\\'t hesitate to get in touch on the [Slack `#ampliseq` channel](https://nfcore.slack.com/channels/ampliseq) (you can join with [this invite](https://nf-co.re/join/slack)).\\n\\n## Citations\\n\\nIf you use `nf-core/ampliseq` for your analysis, please cite the `ampliseq` article as follows:\\n> Daniel Straub, Nia Blackwell, Adrian Langarica-Fuentes, Alexander Peltzer, Sven Nahnsen, Sara Kleindienst **Interpretations of Environmental Microbial Community Studies Are Biased by the Selected 16S rRNA (Gene) Amplicon Sequencing Pipeline** *Frontiers in Microbiology* 2020, 11:2652 [doi: 10.3389/fmicb.2020.550420](https://doi.org/10.3389/fmicb.2020.550420).\\n\\nYou can cite the `nf-core/ampliseq` zenodo record for a specific version using the following [doi: 10.5281/zenodo.1493841](https://zenodo.org/badge/latestdoi/150448201)\\n\\nYou can cite the `nf-core` publication as follows:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n> ReadCube: [Full Access Link](https://rdcu.be/b1GjZ)\\n\\n<!-- TODO In addition, references of tools and data used in this pipeline are as follows: -->\\n\\n<!-- TODO nf-core: Add bibliography of tools and data used in your pipeline -->\\n'\n",
      " 'ATACSeq peak-calling and differential analysis pipeline.'\n",
      " 'Simple bacterial assembly and annotation pipeline.'\n",
      " 'A mapping-based pipeline for creating a phylogeny from bacterial whole genome sequences'\n",
      " 'Workflow converts one or multiple bam files back to the fastq format'\n",
      " 'CAGE-seq pipeline'\n",
      " 'An automated processing pipeline for mammalian bulk calling cards experiments'\n",
      " 'ChIP-seq peak-calling and differential analysis pipeline.'\n",
      " 'Pipeline for the identification of circular DNAs'\n",
      " 'CLIP analysis pipeline'\n",
      " '[![Build Status](https://travis-ci.org/maxibor/coproID.svg?branch=master)](https://travis-ci.org/maxibor/coproID)   [![Documentation Status](https://readthedocs.org/projects/coproid/badge/?version=latest)](https://coproid.readthedocs.io/en/latest/?badge=latest) [![License: GPL v3](https://img.shields.io/badge/License-GPL%20v3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)\\n\\n<img src=\"img/logo.png\" height=\"150\">  \\n\\n## Introduction\\n**CoproID** helps you to identify the *\"true maker\"* of a sequenced Coprolite by comparing the reads mapping to each suspect genome.\\n\\n## Documentation\\n\\nThe documentation of **coproID** can be found here [coproid.readthedocs.io](https://coproid.readthedocs.io)\\n\\n## Requirements\\n- [Conda](https://conda.io/miniconda.html)\\n- Nextflow (`conda install -c bioconda nextflow`)\\n\\n## How to run coproID\\n\\n```\\nnextflow run maxibor/coproid --genome1 \\'genome1.fa\\' --genome2 \\'genome2.fa\\' --name1 \\'Homo_sapiens\\' --name2 \\'Canis_familiaris\\' --reads \\'*_R{1,2}.fastq.gz\\'\\n```\\n\\n## Pipeline overview\\n\\n![](img/dag.png)\\n\\n## Get Help\\n\\n```\\n$ nextflow run maxibor/coproid --help\\nN E X T F L O W  ~  version 0.31.1\\nLaunching `maxibor/coproid` [special_laplace] - revision: b579d0002f\\n\\n=========================================\\n coproID: Coprolite Identification\\n Homepage: https://github.com/maxibor/coproid\\n Documentation: https://coproid.readthedocs.io\\n Author: Maxime Borry <borry@shh.mpg.de>\\n Version 0.6\\n Last updated on October 11th, 2018\\n=========================================\\nUsage:\\nThe typical command for running the pipeline is as follows:\\nnextflow run maxibor/coproid --genome1 \\'genome1.fa\\' --genome2 \\'genome2.fa\\' --name1 \\'Homo_sapiens\\' --name2 \\'Canis_familiaris\\' --reads \\'*_R{1,2}.fastq.gz\\'\\nMandatory arguments:\\n  --reads                       Path to input data (must be surrounded with quotes)\\n  --name1                       Name of candidate 1. Example: \"Homo sapiens\"\\n  --name2                       Name of candidate 2. Example: \"Canis familiaris\"\\n  --genome1                     Path to candidate 1 Coprolite maker\\'s genome fasta file (must be surrounded with quotes)\\n  --genome2                     Path to candidate 2 Coprolite maker\\'s genome fasta file (must be surrounded with quotes)\\n\\nOptions:\\n  --phred                       Specifies the fastq quality encoding (33 | 64). Defaults to 33\\n  --index1                      Path to Bowtie2 index genome andidate 1 Coprolite maker\\'s genome, in the form of /path/to/*.bt2 - Required if genome1 is not set\\n  --index2                      Path to Bowtie2 index genome andidate 2 Coprolite maker\\'s genome, in the form of /path/to/*.bt2 - Required if genome2 is not set\\n  --collapse                    Specifies if AdapterRemoval should merge the paired-end sequences or not (yes |\\xa0no). Default = yes\\n  --identity                    Identity threshold to retain read alignment. Default = 0.95\\n  --bowtie                      Bowtie settings for sensivity (very-fast | very-sensitive). Default = very-sensitive\\nOther options:\\n  --results                     Name of result directory. Defaults to ./results\\n  --help  --h                   Shows this help page\\n```\\n'\n",
      " 'Pipeline for the analysis of crispr data'\n",
      " 'Analysis pipeline for CUT&RUN and CUT&TAG experiments that includes sequencing QC, spike-in normalisation, IgG control normalisation, peak calling and downstream peak analysis.'\n",
      " 'Google DeepVariant variant caller as a Nextflow pipeline'\n",
      " 'Demultiplexing pipeline for Illumina sequencing data'\n",
      " 'A pipeline to identify (and remove) certain sequences from raw genomic data. Default taxa to identify (and remove) are Homo and Homo sapiens. Removal is optional.'\n",
      " 'Automated quantitative analysis of DIA proteomics mass spectrometry measurements.'\n",
      " 'Differential abundance analysis' 'Dual RNA-seq pipeline'\n",
      " 'A fully reproducible and state-of-the-art ancient DNA analysis pipeline'\n",
      " 'A fully reproducible and state of the art epitope prediction pipeline.'\n",
      " 'fgbio Best Practices FASTQ to Consensus Pipeline'\n",
      " 'Pipeline to fetch metadata and raw FastQ files from public databases'\n",
      " 'Pipeline for screening for functional components of assembled contigs'\n",
      " 'A pipeline to investigate horizontal gene transfer from NGS data'\n",
      " 'Analysis of Chromosome Conformation Capture data (Hi-C)'\n",
      " 'This pipeline analyses data for HiCAR data, a robust and sensitive multi-omic co-assay for simultaneous measurement of transcriptome, chromatin accessibility and cis-regulatory chromatin contacts.'\n",
      " 'Precision HLA typing from next-generation sequencing data.'\n",
      " 'Image Mass Cytometry analysis pipeline.'\n",
      " 'Genes and transcripts annotation with Isoseq using uLTRA and TAMA'\n",
      " 'Compare DNA/RNA/protein sequences on k-mer content'\n",
      " 'Assembly, binning and annotation of metagenomes'\n",
      " 'MARS-seq v2 preprocessing pipeline'\n",
      " 'Assembly and annotation of metatranscriptomic data, both prokaryotic and eukaryotic'\n",
      " 'Methylation (Bisulfite-Sequencing) Best Practice analysis pipeline, part of the nf-core community.'\n",
      " 'Identify and quantify peptides from mass spectrometry raw data'\n",
      " 'MNase-seq analysis pipeline using BWA and DANPOS2.'\n",
      " 'An analysis pipeline for Molecular Cartography data from Resolve Biosciences.'\n",
      " 'A pipeline to demultiplex, QC and map Nanopore data'\n",
      " 'A Nanostring nCounter analysis pipeline'\n",
      " 'Nascent Transcription Processing Pipeline'\n",
      " 'De novo assembly pipeline for 10X linked-reads, used at the SciLifeLab National Genomics Infrastructure.'\n",
      " 'A comprehensive cancer NGS analysis and reporting pipeline'\n",
      " 'The pangenome graph construction pipeline renders a collection of sequences into a pangenome graph. Its goal is to build a graph that is locally directed and acyclic while preserving large-scale variation. Maintaining local linearity is important for interpretation, visualization, mapping, comparative genomics, and reuse of pangenome graphs'\n",
      " 'Proteogenomics database creation workflow using pypgatk framework.'\n",
      " 'Performs phylogenetic placement with EPA-NG'\n",
      " 'Pipeline for analysis of Molecular Pixelation assays'\n",
      " 'Protein 3D structure prediction pipeline'\n",
      " 'Proteomics label-free quantification (LFQ) analysis pipeline using OpenMS and MSstats, with feature quantification, feature summarization, quality control and group-based statistical analysis.'\n",
      " 'Quantitative Mass Spectrometry nf-core workflow'\n",
      " 'call and score variants from WGS/WES of rare disease patients'\n",
      " 'A workflow to simulate reads'\n",
      " 'Analysis of ribosome profiling, or Ribo-seq (also named ribosome footprinting)'\n",
      " 'Nextflow rnafusion analysis pipeline, part of the nf-core community.'\n",
      " 'Alternative splicing analysis using RNA-seq.'\n",
      " 'GATK4 RNA variant calling pipeline'\n",
      " 'An open-source analysis pipeline to detect germline or somatic variants from whole genome or targeted sequencing'\n",
      " 'Pipeline for processing 10x Genomics single cell rnaseq data'\n",
      " 'SLAMseq analysis using Slamdunk with various T>C conversion quantifications and QC'\n",
      " 'Small RNA-Seq Best Practice Analysis Pipeline.'\n",
      " 'Taxonomic classification and profiling of shotgun metagenomic data'\n",
      " 'Integration of viral sequences in genomic data'\n",
      " 'Assembly and intrahost/low-frequency variant calling for viral samples'\n",
      " '# ![IMPaCT program](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/png/impact_data_logo_pink_horitzontal.png)\\n\\n[![IMPaCT](https://img.shields.io/badge/Web%20-IMPaCT-blue)](https://impact.isciii.es/)\\n[![IMPaCT-isciii](https://img.shields.io/badge/Web%20-IMPaCT--isciii-red)](https://www.isciii.es/QueHacemos/Financiacion/IMPaCT/Paginas/default.aspx)\\n[![IMPaCT-Data](https://img.shields.io/badge/Web%20-IMPaCT--Data-1d355c.svg?labelColor=000000)](https://impact-data.bsc.es/)\\n\\n## Introduction of the project\\n\\nIMPaCT-Data is the IMPaCT program that aims to support the development of a common, interoperable and integrated system for the collection and analysis of clinical and molecular data by providing the knowledge and resources available in the Spanish Science and Technology System. This development will make it possible to answer research questions based on the different clinical and molecular information systems available. Fundamentally, it aims to provide researchers with a population perspective based on individual data.\\n\\nThe IMPaCT-Data project is divided into different work packages (WP). In the context of IMPaCT-Data WP3 (Genomics), a working group of experts worked on the generation of a specific quality control (QC) workflow for germline exome samples.\\n\\nTo achieve this, a set of metrics related to human genomic data was decided upon, and the toolset or software to extract these metrics was implemented in an existing variant calling workflow called Sarek, part of the nf-core community. The final outcome is a Nextflow subworkflow, called IMPaCT-QC implemented in the Sarek pipeline.\\n\\nBelow you can find the explanation of this workflow (raw pipeline), the link to the documentation of the IMPaCT QC subworkflow and a linked documentation associated to the QC metrics added in the mentioned workflow.\\n\\n- [IMPaCT-data subworkflow documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/tree/master/impact_qc)\\n\\n- [Metrics documentation](https://github.com/EGA-archive/sarek-IMPaCT-data-QC/blob/master/impact_qc/docs/QC_Sarek_supporing_documentation.pdf)\\n\\n<h1>\\n  <picture>\\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"docs/images/nf-core-sarek_logo_dark.png\">\\n    <img alt=\"nf-core/sarek\" src=\"docs/images/nf-core-sarek_logo_light.png\">\\n  </picture>\\n</h1>\\n\\n[![GitHub Actions CI Status](https://github.com/nf-core/sarek/actions/workflows/ci.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/ci.yml)\\n[![GitHub Actions Linting Status](https://github.com/nf-core/sarek/actions/workflows/linting.yml/badge.svg)](https://github.com/nf-core/sarek/actions/workflows/linting.yml)\\n[![AWS CI](https://img.shields.io/badge/CI%20tests-full%20size-FF9900?labelColor=000000&logo=Amazon%20AWS)](https://nf-co.re/sarek/results)\\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\\n[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.3476425-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.3476425)\\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda](http://img.shields.io/badge/run%20with-conda-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://tower.nf/launch?pipeline=https://github.com/nf-core/sarek)\\n\\n[![Get help on Slack](http://img.shields.io/badge/slack-nf--core%20%23sarek-4A154B?labelColor=000000&logo=slack)](https://nfcore.slack.com/channels/sarek)\\n[![Follow on Twitter](http://img.shields.io/badge/twitter-%40nf__core-1DA1F2?labelColor=000000&logo=twitter)](https://twitter.com/nf_core)\\n[![Follow on Mastodon](https://img.shields.io/badge/mastodon-nf__core-6364ff?labelColor=FFFFFF&logo=mastodon)](https://mstdn.science/@nf_core)\\n[![Watch on YouTube](http://img.shields.io/badge/youtube-nf--core-FF0000?labelColor=000000&logo=youtube)](https://www.youtube.com/c/nf-core)\\n\\n## Introduction\\n\\n**nf-core/sarek** is a workflow designed to detect variants on whole genome or targeted sequencing data. Initially designed for Human, and Mouse, it can work on any species with a reference genome. Sarek can also handle tumour / normal pairs and could include additional relapses.\\n\\nThe pipeline is built using [Nextflow](https://www.nextflow.io), a workflow tool to run tasks across multiple compute infrastructures in a very portable manner. It uses Docker/Singularity containers making installation trivial and results highly reproducible. The [Nextflow DSL2](https://www.nextflow.io/docs/latest/dsl2.html) implementation of this pipeline uses one container per process which makes it much easier to maintain and update software dependencies. Where possible, these processes have been submitted to and installed from [nf-core/modules](https://github.com/nf-core/modules) in order to make them available to all nf-core pipelines, and to everyone within the Nextflow community!\\n\\nOn release, automated continuous integration tests run the pipeline on a full-sized dataset on the AWS cloud infrastructure. This ensures that the pipeline runs on AWS, has sensible resource allocation defaults set to run on real-world datasets, and permits the persistent storage of results to benchmark between pipeline releases and other analysis sources. The results obtained from the full-sized test can be viewed on the [nf-core website](https://nf-co.re/sarek/results).\\n\\nIt\\'s listed on [Elixir - Tools and Data Services Registry](https://bio.tools/nf-core-sarek) and [Dockstore](https://dockstore.org/workflows/github.com/nf-core/sarek).\\n\\n<p align=\"center\">\\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_workflow.png\" width=30%>\\n</p>\\n\\n## Pipeline summary\\n\\nDepending on the options and samples provided, the pipeline can currently perform the following:\\n\\n- Form consensus reads from UMI sequences (`fgbio`)\\n- Sequencing quality control and trimming (enabled by `--trim_fastq`) (`FastQC`, `fastp`)\\n- Map Reads to Reference (`BWA-mem`, `BWA-mem2`, `dragmap` or `Sentieon BWA-mem`)\\n- Process BAM file (`GATK MarkDuplicates`, `GATK BaseRecalibrator` and `GATK ApplyBQSR` or `Sentieon LocusCollector` and `Sentieon Dedup`)\\n- Summarise alignment statistics (`samtools stats`, `mosdepth`)\\n- Variant calling (enabled by `--tools`, see [compatibility](https://nf-co.re/sarek/latest/docs/usage#which-variant-calling-tool-is-implemented-for-which-data-type)):\\n  - `ASCAT`\\n  - `CNVkit`\\n  - `Control-FREEC`\\n  - `DeepVariant`\\n  - `freebayes`\\n  - `GATK HaplotypeCaller`\\n  - `Manta`\\n  - `mpileup`\\n  - `MSIsensor-pro`\\n  - `Mutect2`\\n  - `Sentieon Haplotyper`\\n  - `Strelka2`\\n  - `TIDDIT`\\n- Variant filtering and annotation (`SnpEff`, `Ensembl VEP`, `BCFtools annotate`)\\n- Summarise and represent QC (`MultiQC`)\\n\\n<p align=\"center\">\\n    <img title=\"Sarek Workflow\" src=\"docs/images/sarek_subway.png\" width=60%>\\n</p>\\n\\n## Usage\\n\\n> [!NOTE]\\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\\n\\nFirst, prepare a samplesheet with your input data that looks as follows:\\n\\n`samplesheet.csv`:\\n\\n```csv\\npatient,sample,lane,fastq_1,fastq_2\\nID1,S1,L002,ID1_S1_L002_R1_001.fastq.gz,ID1_S1_L002_R2_001.fastq.gz\\n```\\n\\nEach row represents a pair of fastq files (paired end).\\n\\nNow, you can run the pipeline using:\\n\\n```bash\\nnextflow run nf-core/sarek \\\\\\n   -profile <docker/singularity/.../institute> \\\\\\n   --input samplesheet.csv \\\\\\n   --outdir <OUTDIR>\\n```\\n\\n> [!WARNING]\\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\\n\\nFor more details and further functionality, please refer to the [usage documentation](https://nf-co.re/sarek/usage) and the [parameter documentation](https://nf-co.re/sarek/parameters).\\n\\n## Pipeline output\\n\\nTo see the results of an example test run with a full size dataset refer to the [results](https://nf-co.re/sarek/results) tab on the nf-core website pipeline page.\\nFor more details about the output files and reports, please refer to the\\n[output documentation](https://nf-co.re/sarek/output).\\n\\n## Benchmarking\\n\\nOn each release, the pipeline is run on 3 full size tests:\\n\\n- `test_full` runs tumor-normal data for one patient from the SEQ2C consortium\\n- `test_full_germline` runs a WGS 30X Genome-in-a-Bottle(NA12878) dataset\\n- `test_full_germline_ncbench_agilent` runs two WES samples with 75M and 200M reads (data available [here](https://github.com/ncbench/ncbench-workflow#contributing-callsets)). The results are uploaded to Zenodo, evaluated against a truth dataset, and results are made available via the [NCBench dashboard](https://ncbench.github.io/report/report.html#).\\n\\n## Credits\\n\\nSarek was originally written by Maxime U Garcia and Szilveszter Juhos at the [National Genomics Infastructure](https://ngisweden.scilifelab.se) and [National Bioinformatics Infastructure Sweden](https://nbis.se) which are both platforms at [SciLifeLab](https://scilifelab.se), with the support of [The Swedish Childhood Tumor Biobank (Barntumörbanken)](https://ki.se/forskning/barntumorbanken).\\nFriederike Hanssen and Gisela Gabernet at [QBiC](https://www.qbic.uni-tuebingen.de/) later joined and helped with further development.\\n\\nThe Nextflow DSL2 conversion of the pipeline was lead by Friederike Hanssen and Maxime U Garcia.\\n\\nMaintenance is now lead by Friederike Hanssen and Maxime U Garcia (now at [Seqera Labs](https://seqera/io))\\n\\nMain developers:\\n\\n- [Maxime U Garcia](https://github.com/maxulysse)\\n- [Friederike Hanssen](https://github.com/FriederikeHanssen)\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline:\\n\\n- [Abhinav Sharma](https://github.com/abhi18av)\\n- [Adam Talbot](https://github.com/adamrtalbot)\\n- [Adrian Lärkeryd](https://github.com/adrlar)\\n- [Alexander Peltzer](https://github.com/apeltzer)\\n- [Alison Meynert](https://github.com/ameynert)\\n- [Anders Sune Pedersen](https://github.com/asp8200)\\n- [arontommi](https://github.com/arontommi)\\n- [BarryDigby](https://github.com/BarryDigby)\\n- [Bekir Ergüner](https://github.com/berguner)\\n- [bjornnystedt](https://github.com/bjornnystedt)\\n- [cgpu](https://github.com/cgpu)\\n- [Chela James](https://github.com/chelauk)\\n- [David Mas-Ponte](https://github.com/davidmasp)\\n- [Edmund Miller](https://github.com/edmundmiller)\\n- [Francesco Lescai](https://github.com/lescai)\\n- [Gavin Mackenzie](https://github.com/GCJMackenzie)\\n- [Gisela Gabernet](https://github.com/ggabernet)\\n- [Grant Neilson](https://github.com/grantn5)\\n- [gulfshores](https://github.com/gulfshores)\\n- [Harshil Patel](https://github.com/drpatelh)\\n- [James A. Fellows Yates](https://github.com/jfy133)\\n- [Jesper Eisfeldt](https://github.com/J35P312)\\n- [Johannes Alneberg](https://github.com/alneberg)\\n- [José Fernández Navarro](https://github.com/jfnavarro)\\n- [Júlia Mir Pedrol](https://github.com/mirpedrol)\\n- [Ken Brewer](https://github.com/kenibrewer)\\n- [Lasse Westergaard Folkersen](https://github.com/lassefolkersen)\\n- [Lucia Conde](https://github.com/lconde-ucl)\\n- [Malin Larsson](https://github.com/malinlarsson)\\n- [Marcel Martin](https://github.com/marcelm)\\n- [Nick Smith](https://github.com/nickhsmith)\\n- [Nicolas Schcolnicov](https://github.com/nschcolnicov)\\n- [Nilesh Tawari](https://github.com/nilesh-tawari)\\n- [Nils Homer](https://github.com/nh13)\\n- [Olga Botvinnik](https://github.com/olgabot)\\n- [Oskar Wacker](https://github.com/WackerO)\\n- [pallolason](https://github.com/pallolason)\\n- [Paul Cantalupo](https://github.com/pcantalupo)\\n- [Phil Ewels](https://github.com/ewels)\\n- [Sabrina Krakau](https://github.com/skrakau)\\n- [Sam Minot](https://github.com/sminot)\\n- [Sebastian-D](https://github.com/Sebastian-D)\\n- [Silvia Morini](https://github.com/silviamorins)\\n- [Simon Pearce](https://github.com/SPPearce)\\n- [Solenne Correard](https://github.com/scorreard)\\n- [Susanne Jodoin](https://github.com/SusiJo)\\n- [Szilveszter Juhos](https://github.com/szilvajuhos)\\n- [Tobias Koch](https://github.com/KochTobi)\\n- [Winni Kretzschmar](https://github.com/winni2k)\\n\\n## Acknowledgements\\n\\n|      [![Barntumörbanken](docs/images/BTB_logo.png)](https://ki.se/forskning/barntumorbanken)      |            [![SciLifeLab](docs/images/SciLifeLab_logo.png)](https://scilifelab.se)             |\\n| :-----------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------: |\\n| [![National Genomics Infrastructure](docs/images/NGI_logo.png)](https://ngisweden.scilifelab.se/) | [![National Bioinformatics Infrastructure Sweden](docs/images/NBIS_logo.png)](https://nbis.se) |\\n|              [![QBiC](docs/images/QBiC_logo.png)](https://www.qbic.uni-tuebingen.de)              |                   [![GHGA](docs/images/GHGA_logo.png)](https://www.ghga.de/)                   |\\n|                     [![DNGC](docs/images/DNGC_logo.png)](https://eng.ngc.dk/)                     |                                                                                                |\\n\\n## Contributions & Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\nFor further information or help, don\\'t hesitate to get in touch on the [Slack `#sarek` channel](https://nfcore.slack.com/channels/sarek) (you can join with [this invite](https://nf-co.re/join/slack)), or contact us: [Maxime U Garcia](mailto:maxime.garcia@seqera.io?subject=[GitHub]%20nf-core/sarek), [Friederike Hanssen](mailto:friederike.hanssen@qbic.uni-tuebingen.de?subject=[GitHub]%20nf-core/sarek)\\n\\n## Citations\\n\\nIf you use `nf-core/sarek` for your analysis, please cite the `Sarek` article as follows:\\n\\n> Friederike Hanssen, Maxime U Garcia, Lasse Folkersen, Anders Sune Pedersen, Francesco Lescai, Susanne Jodoin, Edmund Miller, Oskar Wacker, Nicholas Smith, nf-core community, Gisela Gabernet, Sven Nahnsen **Scalable and efficient DNA sequencing analysis on different compute infrastructures aiding variant discovery** _NAR Genomics and Bioinformatics_ Volume 6, Issue 2, June 2024, lqae031, [doi: 10.1093/nargab/lqae031](https://doi.org/10.1093/nargab/lqae031).\\n\\n> Garcia M, Juhos S, Larsson M et al. **Sarek: A portable workflow for whole-genome sequencing analysis of germline and somatic variants [version 2; peer review: 2 approved]** _F1000Research_ 2020, 9:63 [doi: 10.12688/f1000research.16665.2](http://dx.doi.org/10.12688/f1000research.16665.2).\\n\\nYou can cite the sarek zenodo record for a specific version using the following [doi: 10.5281/zenodo.3476425](https://doi.org/10.5281/zenodo.3476425)\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nYou can cite the `nf-core` publication as follows:\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n\\n## CHANGELOG\\n\\n- [CHANGELOG](CHANGELOG.md)\\n'\n",
      " '# Monte Carlo Pi Estimation Program Description\\n\\nThis program is a Monte Carlo simulation designed to estimate the value of Pi using PyCOMPSs.\\n\\n## Tasks in the Program\\n\\n1. **Count Points in Circle Task (`count_points_in_circle`)**:\\n   - Generates random points within a square with side length 1.\\n   - Counts points falling within the inscribed circle (x^2 + y^2 <= 1).\\n   - Input: Number of points to generate (num_points)\\n   - Output: Tuple containing count of points within the circle and list of generated points\\n\\n2. **Write Points to File Task (`write_points_to_file`)**:\\n   - Writes a list of points to a file named according to the task ID.\\n   - Input: List of points and task ID\\n   - Output: None\\n\\n## Main Function Operation\\n\\n- Takes num_points and num_tasks as input.\\n- Divides points among tasks for parallel processing.\\n- Launches count_points_in_circle tasks in parallel.\\n- Launches write_points_to_file tasks after count tasks complete.\\n- Calculates Pi estimate and writes to Result.txt.\\n\\n\\n## Execution\\n\\nTo execute the script, use the following command-line format:\\n\\n```bash\\npython script_name.py num_points num_tasks\\n'\n",
      " 'Predict variants and drug resistance from M. tuberculosis sequence samples (Illumina)'\n",
      " '# CNVand\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥8.0.0-brightgreen.svg?style=flat-square)](https://snakemake.bitbucket.io)\\n[![Conda](https://img.shields.io/badge/conda-≥23.11.0-brightgreen.svg?style=flat-square)](https://anaconda.org/conda-forge/mamba)\\n![Docker](https://img.shields.io/badge/docker-≥26.1.4-brightgreen.svg?style=flat-square)\\n![License](https://img.shields.io/badge/license-MIT-blue.svg?style=flat-square)\\n[![Contributor Covenant](https://img.shields.io/badge/Contributor%20Covenant-2.1-4baaaa.svg)](code_of_conduct.md) \\n\\nCNVand is a snakemake workflow for CNV analysis, tailored for preparing data used by the [CNVizard](https://github.com/IHGGM-Aachen/CNVizard) CNV visualization tool. Given a set of BAM and VCF files, it utilizes the tools `CNVkit` and `AnnotSV` to analyze and annotate copy number variations.\\n\\n## General Settings and Samplesheet\\nTo configure this pipeline, modify the config under `config/config.yaml` as needed. Detailed explanations for each setting are provided within the file.\\n\\nAdd samples to the pipeline by completing `config/samplesheet.tsv`. Each `sample` should be associated with a `path` to the corresponding BAM and VCF file.\\n\\nFor detailed instructions on how to configure CNVand see `config/README.md`.\\n\\n## Reference Files\\nTo use CNVand some external reference files are needed alongside your sample data.\\n\\n### Genome\\n\\nFor `cnvkit_fix` to work, you need to specify a reference genome in the config file. Take care to use the same reference file for your entire workflow!\\n\\n### Annotations\\n\\nFor AnnotSV to work, the annotation files must be downloaded separately and be referenced in the config file under the respective key. For human annotations, this can be done [here](https://www.lbgi.fr/~geoffroy/Annotations/Annotations_Human_3.4.2.tar.gz). In case this link is not working, check the original [AnnotSV](https://github.com/lgmgeo/AnnotSV/tree/master) repository for updates on how to obtain the annotations.\\n\\n## Pipeline Setup\\nCNVand can be executed using mamba environments or a pre-built docker container.\\n\\n### Mamba (Snakedeploy)\\nFor a one-click installation, snakedeploy can be used. For further information, see the entry for CNVand in the [Snakemake Workflow Catalog](https://snakemake.github.io/snakemake-workflow-catalog/?repo=IHGGM-Aachen/CNVand)\\n\\n### Mamba (Manual)\\nThis workflow can easily setup manually with the given environment file. Install Snakemake and dependencies using the command:\\n\\n```bash\\nmamba env create -f environment.yml\\n```\\n\\nThen activate the newly created environment with: \\n\\n```bash\\nmamba activate cnvand\\n```\\n\\nNow configure the pipeline and download the needed annotation and refenrece files. When everything is set up, Execute the pipeline with:\\n\\n```bash\\nsnakemake --cores all --use-conda\\n```\\n\\nGenerate a comprehensive execution report by running:\\n\\n```bash\\nsnakemake --report report.zip\\n```\\n\\n\\n### Docker\\n\\nCNVand can also be used inside a Docker container. To do so, first pull the Docker image with:\\n\\n```bash\\ndocker pull ghcr.io/ihggm-aachen/cnvand:latest\\n```\\n\\nThen run the container with the bind mounts needed in your setup:\\n\\n```bash\\ndocker run -it -v /path/to/your/data:/data ghcr.io/ihggm-aachen/cnvand:latest /bin/bash\\n```\\n\\nThis command opens an interactive shell inside the Docker container. Once inside the container, you are placed inside the `/cnvand` the directory. From there then run the pipeline once you set an appropriate configuration:\\n\\n```bash\\nsnakemake --cores all --use-conda\\n```\\n\\n## Contributing\\n\\nWe welcome contributions to improve CNVand. Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to get started.\\n\\n## Code of Conduct\\n\\nWe are committed to fostering an open and welcoming environment. Please see our [CODE_OF_CONDUCT.md](CODE_OF_CONDUCT.md) for our community guidelines.\\n\\n## Documentation\\n\\nDetailed documentation for the workflow can be found in `workflow/documentation.md`.\\n\\n## Testing\\n\\nTo ensure the pipeline runs correctly, we have set up both unit and integration tests. Unit tests are generated from successful workflow runs, and integration tests are configured to run the entire workflow with test data.\\n\\n### Integration Tests\\n\\nThe integration test can be run using the data and config provided. Remember to download the correct reference/annotations (GRCh38 in case of the bundled NIST data) by yourself and adjust your local paths as necessary!\\n\\n### Unit Tests\\n\\nRun the unit tests with:\\n\\n```bash\\npytest -v .tests/unit\\n```\\n\\nThis will check for the correct CNVand output per rule.\\n\\n## License\\n\\nThis project is licensed under the MIT License. See the [LICENSE](LICENSE.md) file for details.\\n'\n",
      " 'Workflow for gene set enrichment analsysis (GSEA) and co-expression analysis (WGCNA) on transcriptomics data to analyze pathways affected in Porto-Sinusoidal Vascular Disease.'\n",
      " 'A pipeline for ortholog fetching and analysis'\n",
      " 'Assembly of bacterial paired-end short read data with generation of quality metrics and reports'\n",
      " 'COMPSs Matrix Multiplication, out-of-core using files. Hypermatrix size used 2x2 blocks (MSIZE=2), block size used 2x2 elements (BSIZE=2)'\n",
      " \"**Name:** SparseLU \\n**Contact Person:** support-compss@bsc.es \\n**Access Level:** public \\n**License Agreement:** Apache2 \\n**Platform:** COMPSs \\n\\n# Description\\nThe Sparse LU application computes an LU matrix factorization on a sparse blocked matrix. The matrix size (number of blocks) and the block size are parameters of the application. \\n\\nAs the algorithm progresses, the area of the matrix that is accessed is smaller; concretely, at each iteration, the 0th row and column of the current matrix are discarded. On the other hand, due to the sparseness of the matrix, some of its blocks might not be allocated and, therefore, no work is generated for them.\\n\\nWhen executed with COMPSs, Sparse LU produces several types of task with different granularity and numerous dependencies between them.\\n\\n# Versions\\nThere are three versions of Sparse LU, depending on the data types used to store the blocks.\\n## Version 1\\n''files'', where the matrix blocks are stored in files.\\n## Version 2\\n''objects'', where the matrix blocks are represented by objects.\\n## Version 3\\n''arrays'', where the matrix blocks are stored in arrays.\\n\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss sparseLU.files.SparseLU numberOfBlocks blockSize\\nruncompss sparseLU.objects.SparseLU numberOfBlocks blockSize\\nruncompss sparseLU.arrays.SparseLU numberOfBlocks blockSize\\n```\\n\\nwhere:\\n  * numberOfBlocks: Number of blocks inside each matrix\\n  * blockSize: Size of each block\\n\\n\\n# Execution Example\\n```\\nruncompss sparseLU.objects.SparseLU 16 4 \\nruncompss sparseLU.files.SparseLU 16 4\\nruncompss sparseLU.arrays.SparseLU 16 4 \\n```\\n\\n\\n# Build\\n## Option 1: Native java\\n```\\ncd application_sources/; javac src/main/java/sparseLU/*/*.java\\ncd src/main/java/; jar cf sparseLU.jar sparseLU/\\ncd ../../../; mv src/main/java/sparseLU.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\ncd application_sources/\\nmvn clean package\\n```\\n\"\n",
      " 'Antimicrobial resistance gene detection from assembled bacterial genomes'\n",
      " 'Annotation of an assembled bacterial genomes to detect genes, potential plasmids, integrons and Insertion sequence (IS) elements.'\n",
      " 'This workflow takes a collection of BAM (output of STAR) and a gtf. It extends the input gtf using de novo annotation.'\n",
      " 'Short paired-end read analysis to provide quality analysis, read cleaning and taxonomy assignation'\n",
      " '# Scaffolding using HiC data with YAHS\\n\\nThis workflow has been created from a Vertebrate Genomes Project (VGP) scaffolding workflow. \\n\\n* For more information about the VGP project see https://galaxyproject.org/projects/vgp/. \\n* The scaffolding workflow is at https://dockstore.org/workflows/github.com/iwc-workflows/Scaffolding-HiC-VGP8/main:main?tab=info\\n* Please see that link for the workflow diagram. \\n\\nSome minor changes have been made to better fit with TSI project data: \\n\\n* optional inputs of SAK info and sequence graph have been removed\\n* the required input format for the genome is changed from gfa to fasta\\n* the estimated genome size now requires user input rather than being extracted from output of a previous workflow.  \\n\\nInputs: \\n\\n* assembly.fasta  [note - scaffolding is done only one haplotype at a time. eg hap1 or primary]\\n* Concatenated HiC forward reads in fastqsanger.gz\\n* Concatenated HiC reverse reads in fastqsanger.gz\\n* Restriction enzyme sequence\\n* Estimated genome size (enter as integer)\\n* Lineage for busco \\n\\nOutputs: the main outputs are: \\n\\n* scaffolded_assmbly.fasta\\n* comparison of pre- post- scaffolding contact maps\\n\\n\\n\\n\\n'\n",
      " 'An nf-core demo pipeline'\n",
      " '## Generate Nx and Size plot for multiple assemblies\\n\\n\\n### Inputs\\n\\nCollection of fasta files. The name of each item in the collection will be used as label for the Nx and Size plots.\\n\\n### Outputs\\n\\n\\n1. Nx plot \\n2. Size plot '\n",
      " '[![GitHub Actions CI Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/ci.yml)\\n[![GitHub Actions Linting Status](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml/badge.svg)](https://github.com/plant-food-research-open/assemblyqc/actions/workflows/linting.yml)[![Cite with Zenodo](http://img.shields.io/badge/DOI-10.5281/zenodo.10647870-1073c8?labelColor=000000)](https://doi.org/10.5281/zenodo.10647870)\\n[![nf-test](https://img.shields.io/badge/unit_tests-nf--test-337ab7.svg)](https://www.nf-test.com)\\n\\n[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with conda ❌](http://img.shields.io/badge/run%20with-conda%20❌-3EB049?labelColor=000000&logo=anaconda)](https://docs.conda.io/en/latest/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n[![Launch on Seqera Platform](https://img.shields.io/badge/Launch%20%F0%9F%9A%80-Seqera%20Platform-%234256e7)](https://cloud.seqera.io/launch?pipeline=https://github.com/plant-food-research-open/assemblyqc)\\n\\n## Introduction\\n\\n**plant-food-research-open/assemblyqc** is a [NextFlow](https://www.nextflow.io/docs/latest/index.html) pipeline which evaluates assembly quality with multiple QC tools and presents the results in a unified html report. The tools are shown in the [Pipeline Flowchart](#pipeline-flowchart) and their references are listed in [CITATIONS.md](./CITATIONS.md).\\n\\n## Pipeline Flowchart\\n\\n```mermaid\\n%%{init: {\\n    \\'theme\\': \\'base\\',\\n    \\'themeVariables\\': {\\n    \\'fontSize\\': \\'52px\",\\n    \\'primaryColor\\': \\'#9A6421\\',\\n    \\'primaryTextColor\\': \\'#ffffff\\',\\n    \\'primaryBorderColor\\': \\'#9A6421\\',\\n    \\'lineColor\\': \\'#B180A8\\',\\n    \\'secondaryColor\\': \\'#455C58\\',\\n    \\'tertiaryColor\\': \\'#ffffff\\'\\n  }\\n}}%%\\nflowchart LR\\n  forEachTag(Assembly) ==> VALIDATE_FORMAT[VALIDATE FORMAT]\\n\\n  VALIDATE_FORMAT ==> ncbiFCS[NCBI FCS\\\\nADAPTOR]\\n  ncbiFCS ==> Check{Check}\\n\\n  VALIDATE_FORMAT ==> ncbiGX[NCBI FCS GX]\\n  ncbiGX ==> Check\\n  Check ==> |Clean|Run(Run)\\n\\n  Check ==> |Contamination|Skip(Skip All)\\n  Skip ==> REPORT\\n\\n  VALIDATE_FORMAT ==> GFF_STATS[GENOMETOOLS GT STAT]\\n\\n  Run ==> ASS_STATS[ASSEMBLATHON STATS]\\n  Run ==> BUSCO\\n  Run ==> TIDK\\n  Run ==> LAI\\n  Run ==> KRAKEN2\\n  Run ==> HIC_CONTACT_MAP[HIC CONTACT MAP]\\n  Run ==> MUMMER\\n  Run ==> MINIMAP2\\n  Run ==> MERQURY\\n\\n  MUMMER ==> CIRCOS\\n  MUMMER ==> DOTPLOT\\n\\n  MINIMAP2 ==> PLOTSR\\n\\n  ASS_STATS ==> REPORT\\n  GFF_STATS ==> REPORT\\n  BUSCO ==> REPORT\\n  TIDK ==> REPORT\\n  LAI ==> REPORT\\n  KRAKEN2 ==> REPORT\\n  HIC_CONTACT_MAP ==> REPORT\\n  CIRCOS ==> REPORT\\n  DOTPLOT ==> REPORT\\n  PLOTSR ==> REPORT\\n  MERQURY ==> REPORT\\n```\\n\\n- [FASTA VALIDATOR](https://github.com/linsalrob/fasta_validator) + [SEQKIT RMDUP](https://github.com/shenwei356/seqkit): FASTA validation\\n- [GENOMETOOLS GT GFF3VALIDATOR](https://genometools.org/tools/gt_gff3validator.html): GFF3 validation\\n- [ASSEMBLATHON STATS](https://github.com/PlantandFoodResearch/assemblathon2-analysis/blob/a93cba25d847434f7eadc04e63b58c567c46a56d/assemblathon_stats.pl): Assembly statistics\\n- [GENOMETOOLS GT STAT](https://genometools.org/tools/gt_stat.html): Annotation statistics\\n- [NCBI FCS ADAPTOR](https://github.com/ncbi/fcs): Adaptor contamination pass/fail\\n- [NCBI FCS GX](https://github.com/ncbi/fcs): Foreign organism contamination pass/fail\\n- [BUSCO](https://gitlab.com/ezlab/busco): Gene-space completeness estimation\\n- [TIDK](https://github.com/tolkit/telomeric-identifier): Telomere repeat identification\\n- [LAI](https://github.com/oushujun/LTR_retriever/blob/master/LAI): Continuity of repetitive sequences\\n- [KRAKEN2](https://github.com/DerrickWood/kraken2): Taxonomy classification\\n- [HIC CONTACT MAP](https://github.com/igvteam/juicebox.js): Alignment and visualisation of HiC data\\n- [MUMMER](https://github.com/mummer4/mummer) → [CIRCOS](http://circos.ca/documentation/) + [DOTPLOT](https://plotly.com) & [MINIMAP2](https://github.com/lh3/minimap2) → [PLOTSR](https://github.com/schneebergerlab/plotsr): Synteny analysis\\n- [MERQURY](https://github.com/marbl/merqury): K-mer completeness, consensus quality and phasing assessment\\n\\n## Usage\\n\\nRefer to [usage](./docs/usage.md), [parameters](./docs/parameters.md) and [output](./docs/output.md) documents for details.\\n\\n> [!NOTE]\\n> If you are new to Nextflow and nf-core, please refer to [this page](https://nf-co.re/docs/usage/installation) on how to set-up Nextflow. Make sure to [test your setup](https://nf-co.re/docs/usage/introduction#how-to-run-a-pipeline) with `-profile test` before running the workflow on actual data.\\n\\nPrepare an `assemblysheet.csv` file with following columns representing target assemblies and associated meta-data.\\n\\n- `tag:` A unique tag which represents the target assembly throughout the pipeline and in the final report\\n- `fasta:` FASTA file\\n\\nNow, you can run the pipeline using:\\n\\n```bash\\nnextflow run plant-food-research-open/assemblyqc \\\\\\n   -profile <docker/singularity/.../institute> \\\\\\n   --input assemblysheet.csv \\\\\\n   --outdir <OUTDIR>\\n```\\n\\n> [!WARNING]\\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\\n\\n### Plant&Food Users\\n\\nDownload the pipeline to your `/workspace/$USER` folder. Change the parameters defined in the [pfr/params.json](./pfr/params.json) file. Submit the pipeline to SLURM for execution.\\n\\n```bash\\nsbatch ./pfr_assemblyqc\\n```\\n\\n## Credits\\n\\nplant-food-research-open/assemblyqc was originally written by Usman Rashid ([@gallvp](https://github.com/gallvp)) and Ken Smith ([@hzlnutspread](https://github.com/hzlnutspread)).\\n\\nRoss Crowhurst ([@rosscrowhurst](https://github.com/rosscrowhurst)), Chen Wu ([@christinawu2008](https://github.com/christinawu2008)) and Marcus Davy ([@mdavy86](https://github.com/mdavy86)) generously contributed their QC scripts.\\n\\nMahesh Binzer-Panchal ([@mahesh-panchal](https://github.com/mahesh-panchal)) helped port the pipeline modules and sub-workflows to [nf-core](https://nf-co.re) schema.\\n\\nWe thank the following people for their extensive assistance in the development of this pipeline:\\n\\n- [Cecilia Deng](https://github.com/CeciliaDeng)\\n- [Ignacio Carvajal](https://github.com/ignacio3437)\\n- [Jason Shiller](https://github.com/jasonshiller)\\n- [Sarah Bailey](https://github.com/SarahBailey1998)\\n- [Susan Thomson](https://github.com/cflsjt)\\n- [Ting-Hsuan Chen](https://github.com/ting-hsuan-chen)\\n\\nThe pipeline uses nf-core modules contributed by following authors:\\n\\n<a href=\"https://github.com/gallvp\"><img src=\"https://github.com/gallvp.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/drpatelh\"><img src=\"https://github.com/drpatelh.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/midnighter\"><img src=\"https://github.com/midnighter.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/mahesh-panchal\"><img src=\"https://github.com/mahesh-panchal.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/jfy133\"><img src=\"https://github.com/jfy133.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/adamrtalbot\"><img src=\"https://github.com/adamrtalbot.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/maxulysse\"><img src=\"https://github.com/maxulysse.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/matthdsm\"><img src=\"https://github.com/matthdsm.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/joseespinosa\"><img src=\"https://github.com/joseespinosa.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/ewels\"><img src=\"https://github.com/ewels.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/sofstam\"><img src=\"https://github.com/sofstam.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/sateeshperi\"><img src=\"https://github.com/sateeshperi.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/priyanka-surana\"><img src=\"https://github.com/priyanka-surana.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/phue\"><img src=\"https://github.com/phue.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/muffato\"><img src=\"https://github.com/muffato.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/lescai\"><img src=\"https://github.com/lescai.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/kevinmenden\"><img src=\"https://github.com/kevinmenden.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/jvhagey\"><img src=\"https://github.com/jvhagey.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/joon-klaps\"><img src=\"https://github.com/joon-klaps.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/jeremy1805\"><img src=\"https://github.com/jeremy1805.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/heuermh\"><img src=\"https://github.com/heuermh.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/grst\"><img src=\"https://github.com/grst.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/friederikehanssen\"><img src=\"https://github.com/friederikehanssen.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/felixkrueger\"><img src=\"https://github.com/felixkrueger.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/erikrikarddaniel\"><img src=\"https://github.com/erikrikarddaniel.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/edmundmiller\"><img src=\"https://github.com/edmundmiller.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/d4straub\"><img src=\"https://github.com/d4straub.png\" width=\"50\" height=\"50\"></a>\\n<a href=\"https://github.com/charles-plessy\"><img src=\"https://github.com/charles-plessy.png\" width=\"50\" height=\"50\"></a>\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\n## Citations\\n\\nIf you use plant-food-research-open/assemblyqc for your analysis, please cite it as:\\n\\n> Rashid, U., Wu, C., Shiller, J., Smith, K., Crowhurst, R., Davy, M., Chen, T.-H., Thomson, S., & Deng, C. (2024). AssemblyQC: A NextFlow pipeline for evaluating assembly quality (2.0.0). Zenodo. https://doi.org/10.5281/zenodo.10647870\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code and infrastructure developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n'\n",
      " 'Microbiome - Taxonomy Profiling'\n",
      " 'Pathogens of all samples report generation and visualization'\n",
      " 'Microbiome - QC and Contamination Filtering'\n",
      " 'Nanopore datasets analysis - Phylogenetic Identification - antibiotic resistance genes detection and contigs building'\n",
      " 'Microbiome - Variant calling and Consensus Building'\n",
      " '[![Nextflow](https://img.shields.io/badge/nextflow%20DSL2-%E2%89%A523.04.0-23aa62.svg)](https://www.nextflow.io/)\\n[![run with docker](https://img.shields.io/badge/run%20with-docker-0db7ed?labelColor=000000&logo=docker)](https://www.docker.com/)\\n[![run with singularity](https://img.shields.io/badge/run%20with-singularity-1d355c.svg?labelColor=000000)](https://sylabs.io/docs/)\\n\\n# mettannotator\\n\\n<img align=\"right\" width=\"162\" height=\"149\" src=\"media/mettannotator-logo.png\">\\n\\n- [ Introduction ](#intro)\\n- [ Workflow and tools](#wf)\\n- [ Installation and dependencies ](#install)\\n  - [Reference databases](#reference-databases)\\n- [ Usage ](#usage)\\n- [ Test ](#test)\\n- [ Outputs ](#out)\\n- [ Mobilome annotation ](#mobilome)\\n- [ Credits ](#credit)\\n- [ Contributions and Support ](#contribute)\\n- [ Citation ](#cite)\\n\\n<a name=\"intro\"></a>\\n\\n## Introduction\\n\\n**mettannotator** is a bioinformatics pipeline that generates an exhaustive annotation of prokaryotic genomes using existing tools. The output is a GFF file that integrates the results of all pipeline components. Results of each individual tool are also provided.\\n\\n<a name=\"wf\"></a>\\n\\n## Workflow and tools\\n\\n<img src=\"media/mettannotator-schema.png\">\\n<br />\\n<br />\\n\\nThe workflow uses the following tools and databases:\\n\\n| Tool/Database                                                                                    | Version                                       | Purpose                                                                                                                |\\n| ------------------------------------------------------------------------------------------------ | --------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------- |\\n| [Prokka](https://github.com/tseemann/prokka)                                                     | 1.14.6                                        | CDS calling and functional annotation (default)                                                                        |\\n| [Bakta](https://github.com/oschwengers/bakta)                                                    | 1.9.3                                         | CDS calling and functional annotation (if --bakta flag is used)                                                        |\\n| [Bakta db](https://zenodo.org/record/10522951/)                                                  | 2024-01-19 with AMRFinderPlus DB 2024-01-31.1 | Bakta DB (when Bakta is used as the gene caller)                                                                       |\\n| [InterProScan](https://www.ebi.ac.uk/interpro/about/interproscan/)                               | 5.62-94.0                                     | Protein annotation (InterPro, Pfam)                                                                                    |\\n| [eggNOG-mapper](https://github.com/eggnogdb/eggnog-mapper)                                       | 2.1.11                                        | Protein annotation (eggNOG, KEGG, COG, GO-terms)                                                                       |\\n| [eggNOG DB](http://eggnog6.embl.de/download/)                                                    | 5.0.2                                         | Database for eggNOG-mapper                                                                                             |\\n| [UniFIRE](https://gitlab.ebi.ac.uk/uniprot-public/unifire)                                       | 2023.4                                        | Protein annotation                                                                                                     |\\n| [AMRFinderPlus](https://github.com/ncbi/amr)                                                     | 3.12.8                                        | Antimicrobial resistance gene annotation; virulence factors, biocide, heat, acid, and metal resistance gene annotation |\\n| [AMRFinderPlus DB](https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/)              | 3.12 2024-01-31.1                             | Database for AMRFinderPlus                                                                                             |\\n| [DefenseFinder](https://github.com/mdmparis/defense-finder)                                      | 1.2.0                                         | Annotation of anti-phage systems                                                                                       |\\n| [DefenseFinder models](https://github.com/mdmparis/defense-finder-models)                        | 1.2.3                                         | Database for DefenseFinder                                                                                             |\\n| [GECCO](https://github.com/zellerlab/GECCO)                                                      | 0.9.8                                         | Biosynthetic gene cluster annotation                                                                                   |\\n| [antiSMASH](https://antismash.secondarymetabolites.org/#!/download)                              | 7.1.0                                         | Biosynthetic gene cluster annotation                                                                                   |\\n| [SanntiS](https://github.com/Finn-Lab/SanntiS)                                                   | 0.9.3.4                                       | Biosynthetic gene cluster annotation                                                                                   |\\n| [run_dbCAN](https://github.com/linnabrown/run_dbcan)                                             | 4.1.2                                         | PUL prediction                                                                                                         |\\n| [dbCAN DB](https://bcb.unl.edu/dbCAN2/download/Databases/)                                       | V12                                           | Database for run_dbCAN                                                                                                 |\\n| [CRISPRCasFinder](https://github.com/dcouvin/CRISPRCasFinder)                                    | 4.3.2                                         | Annotation of CRISPR arrays                                                                                            |\\n| [cmscan](http://eddylab.org/infernal/)                                                           | 1.1.5                                         | ncRNA predictions                                                                                                      |\\n| [Rfam](https://rfam.org/)                                                                        | 14.9                                          | Identification of SSU/LSU rRNA and other ncRNAs                                                                        |\\n| [tRNAscan-SE](https://github.com/UCSC-LoweLab/tRNAscan-SE)                                       | 2.0.9                                         | tRNA predictions                                                                                                       |\\n| [pyCirclize](https://github.com/moshi4/pyCirclize)                                               | 1.4.0                                         | Visualise the merged GFF file                                                                                          |\\n| [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline)                                 | 2.0.0                                         | Viral sequence annotation (runs separately)                                                                            |\\n| [Mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) | 2.0                                           | Mobilome annotation (runs separately)                                                                                  |\\n\\n<a name=\"install\"></a>\\n\\n## Installation and dependencies\\n\\nThis workflow is built using [Nextflow](https://www.nextflow.io/). It uses containers (Docker or Singularity) making installation simple and results highly reproducible.\\n\\n- Install [Nextflow version >=21.10](https://www.nextflow.io/docs/latest/getstarted.html#installation)\\n- Install [Singularity](https://github.com/apptainer/singularity/blob/master/INSTALL.md)\\n- Install [Docker](https://docs.docker.com/engine/install/)\\n\\nAlthough it\\'s possible to run the pipeline on a personal computer, due to the compute requirements, we encourage users to run it on HPC clusters. Any HPC scheduler supported by [Nextflow](https://www.nextflow.io/) is compatible; however, our team primarily uses [Slurm](https://slurm.schedmd.com/) and [IBM LSF](https://www.ibm.com/docs/en/spectrum-lsf) for the EBI HPC cluster, so those are the profiles we ship with the pipeline.\\n\\n<a name=\"reference-databases\"></a>\\n\\n### Reference databases\\n\\nThe pipeline needs reference databases in order to work, they take roughly 180G.\\n\\n| Path                | Size |\\n| ------------------- | ---- |\\n| amrfinder           | 217M |\\n| antismash           | 9.4G |\\n| bakta               | 71G  |\\n| dbcan               | 7.5G |\\n| defense_finder      | 242M |\\n| eggnog              | 48G  |\\n| interproscan        | 45G  |\\n| interpro_entry_list | 2.6M |\\n| rfam_models         | 637M |\\n| total               | 180G |\\n\\n`mettannotator` has an automated mechanism to download the databases using the `--dbs <db_path>` flag. When this flag is provided, the pipeline inspects the folder to verify if the required databases are already present. If any of the databases are missing, the pipeline will automatically download them.\\n\\nUsers can also provide individual paths to each reference database and its version if needed. For detailed instructions, please refer to the Reference databases section in the `--help` of the pipeline.\\n\\nIt\\'s important to note that users are not allowed to mix the `--dbs` flag with individual database paths and versions; they are mutually exclusive. We recommend users to run the pipeline with the `--dbs` flag for the first time in an appropriate path and to avoid downloading the individual databases separately.\\n\\n<a name=\"usage\"></a>\\n\\n## Usage\\n\\n### Input file\\n\\nFirst, prepare an input file in the CSV format that looks as follows:\\n\\n`assemblies_sheet.csv`:\\n\\n```csv\\nprefix,assembly,taxid\\nBU_ATCC8492VPI0062,/path/to/BU_ATCC8492VPI0062_NT5002.fa,820\\nEC_ASM584v2,/path/to/GCF_000005845.2.fna,562\\n...\\n```\\n\\nHere,\\n`prefix` is the prefix and the locus tag that will be assigned to output files and proteins during the annotation process;\\nmaximum length is 24 characters;\\n\\n`assembly` is the path to where the assembly file in FASTA format is located;\\n\\n`taxid` is the NCBI TaxId (if the species-level TaxId is not known, a TaxId for a higher taxonomic level can be used). If the taxonomy is known, look up the TaxID [here](https://www.ncbi.nlm.nih.gov/Taxonomy/Browser/wwwtax.cgi).\\n\\n#### Finding TaxIds\\n\\nIf NCBI taxonomies of input genomes are not known, a tool such as [CAT/BAT](https://github.com/MGXlab/CAT_pack) can be used.\\nFollow the [instructions](https://github.com/MGXlab/CAT_pack?tab=readme-ov-file#installation) for getting the tool and downloading the NCBI nr database for it.\\n\\nIf using CAT/BAT, here is the suggested process for making the `mettannotator` input file:\\n\\n```bash\\n# Run BAT on each input genome, saving all results to the same folder\\nCAT bins -b ${genome_name}.fna -d ${path_to_CAT_database} -t ${path_to_CAT_tax_folder} -o BAT_results/${genome_name}\\n\\n# Optional: to check what taxa were assigned, you can add names to them\\nCAT add_names -i BAT_results/${genome_name}.bin2classification.txt -o BAT_results/${genome_name}.name.txt -t ${path_to_CAT_tax_folder}\\n```\\n\\nTo generate an input file for `mettannotator`, use [generate_input_file.py](preprocessing/generate_input_file.py):\\n\\n```\\npython3 preprocessing/generate_input_file.py -h\\nusage: generate_input_file.py [-h] -i INFILE -d INPUT_DIR -b BAT_DIR -o OUTFILE [--no-prefix]\\n\\nThe script takes a list of genomes and the taxonomy results generated by BAT and makes a\\nmettannotator input csv file. The user has the option to either use the genome file name\\n(minus the extension) as the prefix for mettannotator or leave the prefix off and fill it\\nout themselves after the script generates an input file with just the FASTA location and\\nthe taxid. It is expected that for all genomes, BAT results are stored in the same folder\\nand are named as {fasta_base_name}.bin2classification.txt. The script will use the lowest-\\nlevel taxid without an asterisk as the taxid for the genome.\\n\\noptional arguments:\\n  -h, --help    show this help message and exit\\n  -i INFILE     A file containing a list of genome files to include (file name only, with file\\n                extension, unzipped, one file per line).\\n  -d INPUT_DIR  Full path to the directory where the input FASTA files are located.\\n  -b BAT_DIR    Folder with BAT results. Results for all genomes should be in the same folder\\n                and should be named {fasta_base_name}.bin2classification.txt\\n  -o OUTFILE    Path to the file where the output will be saved to.\\n  --no-prefix   Skip prefix generation and leave the first column of the output file empty for\\n                the user to fill out. Default: False\\n```\\n\\nFor example:\\n\\n```bash\\npython3 generate_input_file.py -i list_of_genome_fasta_files.txt -d /path/to/the/fasta/files/folder/ -b BAT_results/ -o mettannotator_input.csv\\n```\\n\\nIt is always best to check the outputs to ensure the results are as expected. Correct any wrongly detected taxa before starting `mettannotator`.\\n\\nNote, that by default the script uses FASTA file names as prefixes and truncates them to 24 characters if they exceed the limit.\\n\\n### Running mettannotator\\n\\nRunning `mettannotator` with the `--help` option will pull the repository and display the help message:\\n\\n```angular2html\\nnextflow run ebi-metagenomics/mettannotator/main.nf --help\\nN E X T F L O W  ~  version 23.04.3\\nLaunching `mettannotator/main.nf` [disturbed_davinci] DSL2 - revision: f2a0e51af6\\n\\n\\n------------------------------------------------------\\n  ebi-metagenomics/mettannotator <version>\\n------------------------------------------------------\\nTypical pipeline command:\\n\\n  nextflow run ebi-metagenomics/mettannotator --input assemblies_sheet.csv -profile docker\\n\\nInput/output options\\n  --input                            [string]  Path to comma-separated file containing information about the assemblies with the prefix to be used.\\n  --outdir                           [string]  The output directory where the results will be saved. You have to use absolute paths to storage on Cloud\\n                                               infrastructure.\\n  --fast                             [boolean] Run the pipeline in fast mode. In this mode, InterProScan, UniFIRE, and SanntiS won\\'t be executed, saving\\n                                               resources and speeding up the pipeline.\\n  --email                            [string]  Email address for completion summary.\\n  --multiqc_title                    [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\\n\\nReference databases\\n  --dbs                              [string]  Folder for the tools\\' reference databases used by the pipeline for downloading.\\n  --interproscan_db                  [string]  The InterProScan reference database, ftp://ftp.ebi.ac.uk/pub/software/unix/iprscan/\\n  --interproscan_db_version          [string]  The InterProScan reference database version. [default: 5.62-94.0]\\n  --interpro_entry_list              [string]  TSV file listing basic InterPro entry information - the accessions, types and names,\\n                                               ftp://ftp.ebi.ac.uk/pub/databases/interpro/releases/94.0/entry.list\\n  --interpro_entry_list_version      [string]  InterPro entry list version [default: 94]\\n  --eggnog_db                        [string]  The EggNOG reference database folder,\\n                                               https://github.com/eggnogdb/eggnog-mapper/wiki/eggNOG-mapper-v2.1.5-to-v2.1.12#requirements\\n  --eggnog_db_version                [string]  The EggNOG reference database version. [default: 5.0.2]\\n  --rfam_ncrna_models                [string]  Rfam ncRNA models, ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/genomes-pipeline/ncrna/\\n  --rfam_ncrna_models_rfam_version   [string]  Rfam release version where the models come from. [default: 14.9]\\n  --amrfinder_plus_db                [string]  AMRFinderPlus reference database,\\n                                               https://ftp.ncbi.nlm.nih.gov/pathogen/Antimicrobial_resistance/AMRFinderPlus/database/. Go to the following\\n                                               documentation for the db setup https://github.com/ncbi/amr/wiki/Upgrading#database-updates.\\n  --amrfinder_plus_db_version        [string]  The AMRFinderPlus reference database version. [default: 2023-02-23.1]\\n  --defense_finder_db                [string]  Defense Finder reference models, https://github.com/mdmparis/defense-finder#updating-defensefinder. The\\n                                               Microbiome Informatics team provides a pre-indexed version of the models for version 1.2.3 on this ftp location:\\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/defense-finder/defense-finder-models_1.2.3.tar.gz.\\n  --defense_finder_db_version        [string]  The Defense Finder models version. [default: 1.2.3]\\n  --antismash_db                     [string]  antiSMASH reference database, go to this documentation to do the database setup\\n                                               https://docs.antismash.secondarymetabolites.org/install/#installing-the-latest-antismash-release.\\n  --antismash_db_version             [string]  The antiSMASH reference database version. [default: 7.1.0]\\n  --dbcan_db                         [string]  dbCAN indexed reference database, please go to the documentation for the setup\\n                                               https://dbcan.readthedocs.io/en/latest/. The Microbiome Informatics team provides a pre-indexed version of the\\n                                               database for version 4.0 on this ftp location:\\n                                               ftp://ftp.ebi.ac.uk/pub/databases/metagenomics/pipelines/tool-dbs/dbcan/dbcan_4.0.tar.gz\\n  --dbcan_db_version                 [string]  The dbCAN reference database version. [default: 4.1.3_V12]\\n\\nGeneric options\\n  --multiqc_methods_description      [string]  Custom MultiQC yaml file containing HTML including a methods description.\\n\\nOther parameters\\n  --bakta                            [boolean] Use Bakta instead of Prokka for CDS annotation. Prokka will still be used for archaeal genomes.\\n\\n !! Hiding 17 params, use --validationShowHiddenParams to show them !!\\n------------------------------------------------------\\nIf you use ebi-metagenomics/mettannotator for your analysis please cite:\\n\\n* The nf-core framework\\n  https://doi.org/10.1038/s41587-020-0439-x\\n\\n* Software dependencies\\n  https://github.com/ebi-metagenomics/mettannotator/blob/master/CITATIONS.md\\n------------------------------------------------------\\n\\n```\\n\\nNow, you can run the pipeline using:\\n\\n```bash\\nnextflow run ebi-metagenomics/mettannotator \\\\\\n   -profile <docker/singularity/...> \\\\\\n   --input assemblies_sheet.csv \\\\\\n   --outdir <OUTDIR> \\\\\\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\\n```\\n\\n> **Warning:**\\n> Please provide pipeline parameters via the CLI or Nextflow `-params-file` option. Custom config files including those\\n> provided by the `-c` Nextflow option can be used to provide any configuration _**except for parameters**_;\\n> see [docs](https://nf-co.re/usage/configuration#custom-configuration-files).\\n\\n### Gene caller choice\\n\\nBy default, `mettannotator` uses Prokka to identify protein-coding genes. Users can choose to use Bakta instead by\\nrunning `mettannotator` with the `--bakta` flag. `mettannotator` runs Bakta without ncRNA and CRISPR\\nannotation as these are produced by separate tools in the pipeline. Archaeal genomes will continue to be annotated using\\nProkka as Bakta is only intended for annotation of bacterial genomes.\\n\\n### Fast mode\\n\\nTo reduce the compute time and the amount of resources used, the pipeline can be executed with the `--fast` flag. When\\nrun in the fast mode, `mettannotator` will skip InterProScan, UniFIRE and SanntiS. This could be a suitable option\\nfor a first-pass of annotation or if computational resources are limited, however, we recommend running the full version\\nof the pipeline whenever possible.\\n\\nWhen generating an input file for a fast mode run, it is sufficient to indicate the taxid of the superkingdom (`2` for\\nbacteria and `2157` for Archaea) in the \"taxid\" column rather than the taxid of the lowest known taxon.\\n\\n<a name=\"test\"></a>\\n\\n## Test\\n\\nTo run the pipeline using a test dataset, execute the following command:\\n\\n```bash\\nwget https://raw.githubusercontent.com/EBI-Metagenomics/mettannotator/master/tests/test.csv\\n\\nnextflow run ebi-metagenomics/mettannotator \\\\\\n   -profile <docker/singularity/...> \\\\\\n   --input test.csv \\\\\\n   --outdir <OUTDIR> \\\\\\n   --dbs <PATH/TO/WHERE/DBS/WILL/BE/SAVED>\\n```\\n\\n<a name=\"out\"></a>\\n\\n## Outputs\\n\\nThe output folder structure will look as follows:\\n\\n```\\n└─<PREFIX>\\n   ├─antimicrobial_resistance\\n   │  └─amrfinder_plus\\n   ├─antiphage_defense\\n   │  └─defense_finder\\n   ├─biosynthetic_gene_clusters\\n   │  ├─antismash\\n   │  ├─gecco\\n   │  └─sanntis\\n   ├─functional_annotation\\n   │  ├─dbcan\\n   │  ├─eggnog_mapper\\n   │  ├─interproscan\\n   │  ├─merged_gff\\n   │  ├─prokka\\n   │  └─unifire\\n   ├─mobilome\\n   │  └─crisprcas_finder\\n   ├─quast\\n   │  └─<PREFIX>\\n   │      ├─basic_stats\\n   │      └─icarus_viewers\\n   ├─rnas\\n   │  ├─ncrna\\n   │  └─trna\\n   ├─multiqc\\n   │  ├─multiqc_data\\n   │  └─multiqc_plots\\n   │      ├─pdf\\n   │      ├─png\\n   │      └─svg\\n   ├─pipeline_info\\n   │  ├─software_versions.yml\\n   │  ├─execution_report_<timestamp>.txt\\n   │  ├─execution_report_<timestamp>.html\\n   │  ├─execution_timeline_<timestamp>.txt\\n   │  ├─execution_timeline_<timestamp>.html\\n   │  ├─execution_trace_<timestamp>.txt\\n   │  ├─execution_trace_<timestamp>.html\\n   │  └─pipeline_dag_<timestamp>.html\\n\\n```\\n\\n### Merged GFF\\n\\nThe two main output files for each genome are located in `<OUTDIR>/<PREFIX>/functional_annotation/merged_gff/`:\\n\\n- `<PREFIX>_annotations.gff`: annotations produced by all tools merged into a single file\\n\\n- `<PREFIX>_annotations_with_descriptions.gff`: a version of the GFF file above that includes descriptions of all InterPro terms to make the annotations human-readable. Not generated if `--fast` flag was used.\\n\\nBoth files include the genome sequence in the FASTA format at the bottom of the file.\\n\\nAdditionally, for genomes with no more than 50 annotated contigs, a Circos plot of the `<PREFIX>_annotations.gff` file is generated and included in the same folder. An example of such plot is shown below:\\n\\n<img src=\"media/circos-plot-example.png\">\\n\\n#### Data sources\\n\\nBelow is an explanation of how each field in column 3 and 9 of the final GFF file is populated. In most cases, information is taken as is from the reporting tool\\'s output.\\n\\n| Feature (column 3)    | Attribute Name (column 9)                                               | Reporting Tool  | Description                                                                                                                                                                                                 |\\n| --------------------- | ----------------------------------------------------------------------- | --------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\\n| ncRNA                 | all\\\\*                                                                   | cmscan + Rfam   | ncRNA annotation (excluding tRNA)                                                                                                                                                                           |\\n| tRNA                  | all\\\\*                                                                   | tRNAscan-SE     | tRNA annotation                                                                                                                                                                                             |\\n| LeftFLANK, RightFLANK | all\\\\*                                                                   | CRISPRCasFinder | CRISPR array flanking sequence                                                                                                                                                                              |\\n| CRISPRdr              | all\\\\*                                                                   | CRISPRCasFinder | Direct repeat region of a CRISPR array                                                                                                                                                                      |\\n| CRISPRspacer          | all\\\\*                                                                   | CRISPRCasFinder | CRISPR spacer                                                                                                                                                                                               |\\n| CDS                   | `ID`, `eC_number`, `Name`, `Dbxref`, `gene`, `inference`, `locus_tag`   | Prokka/Bakta    | Protein annotation                                                                                                                                                                                          |\\n| CDS                   | `product`                                                               | mettannotator   | Product assigned as described in [ Determining the product ](#product)                                                                                                                                      |\\n| CDS                   | `product_source`                                                        | mettannotator   | Tool that reported the product chosen by mettannotator                                                                                                                                                      |\\n| CDS                   | `eggNOG`                                                                | eggNOG-mapper   | Seed ortholog from eggNOG                                                                                                                                                                                   |\\n| CDS                   | `cog`                                                                   | eggNOG-mapper   | COG category                                                                                                                                                                                                |\\n| CDS                   | `kegg`                                                                  | eggNOG-mapper   | KEGG orthology term                                                                                                                                                                                         |\\n| CDS                   | `Ontology_term`                                                         | eggNOG-mapper   | GO associations                                                                                                                                                                                             |\\n| CDS                   | `pfam`                                                                  | InterProScan    | Pfam accessions                                                                                                                                                                                             |\\n| CDS                   | `interpro`                                                              | InterProScan    | InterPro accessions. In `<PREFIX>_annotations_with_descriptions.gff` each accession is followed by its description and entry type: Domain [D], Family [F], Homologous Superfamily [H], Repeat [R], Site [S] |\\n| CDS                   | `nearest_MiBIG`                                                         | SanntiS         | MiBIG accession of the nearest BGC to the cluster in the MIBIG space                                                                                                                                        |\\n| CDS                   | `nearest_MiBIG_class`                                                   | SanntiS         | BGC class of nearest_MiBIG                                                                                                                                                                                  |\\n| CDS                   | `gecco_bgc_type`                                                        | GECCO           | BGC type                                                                                                                                                                                                    |\\n| CDS                   | `antismash_bgc_function`                                                | antiSMASH       | BGC function                                                                                                                                                                                                |\\n| CDS                   | `amrfinderplus_gene_symbol`                                             | AMRFinderPlus   | Gene symbol according to AMRFinderPlus                                                                                                                                                                      |\\n| CDS                   | `amrfinderplus_sequence_name`                                           | AMRFinderPlus   | Product description                                                                                                                                                                                         |\\n| CDS                   | `amrfinderplus_scope`                                                   | AMRFinderPlus   | AMRFinderPlus database (core or plus)                                                                                                                                                                       |\\n| CDS                   | `element_type`, `element_subtype`                                       | AMRFinderPlus   | Functional category                                                                                                                                                                                         |\\n| CDS                   | `drug_class`, `drug_subclass`                                           | AMRFinderPlus   | Class and subclass of drugs that this gene is known to contribute to resistance of                                                                                                                          |\\n| CDS                   | `dbcan_prot_type`                                                       | run_dbCAN       | Predicted protein function: transporter (TC), transcription factor (TF), signal transduction protein (STP), CAZyme                                                                                          |\\n| CDS                   | `dbcan_prot_family`                                                     | run_dbCAN       | Predicted protein family                                                                                                                                                                                    |\\n| CDS                   | `substrate_dbcan-pul`                                                   | run_dbCAN       | Substrate predicted by dbCAN-PUL search                                                                                                                                                                     |\\n| CDS                   | `substrate_dbcan-sub`                                                   | run_dbCAN       | Substrate predicted by dbCAN-subfam                                                                                                                                                                         |\\n| CDS                   | `defense_finder_type`, `defense_finder_subtype`                         | DefenseFinder   | Type and subtype of the anti-phage system found                                                                                                                                                             |\\n| CDS                   | `uf_prot_rec_fullname`, `uf_prot_rec_shortname`, `uf_prot_rec_ecnumber` | UniFIRE         | Protein recommended full name, short name and EC number according to UniFIRE                                                                                                                                |\\n| CDS                   | `uf_prot_alt_fullname`, `uf_prot_alt_shortname`, `uf_prot_alt_ecnumber` | UniFIRE         | Protein alternative full name, short name and EC number according to UniFIRE                                                                                                                                |\\n| CDS                   | `uf_chebi`                                                              | UniFIRE         | ChEBI identifiers                                                                                                                                                                                           |\\n| CDS                   | `uf_ontology_term`                                                      | UniFIRE         | GO associations                                                                                                                                                                                             |\\n| CDS                   | `uf_keyword`                                                            | UniFIRE         | UniFIRE keywords                                                                                                                                                                                            |\\n| CDS                   | `uf_gene_name`, `uf_gene_name_synonym`                                  | UniFIRE         | Gene name and gene name synonym according to UniFIRE                                                                                                                                                        |\\n| CDS                   | `uf_pirsr_cofactor`                                                     | UniFIRE         | Cofactor names from PIRSR                                                                                                                                                                                   |\\n\\n\\\\*all attributes in column 9 are populated by the tool\\n<br>\\n<br>\\n\\n<a name=\"product\"></a>\\n\\n#### Determining the product\\n\\nThe following logic is used by `mettannotator` to fill out the `product` field in the 9th column of the GFF:\\n\\n<img src=\"media/mettannotator-product.png\">\\n\\nIf the pipeline is executed with the `--fast` flag, only the output of eggNOG-mapper is used to determine the product of proteins that were labeled as hypothetical by the gene caller.\\n\\n### Contents of the tool output folders\\n\\nThe output folders of each individual tool contain select output files of the third-party tools used by `mettannotator`. For file descriptions, please refer to the tool documentation. For some tools that don\\'t output a GFF, `mettannotator` converts the output into a GFF.\\n\\nNote: if the pipeline completed without errors but some of the tool-specific output folders are empty, those particular tools did not generate any annotations to output.\\n\\n<a name=\"mobilome\"></a>\\n\\n## Mobilome annotation\\n\\nThe mobilome annotation workflow is not currently integrated into `mettannotator`. However, the outputs produced by `mettannotator` can be used to run [VIRify](https://github.com/EBI-Metagenomics/emg-viral-pipeline) and the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline) and the outputs of these tools can be integrated back into the GFF file produced by `mettannotator`.\\n\\nAfter installing both tools, follow these steps to add the mobilome annotation:\\n\\n1. Run the [viral annotation pipeline](https://github.com/EBI-Metagenomics/emg-viral-pipeline):\\n\\n```bash\\nnextflow run \\\\\\n    emg-viral-pipeline/virify.nf \\\\\\n    -profile <profile> \\\\\\n    --fasta <genome_fasta.fna> \\\\\\n    --output <prefix>\\n```\\n\\n2. Run the [mobilome annotation pipeline](https://github.com/EBI-Metagenomics/mobilome-annotation-pipeline):\\n\\n```bash\\nnextflow run mobilome-annotation-pipeline/main.nf \\\\\\n    --assembly <genome_fasta.fna> \\\\\\n    --user_genes true \\\\\\n    --prot_gff <mettannotator_results_folder/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\\\\n    --virify true # only if the next two VIRify files exist, otherwise skip this line \\\\\\n    --vir_gff Virify_output_folder/08-final/gff/<prefix>_virify.gff # only if file exists, otherwise skip this line \\\\\\n    --vir_checkv Virify_output_folder/07-checkv/\\\\*quality_summary.tsv # only if the GFF file above exists, otherwise skip this line \\\\\\n    --outdir <mobilome_output_folder> \\\\\\n    --skip_crispr true \\\\\\n    --skip_amr true \\\\\\n    -profile <profile>\"\\n```\\n\\n3. Integrate the output into the `mettannotator` GFF\\n\\n```bash\\n# Add mobilome to the merged GFF produced by mettannotator\\npython3 postprocessing/add_mobilome_to_gff.py \\\\\\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\\\\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations.gff \\\\\\n    -o <prefix>_annotations_with_mobilome.gff\\n\\n# Add mobilome to the GFF with descriptions produced by mettannotator\\npython3 postprocessing/add_mobilome_to_gff.py \\\\\\n    -m <mobilome_output_folder>/gff_output_files/mobilome_nogenes.gff \\\\\\n    -i <mettannotator_results_folder>/<prefix>/functional_annotation/merged_gff/<prefix>_annotations_with_descriptions.gff \\\\\\n    -o <prefix>_annotations_with_descriptions_with_mobilome.gff\\n```\\n\\n4. Optional: regenerate the Circos plot with the mobilome track added\\n\\n```bash\\npip install pycirclize\\npip install matplotlib\\n\\npython3 bin/circos_plot.py \\\\\\n    -i <prefix>_annotations_with_mobilome.gff \\\\\\n    -o plot.png \\\\\\n    -p <prefix> \\\\\\n    --mobilome\\n```\\n\\n<a name=\"credit\"></a>\\n\\n## Credits\\n\\nebi-metagenomics/mettannotator was originally written by the Microbiome Informatics Team at [EMBL-EBI](https://www.ebi.ac.uk/about/teams/microbiome-informatics/)\\n\\n<a name=\"contribute\"></a>\\n\\n## Contributions and Support\\n\\nIf you would like to contribute to this pipeline, please see the [contributing guidelines](.github/CONTRIBUTING.md).\\n\\n<a name=\"cite\"></a>\\n\\n## Citations\\n\\nAn extensive list of references for the tools used by the pipeline can be found in the [`CITATIONS.md`](CITATIONS.md) file.\\n\\nThis pipeline uses code developed and maintained by the [nf-core](https://nf-co.re) community, reused here under the [MIT license](https://github.com/nf-core/tools/blob/master/LICENSE).\\n\\n> **The nf-core framework for community-curated bioinformatics pipelines.**\\n>\\n> Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen.\\n>\\n> _Nat Biotechnol._ 2020 Feb 13. doi: [10.1038/s41587-020-0439-x](https://dx.doi.org/10.1038/s41587-020-0439-x).\\n'\n",
      " '**Name:** TruncatedSVD (Randomized SVD)  \\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum5 \\n\\nTruncatedSVD (Randomized SVD) for computing just 456 singular values out of a (4.5M x 850) size matrix.  \\nThe input matrix represents a CFD transient simulation of air moving past a cylinder.  \\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " '**Name:** KMeans\\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum5 \\n\\nKMEans for clustering the housing.csv dataset (https://github.com/sonarsushant/California-House-Price-Prediction/blob/master/housing.csv). \\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " 'SPA workflow using cryosparc processing engine'\n",
      " '# deepconsensus 1.2 snakemake pipeline\\nThis snakemake-based workflow takes in a subreads.bam and results in a deepconsensus.fastq\\n- no methylation calls !\\n\\nThe metadata id of the subreads file needs to be: \"m[numeric]_[numeric]_[numeric].subreads.bam\"\\n\\nChunking (how many subjobs) and ccs min quality filter can be adjusted in the config.yaml\\n\\nthe checkpoint model for deepconsensus1.2 should be accessible like this:\\ngsutil cp -r gs://brain-genomics-public/research/deepconsensus/models/v1.2/model_checkpoint/* \"${QS_DIR}\"/model/\\nif that does not work, try to download all at:\\nhttps://console.cloud.google.com/storage/browser/brain-genomics-public/research/deepconsensus/models?pageState=(%22StorageObjectListTable%22:(%22f%22:%22%255B%255D%22))&prefix=&forceOnObjectsSortingFiltering=false\\n\\nA run example is included in the run_snake.sh\\n\\nFeedback / pull requests welcome!\\n\\nDeveloped by Daniel Rickert @ WGGC Düsseldorf\\n\\nmore to look at:\\n\\nhttps://www.youtube.com/watch?v=TlWtIao2i9E\\n\\nhttps://www.nature.com/articles/s41587-022-01435-7\\n'\n",
      " 'Lysozyme in water full COMPSs application'\n",
      " 'Workflow for Single-cell ATAC-seq standard processing with SnapATAC2.\\nThis workflow takes a fragment file as input and performs the standard steps of scATAC-seq analysis: filtering, dimension reduction, embedding and visualization of marker genes with SnapATAC2. Finally, the clusters are manually annotated with the help of marker genes. \\nIn an alternative step, the fragment file can also be generated from a BAM file. \\n* newer Version: Updated SnapATAC2 version from 2.5.3 to 2.6.4'\n",
      " 'This Workflow takes a dataset collection of single-cell ATAC-seq fragments and performs:\\n- preprocessing\\n- filtering\\n- concatenation\\n- dimension reduction\\n- batch correction (with Harmony and optionally Scanorama and MNC-correct)\\n- leiden clustering\\n\\n* new SnapATAC2 version: from 2.5.3 to 2.6.4'\n",
      " 'CoCoMiCo analyses on collections of simulated communities.\\nCommunity: scatterplots and KW tests of cooperation and competition\\npotentials, for each collection.\\nSimilarity: cooperation and competition potentials vs model\\nsimilarity, defined as the Jaccard distance on sets of reactions.\\nAdded value: boxplots comparing the added value of models in\\ncommunities.'\n",
      " '**Name:** Matrix Multiplication  \\n**Contact Person:** support-compss@bsc.es  \\n**Access Level:** public  \\n**License Agreement:** Apache2  \\n**Platform:** COMPSs  \\n\\n# Description\\nMatrix multiplication is a binary operation that takes a pair of matrices and produces another matrix.\\n\\nIf A is an n×m matrix and B is an m×p matrix, the result AB of their multiplication is an n×p matrix defined only if the number of columns m in A is equal to the number of rows m in B. When multiplying A and B, the elements of the rows in A are multiplied with corresponding columns in B.\\n\\nIn this implementation, A and B are square matrices (same number of rows and columns), and so it is the result matrix C. Each matrix is divided in N blocks of M doubles. The multiplication of two blocks is done by a multiply task method with a simple three-nested-loop implementation. When executed with COMPSs, the main program generates N^3^ tasks arranged as N^2^ chains of N tasks in the dependency graph.\\n\\nN and M have been hardcoded to 6 and 8 respectively.\\n\\n# Execution instructions\\nUsage:\\n```\\nruncompss --classpath=application_sources/jar/matmul.jar matmul.files.Matmul inputFolder/ outputFolder/\\n``` \\n\\nwhere:\\n  * inputFolder: folder where input files are located\\n  * outputFolder: folder where output files are located\\n\\n# Build\\n## Option 1: Native java\\n```\\njavac src/main/java/matmul/*/*.java\\ncd src/main/java/; jar cf matmul.jar matmul/\\ncd ../../../; mv src/main/java/matmul.jar jar/\\n```\\n\\n## Option 2: Maven\\n```\\nmvn clean package\\n```\\n'\n",
      " 'This workflow takes a cell-type-annotated AnnData object (processed with SnapATAC2) and performs peak calling with MACS3 on the cell types. Next, a cell-by-peak matrix is constructed and differential accessibility tests are performed for comparison of either two cell types or one cell type with a background of all other cells. \\nLastly, differentially accessible marker regions for each cell type are identified. '\n",
      " 'This workflow \\n- Reconstruct phylogeny (insert fragments in a reference)\\n- Alpha rarefaction analysis\\n- Taxonomic analysis'\n",
      " 'cfDNA UniFlow is a unified, standardized, and ready-to-use workflow for processing whole genome sequencing (WGS) cfDNA samples from liquid biopsies. It includes essential steps for pre-processing raw cfDNA samples, quality control and reporting. Additionally, several optional utility functions like GC bias correction and estimation of copy number state are included. Finally, we provide specialized methods for extracting coverage derived signals and visualizations comparing cases and controls. \\n\\nMore Information can be found in the official [documentaion](https://github.com/kircherlab/cfDNA-UniFlow/blob/main/README.md).'\n",
      " '**Name:** GridSearchCV\\n**Contact Person**: support-compss@bsc.es  \\n**Access Level**: public  \\n**License Agreement**: Apache2  \\n**Platform**: COMPSs  \\n**Machine**: MareNostrum5 \\n\\nGridSearch of kNN algorithm for the iris.csv dataset (https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv). \\nThis application used [dislib-0.9.0](https://github.com/bsc-wdc/dislib/tree/release-0.9)\\n'\n",
      " 'The workflow requires the user to provide:\\n* ENSEMBL link address of the annotation GFF3 file\\n* ENSEMBL link address of the assembly FASTA file\\n* NCBI taxonomy ID\\n* BUSCO lineage\\n* OMArk database\\n\\nThw workflow will produce statistics of the annotation based on AGAT, BUSCO and OMArk.'\n",
      " 'A pipeline for de novo transcriptome assembly of short reads from bulk RNA-seq'\n",
      " '# RiboSnake: 16S rRNA analysis workflow with QIIME2 and Snakemake\\n\\n[![Snakemake](https://img.shields.io/badge/snakemake-≥6.10-brightgreen.svg)](https://snakemake.bitbucket.io)\\n[![Build Status](https://travis-ci.org/snakemake-workflows/16S.svg?branch=master)](https://travis-ci.org/snakemake-workflows/16S)\\n\\nQiime2 workflow for 16S analysis created with snakemake.\\n\\n## Authors\\n\\n* Ann-Kathrin Dörr (@AKBrueggemann)\\n\\n## Usage\\n\\nIf you use this workflow in a paper, don\\'t forget to give credits to the authors by citing the URL of this (original) repository and, if available, its DOI (see above).\\n\\n### Step 1: Obtain a copy of this workflow\\n\\nIf you want to use the workflow, please obtain a copy of it by either:\\n[Cloning](https://help.github.com/en/articles/cloning-a-repository) the repository to your local system, into the place where you want to perform the data analysis or\\nDownloading a zip-file of the repository to your local machine.\\n\\nWhen you have the folder structure added on your local machine, please add a \"data\" folder manually.\\n\\n### Step 2: Configure workflow\\n\\nConfigure the workflow according to your needs via editing the files in the `config/` folder. Adjust `config.yaml` to configure the workflow execution, and `metadata.txt` to specify your sample setup.\\n\\nSome important parameters you should check and set according to your own FASTQ-files in the `config.yaml` are primers for the forward and reverse reads, the `datatype`, that should be used by QIIME2 and the `min-seq-length`. Based on the sequencing, the length of the reads can vary.\\n\\nThe default parameters for filtering and truncation were validated with the help of a MOCK community and fitted to retrieve all bacteria from that community.\\n\\nIn addition to that, you need to fit the metadata-parameters to your data. Please change the names of the used metadata-columns according to your information.\\nTake special care of the \"remove-columns\" information. Here you can add the columns you don\\'t want to have analyzed or the workflow can\\'t anlyse. This can happen when\\nall of the values in one column are unique or all the same. You should also look out for the information under \"metadata-parameters\" and \"songbird\" as well as \"ancom\".\\nIn every case you have to specify the column names based on your own data.\\n\\nIf your metadata is not containing numeric values, please use the \"reduced-analysis\" option in the config file to run the workflow, as the workflow is currently not able to run only on categorical metadata for the full analysis version. We are going to fix that in the future.\\n\\nThe workflow is able to perform clustering and denoising either with vsearch, leading to OTU creation, or with DADA2, creating ASVs. You can decide which modus to use by setting the variable \"DADA2\" to `True` (DADA2 usage) or `False` (vsearch).\\n\\nPlease make sure, that the names of your FASTQ files are correctly formatted. They should look like this:\\n\\n    samplename_SNumber_Lane_R1/R2_001.fastq.gz\\n\\nIn the config file you can also set the input and output directory. You can either create a specific directory for your input data and then put that filepath in the config file, or you can put the path to an existing directory where the data is located.\\nThe data will then be copied to the workflow\\'s data directory. The compressed and final file holding the results will be copied to the directory you specified in \"output\". It will also stay in the local \"results\" folder together with important intermediate results.\\nThe \"data\" folder is also not provided by the repository. It is the folder the fastq files are copied to before being used in the workflow. It is best if you create the folder inside the workflows folder structure. It must definitely be created on the machine, the workflow is running on.\\n\\n### Step 3: Install Snakemake\\n\\nCreate a snakemake environment using [mamba](https://mamba.readthedocs.io/en/latest/) via:\\n\\n    mamba create -c conda-forge -c bioconda -n snakemake snakemake\\n\\nFor installation details, see the [instructions in the Snakemake documentation](https://snakemake.readthedocs.io/en/stable/getting_started/installation.html).\\n\\n### Step 4: Execute workflow\\n\\nActivate the conda environment:\\n\\n    conda activate snakemake\\n\\nFill up the `metadata.txt` with the information of your samples:\\n\\n    Please be careful to not include spaces between the commas. If there is a column, that you don\\'t have any information about, please leave it empty and simply go on with the next column.\\n\\nTest your configuration by performing a dry-run via\\n\\n    snakemake --use-conda -n\\n\\nExecuting the workflow takes two steps:\\n\\n    Data preparation: snakemake --cores $N --use-conda data_prep\\n    Workflow execution: snakemake --cores $N --use-conda\\n\\nusing `$N` cores.\\n\\nWhen running on snakemake > 8.0 we recommend setting the --shared-fs-usage none as well as setting the environment variable TEMP to a local directory to prevent problems with the usage of the fs-storage system.\\nThe environment variable can be set like this:\\n\\n    conda activate your_environment_name\\n    export TEMP=/path/to/local/tmp\\n\\nThen run the snakemake command like above with the addition of the storage flag:\\n\\n    snakemake --cores $N --use-conda --shared-fs-usage none\\n\\n### Step 5: Investigate results\\n\\nAfter successful execution, the workflow provides you with a compressed folder, holding all interesting results ready to decompress or to download to your local machine.\\nThe compressed file 16S-report.tar.gz holds several qiime2-artifacts that can be inspected via qiime-view. In the zipped folder report.zip is the snakemake html\\nreport holding graphics as well as the DAG of the executed jobs and html files leading you directly to the qiime2-results, without the need of using qiime-view.\\n\\nThis report can, e.g., be forwarded to your collaborators.\\n\\n### Step 6: Obtain updates from upstream\\n\\nWhenever you want to synchronize your workflow copy with new developments from upstream, do the following.\\n\\n1. Once, register the upstream repository in your local copy: `git remote add -f upstream git@github.com:snakemake-workflows/16S.git` or `git remote add -f upstream https://github.com/snakemake-workflows/16S.git` if you do not have setup ssh keys.\\n2. Update the upstream version: `git fetch upstream`.\\n3. Create a diff with the current version: `git diff HEAD upstream/master workflow > upstream-changes.diff`.\\n4. Investigate the changes: `vim upstream-changes.diff`.\\n5. Apply the modified diff via: `git apply upstream-changes.diff`.\\n6. Carefully check whether you need to update the config files: `git diff HEAD upstream/master config`. If so, do it manually, and only where necessary, since you would otherwise likely overwrite your settings and samples.\\n\\n## Contribute back\\n\\nIn case you have also changed or added steps, please consider contributing them back to the original repository:\\n\\n### Step 1: Forking the repository\\n\\n[Fork](https://help.github.com/en/articles/fork-a-repo) the original repo to a personal or lab account.\\n\\n### Step 2: Cloning\\n\\n[Clone](https://help.github.com/en/articles/cloning-a-repository) the fork to your local system, to a different place than where you ran your analysis.\\n\\n### Step 3: Add changes\\n\\n1. Copy the modified files from your analysis to the clone of your fork, e.g., `cp -r workflow path/to/fork`. Make sure to **not** accidentally copy config file contents or sample sheets. Instead, manually update the example config files if necessary.\\n2. Commit and push your changes to your fork.\\n3. Create a [pull request](https://help.github.com/en/articles/creating-a-pull-request) against the original repository.\\n4. If you want to add your config file and the parameters as a new default parameter sets, please do this by opening a pull request adding the file to the \"contributions\" folder.\\n\\n## Testing\\n\\nTest cases are in the subfolder `.test`. They are automatically executed via continuous integration with [Github Actions](https://github.com/features/actions).\\nIf you want to test the RiboSnake functions yourself, you can use the same data used for the CI/CD tests. The used fastq files can be downloaded [here](https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip). They have been published by Neilson et al., mSystems, 2017.\\n\\n### Example\\n\\n1. First clone teh repository to your local machine as described above.\\n2. Download a dataset of your liking, or the data used for testing the pipeline. The FASTQ files can be downloaded with:\\n    curl -sL \\\\\\n          \"https://data.qiime2.org/2022.2/tutorials/importing/casava-18-paired-end-demultiplexed.zip\"\\n3. Unzip the data into a folder of your liking, it can be called \"incoming\" but it does not have to be.\\nIf you name your folder differently, please change the \"input\" path in the config file.\\n4. If you don\\'t want to use the whole dataset for testing, remove some of the FASTQ files from the folder:\\n    rm PAP*\\n    rm YUN*\\n    rm Rep*\\n    rm blank*\\n5. Use the information that can be found in [this](https://data.qiime2.org/2024.5/tutorials/atacama-soils/sample_metadata.tsv) file from the Qiime2 tutorial, to fill out your metadata.txt file for the samples starting with \"BAQ\".\\n6. The default-parameters to be used in the config file can be found in the provided file \"PowerSoil-Illumina-soil.yaml\" in the config folder.\\n7. With these parameters and the previous steps, you should be able to execute the workflow.\\n\\n## Tools\\n\\nA list of the tools used in this pipeline:\\n\\n| Tool         | Link                                              |\\n|--------------|---------------------------------------------------|\\n| QIIME2       | www.doi.org/10.1038/s41587-019-0209-9             |\\n| Snakemake    | www.doi.org/10.12688/f1000research.29032.1        |\\n| FastQC       | www.bioinformatics.babraham.ac.uk/projects/fastqc |\\n| MultiQC      | www.doi.org/10.1093/bioinformatics/btw354         |\\n| pandas       | pandas.pydata.org                                 |\\n| kraken2      | www.doi.org/10.1186/s13059-019-1891-0             |\\n| vsearch      | www.github.com/torognes/vsearch                   |\\n| DADA2        | www.doi.org/10.1038/nmeth.3869                    |\\n| songbird     | www.doi.org/10.1038/s41467-019-10656-5            |\\n| bowtie2      | www.doi.org/10.1038/nmeth.1923                    |\\n| Ancom        | www.doi.org/10.3402/mehd.v26.27663                |\\n| cutadapt     | www.doi.org/10.14806/ej.17.1.200                  |\\n| BLAST        | www.doi.org/10.1016/S0022-2836(05)80360-2         |\\n| gneiss       | www.doi.org/10.1128/mSystems.00162-16             |\\n| qurro        | www.doi.org/10.1093/nargab/lqaa023                |\\n| Rescript     | www.doi.org/10.1371/journal.pcbi.1009581          |'\n",
      " '**Assembly Evaluation for ERGA-BGE Reports**\\n\\n_One Assmebly, Illumina WGS reads + HiC reads_\\n\\nThe workflow requires the following:\\n* Species Taxonomy ID number\\n* NCBI Genome assembly accession code\\n* BUSCO Lineage\\n* WGS accurate reads accession code\\n* NCBI HiC reads accession code\\n\\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\\n\\n**Use this workflow for ONT-based assemblies where the WGS accurate reads are Illumina PE**'\n",
      " '**Assembly Evaluation for ERGA-BGE Reports**\\n\\n_One Assmebly, HiFi WGS reads + HiC reads_\\n\\nThe workflow requires the following:\\n* Species Taxonomy ID number\\n* NCBI Genome assembly accession code\\n* BUSCO Lineage\\n* WGS accurate reads accession code\\n* NCBI HiC reads accession code\\n\\nThe workflow will get the data and process it to generate genome profiling (genomescope, smudgeplot -optional-), assembly stats (gfastats), merqury stats (QV, completeness), BUSCO, snailplot, contamination blobplot, and HiC heatmap.\\n\\n**Use this workflow for HiFi-based assemblies where the WGS accurate reads are PacBio HiFi**'\n",
      " 'Secondary metabolite biosynthetic gene cluster (SMBGC) Annotation using Neural Networks Trained on Interpro Signatures'\n",
      " '# Swedish Earth Biogenome Project - Genome Assembly Workflow\\n\\nThe primary genome assembly workflow for the Earth Biogenome Project at NBIS.\\n\\n## Workflow overview\\n\\nGeneral aim:\\n\\n```mermaid\\nflowchart LR\\n    hifi[/ HiFi reads /] --> data_inspection\\n    ont[/ ONT reads /] -->  data_inspection\\n    hic[/ Hi-C reads /] --> data_inspection\\n    data_inspection[[ Data inspection ]] --> preprocessing\\n    preprocessing[[ Preprocessing ]] --> assemble\\n    assemble[[ Assemble ]] --> validation\\n    validation[[ Assembly validation ]] --> curation\\n    curation[[ Assembly curation ]] --> validation\\n```\\n\\nCurrent implementation:\\n\\n```mermaid\\nflowchart TD\\n    input[/ Input file/] --> hifi\\n    input --> hic\\n    input --> taxonkit[[ TaxonKit name2taxid/reformat ]]\\n    taxonkit --> goat_taxon[[ GOAT taxon search ]]\\n    goat_taxon --> busco\\n    goat_taxon --> dtol[[ DToL lookup ]]\\n    hifi --> samtools_fa[[ Samtools fasta ]]\\n    samtools_fa --> fastk_hifi\\n    samtools_fa --> mash_screen\\n    hifi[/ HiFi reads /] --> fastk_hifi[[ FastK - HiFi ]]\\n    hifi --> meryl_hifi[[ Meryl - HiFi ]]\\n    hic[/ Hi-C reads /] --> fastk_hic[[ FastK - Hi-C ]]\\n    hifi --> meryl_hic[[ Meryl - Hi-C ]]\\n    assembly[/ Assembly /] --> quast[[ Quast ]]\\n    fastk_hifi --> histex[[ Histex ]]\\n    histex --> genescopefk[[ GeneScopeFK ]]\\n    fastk_hifi --> ploidyplot[[ PloidyPlot ]]\\n    fastk_hifi --> katgc[[ KatGC ]]\\n    fastk_hifi --> merquryfk[[ MerquryFK ]]\\n    assembly --> merquryfk\\n    meryl_hifi --> merqury[[ Merqury ]]\\n    assembly --> merqury\\n    fastk_hifi --> katcomp[[ KatComp ]]\\n    fastk_hic --> katcomp\\n    assembly --> busco[[ Busco ]]\\n    refseq_sketch[( RefSeq sketch )] --> mash_screen[[ Mash Screen ]]\\n    hifi --> mash_screen\\n    fastk_hifi --> hifiasm[[ HiFiasm ]]\\n    hifiasm --> assembly\\n    assembly --> purgedups[[ Purgedups ]]\\n    input --> mitoref[[ Mitohifi - Find reference ]]\\n    assembly --> mitohifi[[ Mitohifi ]]\\n    assembly --> fcsgx[[ FCS GX ]]\\n    fcs_fetchdb[( FCS fetchdb )] --> fcsgx\\n    mitoref --> mitohifi\\n    genescopefk --> quarto[[ Quarto ]]\\n    goat_taxon --> multiqc[[ MultiQC ]]\\n    quarto --> multiqc\\n    dtol --> multiqc\\n    katgc --> multiqc\\n    ploidyplot --> multiqc\\n    busco --> multiqc\\n    quast --> multiqc\\n```\\n\\n## Usage\\n\\n```bash\\nnextflow run -params-file <params.yml> \\\\\\n    [ -c <custom.config> ] \\\\\\n    [ -profile <profile> ] \\\\\\n    NBISweden/Earth-Biogenome-Project-pilot\\n```\\n\\nwhere:\\n- `params.yml` is a YAML formatted file containing workflow parameters\\n    such as input paths to the assembly specification, and settings for tools within the workflow.\\n\\n    Example:\\n\\n    ```yml\\n    input: \\'assembly_spec.yml\\'\\n    outdir: results\\n    fastk: # Optional\\n      kmer_size: 31 # default 31\\n    genescopefk: # Optional\\n      kmer_size: 31 # default 31\\n    hifiasm: # Optional, default = no extra options: Key (e.g. \\'opts01\\') is used in assembly build name (e.g., \\'hifiasm-raw-opts01\\').\\n      opts01: \"--opts A\"\\n      opts02: \"--opts B\"\\n    busco: # Optional, default: retrieved from GOAT\\n      lineages: \\'auto\\' # comma separated string of lineages or auto.\\n    ```\\n\\n    Alternatively parameters can be provided on the\\n    command-line using the `--parameter` notation (e.g., `--input <path>` ).\\n- `<custom.config>` is a Nextflow configuration file which provides\\n    additional configuration. This is used to customise settings other than\\n    workflow parameters, such as cpus, time, and command-line options to tools.\\n\\n    Example:\\n    ```nextflow\\n    process {\\n        withName: \\'BUSCO\\' {  // Selects the process to apply settings.\\n            cpus     = 6     // Overrides cpu settings defined in nextflow.config\\n            time     = 4.d   // Overrides time settings defined in nextflow.config to 4 days. Use .h for hours, .m for minutes.\\n            memory   = \\'20GB\\'  // Overrides memory settings defined in nextflow.config to 20 GB.\\n            // ext.args supplies command-line options to the process tool\\n            // overrides settings found in configs/modules.config\\n            ext.args = \\'--long\\'  // Supplies these as command-line options to Busco\\n        }\\n    }\\n    ```\\n- `<profile>` is one of the preconfigured execution profiles\\n    (`uppmax`, `singularity_local`, `docker_local`, etc: see nextflow.config). Alternatively,\\n    you can provide a custom configuration to configure this workflow\\n    to your execution environment. See [Nextflow Configuration](https://www.nextflow.io/docs/latest/config.html#scope-executor)\\n    for more details.\\n\\n\\n### Workflow parameter inputs\\n\\nMandatory:\\n\\n- `input`: A YAML formatted input file.\\n    Example `assembly_spec.yml` (See also [test profile input](assets/test_hsapiens.yml) TODO:: Update test profile):\\n\\n    ```yml\\n    sample:                          # Required: Meta data\\n      name: \\'Laetiporus sulphureus\\'  # Required: Species name. Correct spelling is important to look up species information.\\n      ploidy: 2                      # Optional: Estimated ploidy (default: retrieved from GOAT)\\n      genome_size: 2345              # Optional: Estimated genome size (default: retrieved from GOAT)\\n      haploid_number: 13             # Optional: Estimated haploid chromosome count (default: retrieved from GOAT)\\n      taxid: 5630                    # Optional: Taxon ID (default: retrieved with Taxonkit)\\n      kingdom: Eukaryota             #\\xa0Optional: (default: retrived with Taxonkit)\\n    assembly:                        # Optional: List of assemblies to curate and validate.\\n      - assembler: hifiasm           # For each entry, the assembler,\\n        stage: raw                   # stage of assembly,\\n        id: uuid                     # unique id,\\n        pri_fasta: /path/to/primary_asm.fasta # and paths to sequences are required.\\n        alt_fasta: /path/to/alternate_asm.fasta\\n        pri_gfa: /path/to/primary_asm.gfa\\n        alt_gfa: /path/to/alternate_asm.gfa\\n      - assembler: ipa\\n        stage: raw\\n        id: uuid\\n        pri_fasta: /path/to/primary_asm.fasta\\n        alt_fasta: /path/to/alternate_asm.fasta\\n    hic:                             # Optional: List of hi-c reads to QC and use for scaffolding\\n      - read1: \\'/path/to/raw/data/hic/LS_HIC_R001_1.fastq.gz\\'\\n        read2: \\'/path/to/raw/data/hic/LS_HIC_R001_2.fastq.gz\\'\\n    hifi:                            # Required: List of hifi-reads to QC and use for assembly/validation\\n      - reads: \\'/path/to/raw/data/hifi/LS_HIFI_R001.bam\\'\\n    rnaseq:                          # Optional: List of Rna-seq reads to use for validation\\n      - read1: \\'/path/to/raw/data/rnaseq/LS_RNASEQ_R001_1.fastq.gz\\'\\n        read2: \\'/path/to/raw/data/rnaseq/LS_RNASEQ_R001_2.fastq.gz\\'\\n    isoseq:                          # Optional: List of Isoseq reads to use for validation\\n      - reads: \\'/path/to/raw/data/isoseq/LS_ISOSEQ_R001.bam\\'\\n    ```\\n\\n\\nOptional:\\n\\n- `outdir`: The publishing path for results (default: `results`).\\n- `publish_mode`: (values: `\\'symlink\\'` (default), `\\'copy\\'`) The file\\npublishing method from the intermediate results folders\\n(see [Table of publish modes](https://www.nextflow.io/docs/latest/process.html#publishdir)).\\n- `steps`: The workflow steps to execute (default is all steps). Choose from:\\n\\n    - `inspect`: 01 - Read inspection\\n    - `preprocess`: 02 - Read preprocessing\\n    - `assemble`: 03 - Assembly\\n    - `purge`: 04 - Duplicate purging\\n    - `polish`: 05 - Error polishing\\n    - `screen`: 06 - Contamination screening\\n    - `scaffold`: 07 - Scaffolding\\n    - `curate`: 08 - Rapid curation\\n    - `alignRNA`: 09 - Align RNAseq data\\n\\nSoftware specific:\\n\\nTool specific settings are provided by supplying values to specific keys or supplying an array of\\nsettings under a tool name. The input to `-params-file` would look like this:\\n\\n```yml\\ninput: assembly.yml\\noutdir: results\\nfastk:\\n  kmer_size: 31\\ngenescopefk:\\n  kmer_size: 31\\nhifiasm:\\n  opts01: \"--opts A\"\\n  opts02: \"--opts B\"\\nbusco:\\n  lineages: \\'auto\\'\\n```\\n\\n- `multiqc_config`: Path to MultiQC configuration file (default: `configs/multiqc_conf.yaml`).\\n\\nUppmax and PDC cluster specific:\\n\\n- `project`: NAISS Compute allocation number.\\n\\n### Workflow outputs\\n\\nAll results are published to the path assigned to the workflow parameter `results`.\\n\\nTODO:: List folder contents in results file\\n### Customization for Uppmax\\n\\nA custom profile named `uppmax` is available to run this workflow specifically\\non UPPMAX clusters. The process `executor` is `slurm` so jobs are\\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\\nmust have a project allocation. This is automatically added to the `clusterOptions`\\nin the `uppmax` profile. All Uppmax clusters have node local disk space to do\\ncomputations, and prevent heavy input/output over the network (which\\nslows down the cluster for all).\\nThe path to this disk space is provided by the `$SNIC_TMP` variable, used by\\nthe `process.scratch` directive in the `uppmax` profile. Lastly\\nthe profile enables the use of Singularity so that all processes must be\\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\\nfor the profile specification.\\n\\nThe profile is enabled using the `-profile` parameter to nextflow:\\n```bash\\nnextflow run -profile uppmax <nextflow_script>\\n```\\n\\nA NAISS compute allocation should also be supplied using the `--project` parameter.\\n\\n### Customization for PDC\\n\\nA custom profile named `dardel` is available to run this workflow specifically\\non the PDC cluster *Dardel*. The process `executor` is `slurm` so jobs are\\nsubmitted to the Slurm Queue Manager. All jobs submitted to slurm\\nmust have a project allocation. This is automatically added to the `clusterOptions`\\nin the `dardel` profile. Calculations are performed in the scratch space allocated\\nby `PDC_TMP` which is also on the lustre file system and is not node local storage.\\nThe path to this disk space is provided by the `$PDC_TMP` variable, used by\\nthe `process.scratch` directive in the `dardel` profile. Lastly\\nthe profile enables the use of Singularity so that all processes must be\\nexecuted within Singularity containers. See [nextflow.config](nextflow.config)\\nfor the profile specification.\\n\\nThe profile is enabled using the `-profile` parameter to nextflow:\\n```bash\\nnextflow run -profile dardel <nextflow_script>\\n```\\n\\nA NAISS compute allocation should also be supplied using the `--project` parameter.\\n\\n## Workflow organization\\n\\nThe workflows in this folder manage the execution of your analyses\\nfrom beginning to end.\\n\\n```\\nworkflow/\\n | - .github/                        Github data such as actions to run\\n | - assets/                         Workflow assets such as test samplesheets\\n | - bin/                            Custom workflow scripts\\n | - configs/                        Configuration files that govern workflow execution\\n | - dockerfiles/                    Custom container definition files\\n | - docs/                           Workflow usage and interpretation information\\n | - modules/                        Process definitions for tools used in the workflow\\n | - subworkflows/                   Custom workflows for different stages of the main analysis\\n | - tests/                          Workflow tests\\n | - main.nf                         The primary analysis script\\n | - nextflow.config                 General Nextflow configuration\\n \\\\ - modules.json                    nf-core file which tracks modules/subworkflows from nf-core\\n```\\n\\n'\n",
      " 'GraphRBF is a state-of-the-art protein-protein/nucleic acid interaction site prediction model built by enhanced graph neural networks and prioritized radial basis function neural networks. \\nThis project serves users to use our software to directly predict protein binding sites or train our model on a new database.  \\nIdentification of protein-protein and protein-nucleic acid binding sites provides insights into biological processes related to protein functions and technical guidance for disease diagnosis and drug design. However, accurate predictions by computational approaches remain highly challenging due to the limited knowledge of residue binding patterns. The binding pattern of a residue should be characterized by the spatial distribution of its neighboring residues combined with their physicochemical information interaction, which yet can not be achieved by previous methods. Here, we design GraphRBF, a hierarchical geometric deep learning model to learn residue binding patterns from big data. To achieve it, GraphRBF describes physicochemical information interactions by designing an enhanced graph neural network and characterizes residue spatial distributions by introducing a prioritized radial basis function neural network. After training and testing, GraphRBF shows great improvements over existing state-of-the-art methods and strong interpretability of its learned representations. \\n'\n",
      " 'Pairwise alignment pipeline (genome to genome or reads to genome)'\n",
      " 'A R workflow for proteomics data analysis is reported. This pipeline was basing on protein expression projects, stored on the PRIDE database and reported on the COVID-19 Data portal. This is an R pipeline to analyze protein expression data, built on lung cell lines infected by SARS-CoV-2 variants: \\u200bB.1, Delta, and Omicron BA.1 (Mezler et al. 2023) https://www.ebi.ac.uk/pride/archive/projects/PXD037265. This pipeline can obtain DEPs for each variant, starting from normalized protein expression data, and it enables to obtain LogFC and FDR values highly overlapping with DEPs in Mezler et al. 2023, as well as building an input file to overlay the DEPs on COVID-19 Disease Map (C19DM) (https://covid19map.elixir-luxembourg.org/minerva/). ']\n"
     ]
    }
   ],
   "source": [
    "# most are null for other_creators, doi, and revisions, other null values are probably not that important\n",
    "condition = nan!= 0 # look for all columns with missing values\n",
    "cols = nan[condition].index # get the column names\n",
    "print(cols)\n",
    "cols = list(cols) # convert column names to list\n",
    "print(cols)\n",
    "print(df[cols].dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02142a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
